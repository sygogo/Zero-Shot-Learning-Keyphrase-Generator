enhancing product recommender systems on sparse binary data . <eos> commercial recommender systems use various data mining techniques to make appropriate recommendations to users during online , real time sessions . published algorithms focus more on the discrete user ratings instead of binary results , which hampers their predictive capabilities when usage data is sparse . the system proposed in this paper , e <unk> , is an association mining based recommender tool designed to overcome these problems through a two phase approach . in the first phase , batches of customer historical data are analyzed through association mining in order to determine the association rules for the second phase . during the second phase , a scoring algorithm is used to rank the recommendations online for the customer . the second phase differs from the traditional approach and an empirical comparison between the methods used in e <unk> and other collaborative filtering methods including dependency networks , item based , and association mining is provided in this paper . this comparison evaluates the algorithms used in each of the above methods using two internal customer datasets and a benchmark dataset . the results of this comparison clearly show that e <unk> performs well compared to dependency networks and association mining . in general , item based algorithms with cosine similarity measures have the best performance .
a unifying semantics for time and events . <eos> we give a formal semantics for a highly expressive language for representing temporal relationships and events . this language , which we call versatile event logic ( vel ) , provides a general temporal ontology and semantics encompassing many other representations . the system incorporates a number of features that have not been widely employed in ai formalisms . it has the ability to describe alternative histories using a modal operator . it provides a semantics for individuals that explicitly models their identity through time and across alternative possible histories and enables one to distinguish between necessary and extensional identity of individuals . in virtue of its treatment of individuals and count nouns , the formalism offers a solution to certain puzzles of identity , which arise when individuals are described in different ways . we propose that vel can be used as a foundational interlingua for comparing and interfacing different ai languages and illustrate this by considering how situation calculus and event calculus can be represented within vel .
representing the zoo world and the traffic world in the language of the causal calculator . <eos> the work described in this report is motivated by the desire to test the expressive possibilities of action language c . the causal calculator ( ccalc ) is a system that answers queries about action domains described in a fragment of that language . the zoo world and the traffic world have been proposed by erik <unk> in his logic modelling workshop an environment for communicating axiomatizations of action domains of nontrivial size . the zoo world consists of several cages and the exterior , gates between them , and animals of several species , including humans . actions in this domain include moving within and between cages , opening and closing gates , and mounting and riding animals . the traffic world includes vehicles moving continuously between road crossings subject to a number of restrictions , such as speed limits and keeping a fixed safety distance away from other vehicles on the road . we show how to represent the two domains in the input language of ccalc , and how to use ccalc to test these representations .
logic based subsumption architecture . <eos> we describe a logic based ai architecture based on brooks ' subsumption architecture . in this architecture , we axiomatize different layers of control in first order logic ( fol ) and use independent theorem provers to derive each layer ' s outputs given its inputs . we implement the subsumption of lower layers by higher layers using nonmonotonic reasoning principles . in particular , we use circumscription to make default assumptions in lower layers , and nonmonotonically retract those assumptions when higher layers draw new conclusions . we also give formal semantics to our approach . finally , we describe layers designed for the task of robot control and a system that we have implemented that uses this architecture for the control of a nomad <digit> mobile robot . our system combines the virtues of using the represent and reason paradigm and the behavioral decomposition paradigm . it allows multiple goals to be serviced simultaneously and reactively . it also allows high level tasks and is tolerant to different changes and elaborations of its knowledge in runtime . finally , it allows us to give more commonsense knowledge to robots . we report on several experiments that empirically show the feasibility of using fully expressive fol theorem provers for robot control with our architecture and the benefits claimed above .
excluding any graph as a minor allows a low tree width <digit> coloring . <eos> this article proves the conjecture of thomas that , for every graph g , there is an integer k such that every graph with no minor isomorphic to g has a <digit> coloring of either its vertices or its edges where each color induces a graph of tree <unk> at most k . some generalizations are also proved .
a model of bgp routing for network engineering . <eos> the performance of ip networks depends on a wide variety of dynamic conditions . traffic shifts , equipment failures , planned maintenance , and topology changes in other parts of the internet can all degrade performance . to maintain good performance , network operators must continually reconfigure the routing protocols . operators configure bgp to control how traffic flows to neighboring autonomous systems ( ases ) , as well as how traffic traverses their networks . however , because bgp route selection is distributed , indirectly controlled by configurable policies , and influenced by complex interactions with intradomain routing protocols , operators can not predict how a particular bgp configuration would behave in practice . to avoid inadvertently degrading network performance , operators need to evaluate the effects of configuration changes before deploying them on a live network . we propose an algorithm that computes the outcome of the bgp route selection process for each router in a single as , given only a static snapshot of the network state , without simulating the complex details of bgp message passing . we describe a bgp emulator based on this algorithm the emulator exploits the unique characteristics of routing data to reduce computational overhead . using data from a large isp , we show that the emulator correctly computes bgp routing decisions and has a running time that is acceptable for many tasks , such as traffic engineering and capacity planning .
collective mining of bayesian networks from distributed heterogeneous data . <eos> we present a collective approach to learning a bayesian network from distributed heterogeneous data . in this approach , we first learn a local bayesian network at each site using the local data . then each site identifies the observations that are most likely to be evidence of coupling between local and non local variables and transmits a subset of these observations to a central site . another bayesian network is learnt at the central site using the data transmitted from the local site . the local and central bayesian networks are combined to obtain a collective bayesian network , which models the entire data . experimental results and theoretical justification that demonstrate the feasibility of our approach are presented .
automatic performance evaluation of web search engines . <eos> measuring the information retrieval effectiveness of world wide web search engines is costly because of human relevance judgments involved . however , both for business enterprises and people it is important to know the most effective web search engines , since such search engines help their users find higher number of relevant web pages with less effort . furthermore , this information can be used for several practical purposes . in this study we introduce automatic web search engine evaluation method as an efficient and effective assessment tool of such systems . the experiments based on eight web search engines , <digit> queries , and binary user relevance judgments show that our method provides results consistent with human based evaluations . it is shown that the observed consistencies are statistically significant . this indicates that the new method can be successfully used in the evaluation of web search engines .
exact algorithms for finding minimum transversals in rank <digit> hypergraphs . <eos> we present two algorithms for the problem of finding a minimum transversal in a hypergraph of rank <digit> , also known as the <digit> hitting set problem . this problem is a natural extension of the vertex cover problem for ordinary graphs . the first algorithm runs in time o ( <digit> . <unk> ) for a hypergraph with n vertices , and needs polynomial space . the second algorithm uses exponential space and runs in time o ( <digit> . <unk> ) .
establishing wireless conference calls under delay constraints . <eos> a prevailing feature of mobile telephony systems is that the cell where a mobile user is located may be unknown . therefore , when the system is to establish a call between users , it may need to search , or page , all the cells that it suspects the users are located in , to find the cells where the users currently reside . the search consumes expensive wireless links and so it is desirable to develop search techniques that page as few cells as possible . we consider cellular systems with c cells and m mobile users roaming among the cells . the location of the users is uncertain as given by m probability distribution vectors . whenever the system needs to find specific users , it conducts a search operation lasting some number of rounds ( the delay constraint ) . in each round , the system may check an arbitrary subset of cells to see which users are located there . in this setting the problem of finding one user with minimum expected number of cells searched is known to be solved optimally in polynomial time . in this paper we address the problem of finding several users with the same optimization goal . this task is motivated by the problem of establishing a conference call between mobile users . we first show that the problem is np hard . then we prove that a natural heuristic is an e ( e <digit> ) approximation solution .
the simplest examples where the simplex method cycles and conditions where expand fails to prevent cycling . <eos> this paper introduces a class of linear programming examples that cause the simplex method to cycle and that are the simplest possible examples showing this behaviour . the structure of examples from this class repeats after two iterations . cycling is shown to occur for both the most negative reduced cost and steepest edge column selection criteria . in addition it is shown that the expand anti cycling procedure of gill et al . is not guaranteed to prevent cycling .
an experimental evaluation of continuous testing during development . <eos> continuous testing uses excess cycles on a developer ' s workstation to continuously run regression tests in the background , providing rapid feedback about test failures as source code is edited . it is intended to reduce the time and energy required to keep code well tested and prevent regression errors from persisting uncaught for long periods of time . this paper reports on a controlled human experiment to evaluate whether students using continuous testing are more successful in completing programming assignments . we also summarize users ' subjective impressions and discuss why the results may generalize . the experiment indicates that the tool has a statistically significant effect on success in completing a programming task , but no such effect on time worked . participants using continuous testing were three times more likely to complete the task before the deadline than those without . participants using continuous compilation were twice as likely to complete the task , providing empirical support to a common feature in modern development environments . most participants found continuous testing to be useful and believed that it helped them write better code faster , and <digit> % would recommend the tool to others . the participants did not find the tool distracting , and intuitively developed ways of incorporating the feedback into their workflow .
test input generation with java pathfinder . <eos> we show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures . we focus on obtaining branch coverage during unit testing of some of the core methods of the red black tree implementation in the java treemap library , using the java pathfinder model checker . three different test generation techniques will be introduced and compared , namely , straight model checking of the code , model checking used in a black box fashion to generate all inputs up to a fixed size , and lastly , model checking used during white box test input generation . the main contribution of this work is to show how efficient white box test input generation can be done for code manipulating complex data , taking into account complex method preconditions .
improving the adaptability of multi mode systems via program steering . <eos> a multi mode software system contains several distinct modes of operation and a controller for deciding when to switch between modes . even when developers rigorously test a multi mode system before deployment , they can not foresee and test for every possible usage scenario . as a result , unexpected situations in which the program fails or underperforms ( for example , by choosing a non optimal mode ) may arise . this research aims to mitigate such problems by creating a new mode selector that examines the current situation , then chooses a mode that has been successful in the past , in situations like the current one . the technique , called program steering , creates a new mode selector via machine learning from good behavior in testing or in successful operation . such a strategy , which generalizes the knowledge that a programmer has built into the system , may select an appropriate mode even when the original controller can not . we have performed experiments on robot control programs written in a month long programming competition . augmenting these programs via our program steering technique had a substantial positive effect on their performance in new environments .
model checking xml manipulating software . <eos> the use of xml as the de facto data exchange standard has allowed integration of heterogeneous web based software systems regardless of implementation platforms and programming languages . on the other hand , the rich tree structured data representation , and the expressive xml query languages ( such as xpath ) make formal specification and verification of software systems that manipulate xml data a challenge . in this paper , we present our initial efforts in automated verification of xml data manipulation operations using the spin model checker . we present algorithms for translating ( bounded ) xml data and xpath expressions to promela , the input language of spin . the techniques presented in this paper constitute the basis of our web service analysis tool ( wsat ) which verifies ltl properties of composite web services .
optimal time bounds for approximate clustering . <eos> clustering is a fundamental problem in unsupervised learning , and has been studied widely both as a problem of learning mixture models and as an optimization problem . in this paper , we study clustering with respect to the k median objective function , a natural formulation of clustering in which we attempt to minimize the average distance to cluster centers . one of the main contributions of this paper is a simple but powerful sampling technique that we call successive sampling that could be of independent interest . we show that our sampling procedure can rapidly identify a small set of points ( of size just o ( k log frac n k ) ) that summarize the input points for the purpose of clustering . using successive sampling , we develop an algorithm for the k median problem that runs in o ( nk ) time for a wide range of values of k and is guaranteed , with high probability , to return a solution with cost at most a constant factor times optimal . we also establish a lower bound of ( nk ) on any randomized constant factor approximation algorithm for the k median problem that succeeds with even a negligible ( say frac <digit> <digit> ) probability . the best previous upper bound for the problem was ( nk ) , where the notation hides polylogarithmic factors in n and k . the best previous lower bound of ( nk ) applied only to deterministic k median algorithms . while we focus our presentation on the k median objective , all our upper bounds are valid for the k means objective as well . in this context our algorithm compares favorably to the widely used k means heuristic , which requires o ( nk ) time for just one iteration and provides no useful approximation guarantees .
correlation clustering . <eos> we consider the following clustering problem we have a complete graph on n vertices ( items ) , where each edge ( u , v ) is labeled either or depending on whether u and v have been deemed to be similar or different . the goal is to produce a partition of the vertices ( a clustering ) that agrees as much as possible with the edge labels . that is , we want a clustering that maximizes the number of edges within clusters , plus the number of edges between clusters ( equivalently , minimizes the number of disagreements the number of edges inside clusters plus the number of edges between clusters ) . this formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data , and the goal is to partition the current set of documents in a way that correlates with f as much as possible semi it can also be viewed as a kind of agnostic learning problem . an interesting feature of this clustering formulation is that one does not need to specify the number of clusters k as a separate parameter , as in measures such as k median or min sum or min max clustering . instead , in our formulation , the optimal number of clusters could be any value between <digit> and n , depending on the edge labels . we look at approximation algorithms for both minimizing disagreements and for maximizing agreements . for minimizing disagreements , we give a constant factor approximation . for maximizing agreements we give a ptas , building on ideas of goldreich , goldwasser , and ron ( <digit> ) and de la veg ( <digit> ) . we also show how to extend some of these results to graphs with edge labels in <digit> , <digit> , and give some results for the case of random noise .
subquadratic approximation algorithms for clustering problems in high dimensional spaces . <eos> one of the central problems in information retrieval , data mining , computational biology , statistical analysis , computer vision , geographic analysis , pattern recognition , distributed protocols is the question of classification of data according to some clustering rule . often the data is noisy and even approximate classification is of extreme importance . the difficulty of such classification stems from the fact that usually the data has many incomparable attributes , and often results in the question of clustering problems in high dimensional spaces . since they require measuring distance between every pair of data points , standard algorithms for computing the exact clustering solutions use quadratic or nearly quadratic running time semi i . e . , o ( dn2 ( d ) ) time where n is the number of data points , d is the dimension of the space and ( d ) approaches <digit> as d grows . in this paper , we show ( for three fairly natural clustering rules ) that computing an approximate solution can be done much more efficiently . more specifically , for agglomerative clustering ( used , for example , in the alta vista search engine ) , for the clustering defined by sparse partitions , and for a clustering based on minimum spanning trees we derive randomized ( <digit> approximation algorithms with running times ( d2 n2 ) where > <digit> depends only on the approximation parameter epsi and is independent of the dimension d .
untangling of 2d meshes in ale simulations . <eos> a procedure is presented to untangle unstructured 2d meshes containing inverted elements by node repositioning . the inverted elements may result from node movement in arbitrary lagrangian eulerian ( ale ) simulations of continuum mechanics problems with large shear deformation such as fluid flow and metal forming . meshes with inverted elements may also be created due to the limitations of mesh generation algorithms particularly for nonsimplicial mesh generation . the untangling procedure uses a combination of direct node placement based on geometric computation of the feasible set , and node repositioning driven by numerical optimization of an objective function that achieves its minimum on a valid mesh . it is shown that a combination of the feasible set , based method and the optimization method achieves the best results in untangling complex 2d meshes . preliminary results are also presented for untangling of 3d unstructured meshes by the same approach .
online hierarchical cooperative caching . <eos> we address a hierarchical generalization of the well known disk paging problem . in the hierarchical cooperative caching problem , a set of n machines residing in an ultrametric space cooperate with one another to satisfy a sequence of read requests to a collection of ( read only ) files . a seminal result in the area of competitive analysis states that lru ( the widely used deterministic online paging algorithm based on the least recently used eviction policy ) is constant competitive if it is given a constant factor blowup in capacity over the offline algorithm . does such a constant competitive deterministic algorithm ( with a constant factor blowup in the machine capacities ) exist for the hierarchical cooperative caching problem the main contribution of the present paper is to answer this question in the negative . more specifically , we establish an ( log log n ) lower bound on the competitive ratio of any online hierarchical cooperative caching algorithm with capacity blowup o ( ( log n ) <digit> ) , where denotes an arbitrarily small positive constant .
theory and applications of inverting functions as folds . <eos> this paper is devoted to the proof , applications , and generalisation of a theorem , due to bird and de moor , that gave conditions under which a total function can be expressed as a relational fold . the theorem is illustrated with three problems , all dealing with constructing trees with various properties . it is then generalised to give conditions under which the inverse of a partial function can be expressed as a relational hylomorphism . its proof makes use of doornbos and backhouse ' s theory on well foundedness and reductivity . possible applications of the generalised theorem is discussed .
accurate conjugate gradient methods for families of shifted systems . <eos> we consider the solution of the linear system real values of . this family of shifted systems arises , for example , in tikhonov regularization and computations in lattice quantum chromodynamics . for each single shift this system can be solved using the conjugate gradient method for least squares problems ( cgls ) . in literature various implementations of the , so called , multishift cgls methods have been proposed . these methods are mathematically equivalent to applying the cgls method to each shifted system separately but they solve all systems simultaneously and require only two matrix vector products ( one by a and one by at ) and two inner products per iteration step . unfortunately , numerical experiments show that , due to roundoff errors , in some cases these implementations of the multishift cgls method can only attain an accuracy that depends on the square of condition number of the matrix a . in this paper we will argue that , in the multishift cgls method , the impact on the attainable accuracy of rounding errors in the lanczos part of the method is independent of the effect of roundoff errors made in the construction of the iterates . by making suitable design choices for both parts , we derive a new ( and efficient ) implementation that tries to remove the limitation of previous proposals . a partial roundoff error analysis and various numerical experiments show promising results .
computing smallest singular triplets with implicitly restarted lanczos bidiagonalization . <eos> a matrix free algorithm , <unk> , for the efficient computation of the smallest singular triplets of large and possibly sparse matrices is described . key characteristics of the approach are its use of lanczos bidiagonalization , implicit restarting , and harmonic ritz values . the algorithm also uses a deflation strategy that can be applied directly on lanczos bidiagonalization . a refinement postprocessing phase is applied to the converged singular vectors . the computational costs of the above techniques are kept small as they make direct use of the bidiagonal form obtained in the course of the lanczos factorization . several numerical experiments with the method are presented that illustrate its effectiveness and indicate that it performs well compared to existing codes .
unifying proof methodologies of duration calculus and timed linear temporal logic . <eos> linear temporal logic ( ltl ) has been widely used for specification and verification of reactive systems . its standard model is sequences of states ( or state transitions ) , and formulas describe sequencing of state transitions . when ltl is used to model real time systems , a state is extended with a time stamp to record when a state transition takes place . duration calculus ( dc ) is another well studied approach for real time systems development . dc models behaviours of a system by functions from the domain of reals representing time to the system states . this paper extends this time domain to the cartesian product of the real and the natural numbers . with the extended time domain , we provide the chop modality with a non overlapping interpretation . this allows some linear temporal operators explicitly dealing with the discrete dimension of time to be derivable from the chop modality in essentially the same way that their continuous time counterparts are in the classical dc . this provides a nice embedding of some timed ltl ( tltl ) modalities into dc to unify the methods from dc and ltl for real time systems development requirements and high level design decisions are interval properties and are therefore specified and reasoned about in dc , while properties of an implementation , as well as the refinement relation between two implementations , are specified and verified compositionally and inductively in ltl . implementation properties are related to requirement and design properties by rules for lifting ltl formulas to dc formulas .
tight bounds on the competitive ratio on accommodating sequences for the seat reservation problem . <eos> the unit price seat reservation problem is investigated . the seat reservation problem is the problem of assigning seat numbers on line to requests for reservations in a train traveling through k stations . we are considering the version where all tickets have the same price and where requests are treated fairly , that is , a request which can be fulfilled must be granted . for fair deterministic algorithms , we provide an asymptotically matching upper bound to the existing lower bound which states that all fair algorithms for this problem are competitive on accommodating sequences , when there are at least three seats . additionally , we give an asymptotic upper bound of <digit> <digit> for fair randomized algorithms against oblivious adversaries . we also examine concrete on line algorithms , first fit and random for the special case of two seats . tight analyses of their performance are given .
admission control with immediate notification . <eos> when admission control is used , an on line scheduler chooses whether or not to complete each individual job successfully by its deadline . an important consideration is at what point in time the scheduler determines if a job request will be satisfied , and thus at what point the scheduler is able to provide notification to the job owner as to the fate of the request . in the loosest model , often seen in real time systems , such a decision can be deferred up until the job ' s deadline passes . in the strictest model , more suitable for customer based applications , a scheduler would be required to give notification at the instant that a job request arrives . unfortunately there seems to be little existing research which explicitly studies the effect of the notification model on the performance guarantees of a scheduler . we undertake such a study by reexamining a problem from the literature . specifically , we study the effect of the notification model on the non preemptive scheduling of a single resource in order to maximize utilization . at first glance , it appears severely more restrictive to compare a scheduler required to give immediate notification to one which need not give any notification . yet we are able to present alternate algorithms which provide immediate notification , while matching most of the performance guarantees which are possible by schedulers which provide no such notification . in only one case are we able to give evidence that providing immediate notification may be more difficult .
distributed council election . <eos> this paper studies the problem of electing a small number of representatives ( council ) out of a ( possible large ) group of anonymous candidates . the problem arises in scenarios such as multicast where , to avoid feedback implosion , a small subset of the receivers is chosen to provide feedback on network conditions . we present several algorithms for this problem and analyze the expected number of messages and rounds required for their convergence . in particular , we present an algorithm that almost always converges in one round using a small number of messages ( for typical council size ) when the number of hosts is known . in the case where the number of hosts is unknown ( and too large to be polled ) , our algorithms converge in a small number of rounds that improves previous results by bolot et al . ( <digit> ) .
rsa oaep is secure under the rsa assumption . <eos> recently victor shoup noted that there is a gap in the widely believed security result of oaep against adaptive chosen ciphertext attacks . moreover , he showed that , presumably , oaep can not be proven secure from the one wayness of the underlying trapdoor permutation . this paper establishes another result on the security of oaep . it proves that oaep offers semantic security against adaptive chosen ciphertext attacks , in the random oracle model , under the partial domain one wayness of the underlying permutation . therefore , this uses a formally stronger assumption . nevertheless , since partial domain one wayness of the rsa function is equivalent to its ( full domain ) <unk> , it follows that the security of rsa oaep can actually be proven under the sole rsa assumption , although the reduction is not tight .
corpus structure , language models , and ad hoc information retrieval . <eos> most previous work on the recently developed language modeling approach to information retrieval focuses on document specific characteristics , and therefore does not take into account the structure of the surrounding corpus . we propose a novel algorithmic framework in which information provided by document based language models is enhanced by the incorporation of information drawn from clusters of similar documents . using this framework , we develop a suite of new algorithms . even the simplest typically outperforms the standard language modeling approach in precision and recall , and our new interpolation algorithm posts statistically significant improvements for both metrics over all three corpora tested .
solving equations in the relational algebra . <eos> enumerating all solutions of a relational algebra equation is a natural and powerful operation which , when added as a query language primitive to the nested relational algebra , yields a query language for nested relational databases , equivalent to the well known powerset algebra . we study sparse equations , which are equations with at most polynomially many solutions . we look at their complexity and compare their expressive power with that of similar notions in the powerset algebra .
on the hardness of <digit> coloring a <digit> colorable graph . <eos> we give a new proof showing that it is np hard to color a <digit> colorable graph using just <digit> colors . this result is already known , s . khanna , n . linial , and s . safra , combinatorica , <digit> ( <digit> ) , pp . <digit> <digit> , but our proof is novel because it does not rely on the pcp theorem , while the known one does . this highlights a qualitative difference between the known hardness result for coloring <digit> colorable graphs and the factor n epsilon hardness for approximating the chromatic number of general graphs , as the latter result is known to imply ( some form of ) pcp theorem m . bellare , o . goldreich , and m . sudan , siam j . comput . , <digit> ( <digit> ) , pp . <digit> <digit> . another aspect in which our proof is novel is in its use of the pcp theorem to show that <digit> coloring of <digit> colorable graphs remains np hard even on bounded degree graphs ( this hardness result does not seem to follow from the earlier reduction of khanna , linial , and safra ) . we point out that such graphs can always be colored using o ( <digit> ) colors by a simple greedy algorithm , while the best known algorithm for coloring ( general ) <digit> colorable graphs requires n omega ( <digit> ) colors . our proof technique also shows that there is an varepsilon <digit> > <digit> such that it is np hard to legally <digit> color even a ( <digit> varepsilon <digit> ) fraction of the edges of a <digit> colorable graph .
mining constrained gradients in large databases . <eos> many data analysis tasks can be viewed as search or mining in a multidimensional space ( mds ) . in such mdss , dimensions capture potentially important factors for given applications , and cells represent combinations of values for the factors . to systematically analyze data in mds , an interesting notion , called cubegrade was recently introduced by imielinski et al . check end of sentence , which focuses on the notable changes in measures in mds by comparing a cell ( which we refer to as probe cell ) with its gradient cells , namely , its ancestors , descendants , and siblings . we call such queries gradient analysis queries ( gqs ) . since an mds can contain billions of cells , it is important to answer gqs efficiently . in this study , we focus on developing efficient methods for mining gqs constrained by certain ( weakly ) antimonotone constraints . instead of conducting an independent gradient cell search once per probe cell , which is inefficient due to much repeated work , we propose an efficient algorithm , liveset driven . this algorithm finds all good gradient probe cell pairs in one search pass . it utilizes measure value analysis and dimension match analysis in a set oriented manner , to achieve bidirectional pruning between the sets of hopeful probe cells and of hopeful gradient cells . moreover , it adopts a hypertree structure and an h cubing method to compress data and to maximize sharing of computation . our performance study shows that this algorithm is efficient and scalable . in addition to data cubes , we extend our study to another important scenario mining constrained gradients in transactional databases where each item is associated with some measures such as price . such transactional databases can be viewed as sparse mdss where items represent dimensions , although they have significantly different characteristics than data cubes . we outline efficient mining methods for this problem in this paper .
development of a verified erlang program for resource locking . <eos> in this paper , we describe a tool to verify erlang programs and show , by means of an industrial case study , how this tool is used . the tool includes a number of components , including a translation component , a state space generation component and a model checking component . to verify properties of the code , the tool first translates the erlang code into a process algebraic specification . the outcome of the translation is made more efficient by taking advantage of the fact that software written in erlang builds upon software design patterns such as clientserver behaviours . a labelled transition system is constructed from the specification by use of the crl toolset . the resulting labelled transition system is model checked against a set of properties formulated in the calculus using the caesar <unk> toolset . as a case study we focus on a simplified resource manager modelled on a real implementation in the control software of the axd <digit> atm switch . some of the key properties we verified for the program are mutual exclusion and non starvation . since the toolset supports only the regular alternation free calculus , some ingenuity is needed for checking the liveness property non starvation . the case study has been refined step by step to provide more functionality , with each step motivated by a corresponding formal verification using model checking .
equilogical spaces . <eos> it is well known that one can build models of full higher order dependent type theory ( also called the calculus of constructions ) using partial equivalence relations ( pers ) and assemblies over a partial combinatory algebra . but the idea of categories of pers and ers ( total equivalence relations ) can be applied to other structures as well . in particular , we can easily define the category of ers and equivalence preserving continuous mappings over the standard category <unk> of topological t0 spaces we call these spaces ( a topological space together with an er ) equilogical spaces and the resulting category equ . we show that this category in contradistinction to <unk> is a cartesian closed category . the direct proof outlined here uses the equivalence of the category equ to the category <unk> of pers over algebraic lattices ( a full subcategory of <unk> that is well known to be cartesian closed from domain theory ) . in another paper with <unk> and <unk> ( cited herein ) , a more abstract categorical generalization shows why many such categories are cartesian closed . the category equ obviously contains <unk> as a full subcategory , and it naturally contains many other well known subcategories . in particular , we show why , as a consequence of work of ershov , berger , and others , the kleene kreisel hierarchy of countable functionals of finite types can be naturally constructed in equ from the natural numbers object n by repeated use in equ of exponentiation and binary products . we also develop for equ notions of modest sets ( a category equivalent to equ ) and assemblies to explain why a model of dependent type theory is obtained . we make some comparisons of this model to other , known models .
two level languages for program optimization . <eos> two level languages incorporate binding time information inside types , that is , whether a piece of code is completely known at compile time , or needs some more inputs and can be evaluated only at run time . we consider the use of <digit> level languages in the framework of partial evaluation , and use a <digit> level version of the simply typed lambda calculus with recursion . we give an operational semantics , an equational theory and a denotational semantics , that give an account of the distinction between compilation and execution phases . an adequacy theorem is given to relate the two semantics , showing in particular how they agree on non termination at compile time . we finally give a more refined model using functor categories .
poset valued sets or how to build models for linear logics . <eos> we describe a method for constructing models of linear logic based on the category of sets and relations . the resulting categories are non degenerate in general in particular they are not compact closed nor do they have biproducts . the construction is simple , lifting the structure of a poset to the new category . the underlying poset thus controls the structure of this category , and different posets give rise to differently flavoured models . as a result , this technique allows the construction of models for both , intuitionistic or classical linear logic as desired . a number of well known models , for example coherence spaces and hypercoherences , are instances of this method .
semantics and logic of object calculi . <eos> the main contribution of this paper is a formal characterization of recursive object specifications and their existence based on a denotational untyped semantics of the object calculus . existence is not guaranteed but can be shown employing pitts ' results on relational properties of domains . the semantics can be used to analyse and verify abadi and <unk> ' s object logic but it also suggests extensions . for example , specifications of methods may not only refer to fields but also to methods of objects in the store . this can be achieved without compromising the existence theorem . an informal logic of predomains is in use intentionally in order to avoid any commitment to a particular syntax of specification logic .
optimistic register coalescing . <eos> graph coloring register allocators eliminate copies by coalescing the source and target nodes of a copy if they do not interfere in the interference graph . coalescing , however , can be harmful to the colorability of the graph because it tends to yield a graph with nodes of higher degrees . unlike aggressive coalescing , which coalesces any pair of noninterfering copy related nodes , conservative coalescing or iterated coalescing perform safe coalescing that preserves the colorability . unfortunately , these heuristics give up coalescing too early , losing many opportunities for coalescing that would turn out to be safe . moreover , they ignore the fact that coalescing may even improve the colorability of the graph by reducing the degree of neighbor nodes that are interfering with both the source and target nodes being coalesced . this article proposes a new heuristic called optimistic coalescing which optimistically performs aggressive coalescing , thus exploiting the positive impact of coalescing aggressively , but when a coalesced node is to be spilled , it is split back into separate nodes . since there is a better chance of coloring one of those splits , we can reduce the overall spill amount .
computation in networks of passively mobile finite state sensors . <eos> we explore the computational power of networks of small resource limited mobile agents . we define two new models of computation based on pairwise interactions of finite state agents in populations of finite but unbounded size . with a fairness condition on interactions , we define the concept of stable computation of a function or predicate , and give protocols that stably compute functions in a class including boolean combinations of threshold k , parity , majority , and simple arithmetic . we prove that all stably computable predicates are in nl . with uniform random sampling of pairs to interact , we define the model of conjugating automata and show that any counter machine with o ( <digit> ) counters of capacity o ( n ) can be simulated with high probability by a protocol in a population of size n . we prove that all predicates computable with high probability in this model are in p rl . several open problems and promising future directions are discussed .
asynchronous group key exchange with failures . <eos> group key exchange protocols allow a group of servers communicating over an asynchronous network of point to point links to establish a common key , such that an adversary which fully controls the network links ( but not the group members ) can not learn the key . currently known group key exchange protocols rely on the assumption that all group members participate in the protocol and if a single server crashes , then no server may terminate the protocol . in this paper , we propose the first purely asynchronous group key exchange protocol that tolerates a minority of servers to crash . our solution uses a constant number of rounds , which makes it suitable for use in practice . furthermore , we also investigate how to provide forward secrecy with respect to an adversary that may break into some servers and observe their internal state . we show that any group key exchange protocol among n servers that tolerates tc > <digit> servers to crash can only provide forward secrecy if the adversary breaks into less than n <unk> servers , and propose a group key exchange protocol that achieves this bound .
euclidean group invariant computation of stochastic completion fields using shiftable twistable functions . <eos> we describe a method for computing the likelihood that a completion joining two contour fragments passes through any given position and orientation in the image plane . like computations in primary visual cortex ( and unlike all previous models of contour completion ) , the output of our computation is invariant under rotations and translations of the input pattern . this is achieved by representing the input , output , and intermediate states of the computation in a basis of shiftable twistable functions .
a linear algebraic approach to metering schemes . <eos> a metering scheme is a method by which an audit agency is able to measure the interaction between servers and clients during a certain number of time frames . naor and pinkas ( vol . <digit> of lncs , pp . <digit> ) proposed metering schemes where any server is able to compute a proof ( i . e . , a value to be shown to the audit agency at the end of each time frame ) , if and only if it has been visited by a number of clients larger than or equal to some threshold h during the time frame . masucci and stinson ( vol . <digit> of lncs , pp . <digit> ) showed how to construct a metering scheme realizing any access structure , where the access structure is the family of all subsets of clients which enable a server to compute its proof . they also provided lower bounds on the communication complexity of metering schemes . in this paper we describe a linear algebraic approach to design metering schemes realizing any access structure . namely , given any access structure , we present a method to construct a metering scheme realizing it from any linear secret sharing scheme with the same access structure . besides , we prove some properties about the relationship between metering schemes and secret sharing schemes . these properties provide some new bounds on the information distributed to clients and servers in a metering scheme . according to these bounds , the optimality of the metering schemes obtained by our method relies upon the optimality of the linear secret sharing schemes for the given access structure .
finite state machines for strings over infinite alphabets . <eos> motivated by formal models recently proposed in the context of xml , we study automata and logics on strings over infinite alphabets . these are conservative extensions of classical automata and logics defining the regular languages on finite alphabets . specifically , we consider register and pebble automata , and extensions of first order logic and monadic second order logic . for each type of automaton we consider one way and two way variants , as well as deterministic , nondeterministic , and alternating control . we investigate the expressiveness and complexity of the automata and their connection to the logics , as well as standard decision problems . some of our results answer open questions of kaminski and francez on register automata .
reflective metalogical frameworks . <eos> a metalogical framework is a logic with an associated methodology that is used to represent other logics and to reason about their metalogical properties . we propose that logical frameworks can be good metalogical frameworks when their theories always have initial models and they support reflective and parameterized reasoning . we develop this thesis both abstractly and concretely . abstractly , we formalize our proposal as a set of requirements and explain how any logic satisfying these requirements can be used for metalogical reasoning . concretely , we present membership equational logic as a particular metalogic that satisfies these requirements . using membership equational logic , and its realization in the maude system , we show how reflection can be used for different , nontrivial kinds of formal metatheoretic reasoning . in particular , one can prove metatheorems that relate theories or establish properties of parameterized classes of theories .
scheduling search procedures . <eos> we analyze preemptive on line scheduling against randomized adversaries , with the goal to finish an unknown distinguished target job . our motivation comes from clinical gene search projects , but the subject leads to general theoretical questions of independent interest , including some natural but unusual probabilistic models . we study problem versions with known and unknown processing times of jobs and target probabilities , and models where the on line player gets some randomized extra information about the target . for some versions we get optimal competitive ratios , expressed in terms of given parameters of instances .
relating communicating processes with different interfaces . <eos> we present here an implementation relation intended to formalise the notion that a system built of communicating processes is an acceptable implementation of another base , or target , system in the event that the two systems have different interfaces . such a treatment has clear applicability in the software development process , where ( the interface of ) an implementation component may be expressed at a different level of abstraction to ( the interface of ) the relevant specification component . technically , processes are formalised using hoare ' s csp language , with its standard failures divergences model . the implementation relation is formulated in terms of failures and divergences of the implementation and target processes . interface difference is modelled by endowing the implementation relation with parameters called extraction patterns . these are intended to interpret implementation behaviour as target behaviour , and suitably constrain the former in connection to well formedness and deadlock properties . we extend the results of our previous work and replace implementation relations previously presented by a single , improved scheme . we also remove all the restrictions previously placed upon target processes . two basic kinds of results are obtained realisability and compositionality . the latter means that a target composed of several connected systems may be implemented by connecting their respective implementations . the former means that , if target and implementation in fact have the same interface , then the implementation relation they should satisfy collapses into standard implementation pre order . we also show how to represent processes and extraction patterns in a manner amenable to computer implementation , and detail a graph theoretic restatement of the conditions defining the implementation relation , from which algorithms for their automatic verification are easily derived .
an information theoretic model for steganography . <eos> an information theoretic model for steganography with a passive adversary is proposed . the adversary ' s task of distinguishing between an innocent cover message c and a modified message s containing hidden information is interpreted as a hypothesis testing problem . the security of a steganographic system is quantified in terms of the relative entropy ( or discrimination ) between the distributions of c and s , which yields bounds on the detection capability of any adversary . it is shown that secure steganographic schemes exist in this model provided the covertext distribution satisfies certain conditions . a universal stegosystem is presented in this model that needs no knowledge of the covertext distribution , except that it is generated from independently repeated experiments .
minimization and np multifunctions . <eos> the implicit characterizations of the polynomial time computable functions fp given by bellantoni cook and leivant suggest that this class is the complexity theoretic analog of the primitive recursive functions . hence , it is natural to add minimization operators to these characterizations and investigate the resulting class of partial functions as a candidate for the analog of the partial recursive functions . we do so in this paper for cobham ' s definition of fp by bounded recursion and for bellantoni cook ' s safe recursion and prove that the resulting classes capture exactly npmv , the nondeterministic polynomial time computable partial multifunctions . we also consider the relationship between our schemes and a notion of nondeterministic recursion defined by leivant and show that the latter characterizes the total functions of npmv . we view these results as giving evidence that npmv is the appropriate analog of partial recursive . this view is reinforced by earlier results of <unk> and stahl who show that for many of the relationships between partial recursive functions and r . e . sets , analogous relationships hold between npmv and np sets . furthermore , since npmv is obtained from fp in the same way as the recursive functions are obtained from the primitive recursive functions ( when defined via function schemes ) , this also gives further evidence that fp is properly seen as playing the role of primitive recursion .
realizability models for <unk> like languages . <eos> we give a realizability model of girard scedrov scott ' s bounded linear logic ( <unk> ) . this gives a new proof that all numerical functions representable in that system are polytime . our analysis naturally justifies the design of the <unk> syntax and suggests further extensions .
space efficient planar convex hull algorithms . <eos> a space efficient algorithm is one in which the output is given in the same location as the input and only a small amount of additional memory is used by the algorithm . we describe four space efficient algorithms for computing the convex hull of a planar point set .
electronic jury voting protocols . <eos> this work stresses the fact that all current proposals for electronic voting schemes disclose the final tally of the votes . in certain situations , like jury voting , this may be undesirable . we present a robust and universally verifiable membership testing scheme ( mts ) that allows , among other things , a collection of voters to cast votes and determine whether their tally belongs to some pre specified small set ( e . g . , exceeds a given threshold ) our scheme discloses no additional information than that implied from the knowledge of such membership . we discuss several extensions of our basic mts . all the constructions presented combine features of two parallel lines of research concerning electronic voting schemes , those based on mix networks and in homomorphic encryption .
detecting unauthorized activities using a sensor network . <eos> sensing devices can be deployed to form a network for monitoring a region of interest . this chapter investigates the detection of a target in the region being monitored by using collaborative target detection algorithms among the sensors . the objective is to develop a low cost sensor deployment strategy to meet a performance criteria . a path exposure metric is proposed to measure the goodness of deployment . exposure can be defined and efficiently computed for various target activities , targets traveling at variable speed and in the presence of obstacles in the region . using exposure to evaluate the detection performance , the problem of random sensor deployment is formulated . the problem defines cost functions that take into account the cost of single sensors and the cost of deployment . a sequential sensor deployment approach is then developed . the chapter illustrates that the overall cost of deployment can be minimized to achieve a desired detection performance by appropriately choosing the number of sensors deployed in each step of the sequential deployment strategy .
symmetry groups for beta lattices . <eos> we present a construction of symmetry plane groups for quasiperiodic point sets named beta lattices . the framework is issued from beta integers counting systems . beta lattices are vector superpositions of beta integers . when > <digit> is a quadratic pisot <unk> algebraic unit , the set of beta integers can be equipped with an abelian group structure and an internal multiplicative law . when we show that these arithmetic and algebraic structures lead to freely generated symmetry plane groups for beta lattices . these plane groups are based on repetitions of discrete adapted rotations and translations we shall refer to as beta rotations and beta translations . hence beta lattices , endowed with beta rotations and beta translations , can be viewed like lattices . the quasiperiodic function s ( n ) , defined on the set of beta integers as counting the number of small tiles between the origin and the nth beta integer , plays a central part in these new group structures . in particular , this function behaves asymptotically like a linear function . as an interesting consequence , beta lattices and their symmetries behave asymptotically like lattices and lattice symmetries , respectively .
mining the space of graph properties . <eos> existing data mining algorithms on graphs look for nodes satisfying specific properties , such as specific notions of structural similarity or specific measures of link based importance . while such analyses for predetermined properties can be effective in well understood domains , sometimes identifying an appropriate property for analysis can be a challenge , and focusing on a single property may neglect other important aspects of the data . in this paper , we develop a foundation for mining the properties themselves . we present a theoretical framework defining the space of graph properties , a variety of mining queries enabled by the framework , techniques to handle the enormous size of the query space , and an experimental system called f miner that demonstrates the utility and feasibility of property mining .
using simulated execution in verifying distributed algorithms . <eos> this paper presents a methodology for using simulated execution to assist a theorem prover in verifying safety properties of distributed systems . execution based techniques such as testing can increase confidence in an implementation , provide intuition about behavior , and detect simple errors quickly . they can not by themselves demonstrate correctness . however , they can aid theorem provers by suggesting necessary lemmas and providing tactics to structure proofs . this paper describes the use of these techniques in a machine checked proof of correctness of the paxos algorithm for distributed consensus .
fast , distributed approximation algorithms for positive linear programming with applications to flow control . <eos> we study combinatorial optimization problems in which a set of distributed agents must achieve a global objective using only local information . papadimitriou and yannakakis proceedings of the 25th acm symposium on theory of computing , <digit> , pp . <digit> <digit> initiated the study of such problems in a framework where distributed decision makers must generate feasible solutions to positive linear programs with information only about local constraints . we extend their model by allowing these distributed decision makers to perform local communication to acquire information over time and then explore the tradeoff between the amount of communication and the quality of the solution to the linear program that the decision makers can obtain . our main result is a distributed algorithm that obtains a ( <digit> approximation to the optimal linear programming solution while using only a polylogarithmic number of rounds of local communication . this algorithm offers a significant improvement over the logarithmic approximation ratio previously obtained by awerbuch and azar proceedings of the 35th annual ieee symposium on foundations of computer science , <digit> , pp . <digit> <digit> for this problem while providing a comparable running time . our results apply directly to the application of network flow control , an application in which distributed routers must quickly choose how to allocate bandwidth to connections using only local information to achieve global objectives . the sequential version of our algorithm is faster and considerably simpler than the best known approximation algorithms capable of achieving a ( <digit> approximation ratio for positive linear programming .
splitting a matrix of laurent polynomials with symmetry and its application to symmetric framelet filter banks . <eos> let m be a <digit> times <digit> matrix of laurent polynomials with real coefficients and symmetry . in this paper , we obtain a necessary and sufficient condition for the existence of four laurent polynomials ( or finite impulse response filters ) u1 , u2 , v1 , v2 with real coefficients and symmetry such that left begin matrix u <digit> ( z ) v <digit> ( z ) u <digit> ( z ) v <digit> ( z ) end matrix right left begin matrix u <digit> ( <digit> z ) u <digit> ( <digit> z ) v <digit> ( <digit> z ) v <digit> ( <digit> z ) end matrix right m ( z ) qquad forall z in cc bs <digit> and <unk> ( z ) sv2 ( z ) <unk> ( z ) sv1 ( z ) , where sp ( z ) p ( z ) p ( <digit> z ) for a nonzero laurent polynomial p . our criterion can be easily checked and a step by step algorithm will be given to construct the symmetric filters u1 , u2 , v1 , v2 . as an application of this result to symmetric framelet filter banks , we present a necessary and sufficient condition for the construction of a symmetric multiresolution analysis tight wavelet frame with two compactly supported generators derived from a given symmetric refinable function . once such a necessary and sufficient condition is satisfied , an algorithm will be used to construct a symmetric framelet filter bank with two high pass filters which is of interest in applications such as signal denoising and image processing . as an illustration of our results and algorithms in this paper , we give several examples of symmetric framelet filter banks with two high pass filters which have good vanishing moments and are derived from various symmetric low pass filters including some b spline filters .
inference of termination conditions for numerical loops in prolog . <eos> we present a new approach to termination analysis of numerical computations in logic programs . traditional approaches fail to analyse them due to non well foundedness of the integers . we present a technique that allows overcoming these difficulties . our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query mapping pairs with the well known framework of acceptability . such an integration not only contributes to the understanding of termination behaviour of numerical computations , but also allows us to perform a correct analysis of such computations automatically , by extending previous work on a constraint based approach to termination . finally , we discuss possible extensions of the technique , including incorporating general term orderings .
probabilistic symbolic model checking with prism a hybrid approach . <eos> in this paper we present efficient symbolic techniques for probabilistic model checking . these have been implemented in prism , a tool for the analysis of probabilistic models such as discrete time markov chains , continuous time markov chains and markov decision processes using specifications in the probabilistic temporal logics pctl and csl . motivated by the success of model checkers such as smv which use bdds ( binary decision diagrams ) , we have developed an implementation of pctl and csl model checking based on mtbdds ( multi terminal bdds ) and bdds . existing work in this direction has been hindered by the generally poor performance of mtbdd based numerical computation , which is often substantially slower than explicit methods using sparse matrices . the focus of this paper is a novel hybrid technique which combines aspects of symbolic and explicit approaches to overcome these performance problems . for typical examples , we achieve a dramatic improvement over the purely symbolic approach . in addition , thanks to the compact model representation using mtbdds , we can verify systems an order of magnitude larger than with sparse matrices , while almost matching or even beating them for speed .
test sequence generation and model checking using dynamic transition relations . <eos> the task of finding a set of test sequences that provides good coverage of industrial circuits is infeasible because of the size of the circuits . for small critical subcircuits of the design , however , designers can create a set of test sequences that achieve good coverage . these sequences can not be used on the full design because the inputs to the subcircuit may not be accessible . in this work we present an efficient test generation algorithm that receives a test sequence created for the subcircuit and finds a test sequence for the full design that reproduces the given sequence on the subcircuit . the algorithm uses a new technique called dynamic transition relations to increase its efficiency . the most common and most expensive step in our algorithm is the computation of the set of predecessors of a set of states . to make this computation more efficient we exploit a partitioning of the transition relation into a set of simpler relations . at every step we use only those that are necessary , resulting in a smaller relation than the original one . a different relation is used for each step , hence the name dynamic transition relations . the same idea can be used to improve symbolic model checking for the temporal logic ctl . we have implemented the new method in smv and run it on several large circuits . our experiments indicate that the new method can provide gains of up to two orders of magnitude in time and space during verification . these results show that dynamic transition relations can make it possible to verify circuits that were previously unmanageable due to their size and complexity .
on the performance of group key agreement protocols . <eos> group key agreement is a fundamental building block for secure peer group communication systems . several group key management techniques were proposed in the last decade , all assuming the existence of an underlying group communication infrastructure to provide reliable and ordered message delivery as well as group membership information . despite analysis , implementation , and deployment of some of these techniques , the actual costs associated with group key management have been poorly understood so far . this resulted in an undesirable tendency on the one hand , adopting suboptimal security for reliable group communication , while , on the other hand , constructing excessively costly group key management protocols . this paper presents a thorough performance evaluation of five notable distributed key management techniques ( for collaborative peer groups ) integrated with a reliable group communication system . an in depth comparison and analysis of the five techniques is presented based on experimental results obtained in actual local and wide area networks . the extensive performance measurement experiments conducted for all methods offer insights into their scalability and practicality . furthermore , our analysis of the experimental results highlights several observations that are not obvious from the theoretical analysis .
sizing router buffers . <eos> all internet routers contain buffers to hold packets during times of congestion . today , the size of the buffers is determined by the dynamics of tcp ' s congestion control algorithm . in particular , the goal is to make sure that when a link is congested , it is busy <digit> % of the time which is equivalent to making sure its buffer never goes empty . a widely used rule of thumb states that each link needs a buffer of size is the average round trip time of a flow passing across the link , and c is the data rate of the link . for example , a 10gb s router linecard needs approximately 250ms x <digit> . 5gbits of buffers and the amount of buffering grows linearly with the line rate . such large buffers are challenging for router manufacturers , who must use large , slow , off chip drams . and queueing delays can be long , have high variance , and may destabilize the congestion control algorithms . in this paper we argue that the rule of thumb now outdated and incorrect for backbone routers . this is because of the large number of flows ( tcp connections ) multiplexed together on a single backbone link . using theory , simulation and experiments on a network of real routers , we show that a link with n flows requires no more than long lived or short lived tcp flows . the consequences on router design are enormous a <digit> . 5gb s link carrying <digit> , <digit> flows could reduce its buffers by <digit> % with negligible difference in throughput and a 10gb s link carrying <digit> , <digit> flows requires only 10mbits of buffering , which can easily be implemented using fast , on chip sram .
a layered naming architecture for the internet . <eos> currently the internet has only one level of name resolution , dns , which converts user level domain names into ip addresses . in this paper we borrow liberally from the literature to argue that there should be three levels of name resolution from user level descriptors to service identifiers from service identifiers to endpoint identifiers and from endpoint identifiers to ip addresses . these additional levels of naming and resolution ( <digit> ) allow services and data to be first class internet objects ( in that they can be directly and persistently named ) , ( <digit> ) seamlessly accommodate mobility and multi homing and ( <digit> ) integrate middleboxes ( such as nats and firewalls ) into the internet architecture . we further argue that flat names are a natural choice for the service and endpoint identifiers . hence , this architecture requires scalable resolution of flat names , a capability that distributed hash tables ( dhts ) can provide .
style based inverse kinematics . <eos> this paper presents an inverse kinematics system based on a learned model of human poses . given a set of constraints , our system can produce the most likely pose satisfying those constraints , in real time . training the model on different input data leads to different styles of ik . the model is represented as a probability distribution over the space of all possible poses . this means that our ik system can generate any pose , but prefers poses that are most similar to the space of poses in the training data . we represent the probability with a novel model called a scaled gaussian process latent variable model . the parameters of the model are all learned automatically no manual tuning is required for the learning component of the system . we additionally describe a novel procedure for interpolating between styles . our style based ik can replace conventional ik , wherever it is used in computer animation and computer vision . we demonstrate our system in the context of a number of applications interactive character posing , trajectory keyframing , real time motion capture with missing markers , and posing from a 2d image .
parametric search made practical . <eos> in this paper we show that in sorting based applications of parametric search , quicksort can replace the parallel sorting algorithms that are usually advocated . because of the simplicity of quicksort , this may lead to applications of parametric search that are not only efficient in theory , but in practice as well . also , we argue that cole ' s optimization of certain parametric search algorithms may be unnecessary under realistic assumptions about the input . furthermore , we present a generic , flexible , and easy to use framework that greatly simplifies the implementation of algorithms based on parametric search . we use our framework to implement an algorithm that solves the <unk> distance problem . the implementation based on parametric search is faster than the binary search approach that is often suggested as a practical replacement for the parametric search technique .
minimizing end to end delay in high speed networks with a simple coordinated schedule . <eos> we study the problem of providing end to end delay guarantees in connection oriented networks . in this environment , multiple hop sessions coexist and interfere with one another . parekh and gallager showed that the weighted fair queueing ( wfq ) scheduling discipline provides a worst case delay guarantee comparable to ( <digit> i ) ki for a session with rate i and ki hops . such delays can occur since a session i packet can wait for time <digit> i at every hop . we describe a randomized work conserving scheme that guarantees , with high probability , an additive delay bound of approximately <digit> i ki . this bound is smaller than the multiplicative bound ( <digit> i ) ki of wfq , especially when the hop count ki is large . we call our scheme coordinated earliest deadline first ( cedf ) since it uses an earliest deadline first approach in which simple coordination is applied to the deadlines for consecutive hops of a session . the key to the bound is that once a packet has passed through its first server , it can pass through all its subsequent servers quickly . we conduct simulations to compare the delays actually produced by the two scheduling disciplines . in many cases , these actual delays are comparable to their analytical worst case bounds , implying that cedf outperforms wfq .
testing juntas . <eos> we show that a boolean valued function over n variables , where each variable ranges in an arbitrary probability space , can be tested for the property of depending on only j of them using a number of queries that depends only polynomially on j and the approximation parameter . we present several tests that require a number of queries that is polynomial in j and linear in <digit> . we <unk> non adaptive tests that has one sided error , an adaptive version of it that requires fewer queries , and a non adaptive two sided version of the test that requires the least number of queries . we also show a two sided non adaptive test that applies to functions over n boolean variables , and has a more compact analysis . we then provide a lower bound of ( j ) on the number of queries required for the nonadaptive testing of the above property a lower bound of ( log ( j <digit> ) ) for adaptive algorithms naturally follows from this . in establishing this lower bound we also prove a result about random walks on the group <unk> that may be interesting in its own right . we show that for some the distributions of the random walk at times t and t are close to each other , independently of the step distribution of the walk . we also discuss related questions . in particular , when given in advance a known j junta function h , we show how to test a function f for the property of being identical to h up to a permutation of the variables , in a number of queries that is polynomial in j and <digit> .
shrinkwrap an efficient adaptive algorithm for triangulating an iso surface . <eos> an algorithm is presented which generates a triangular mesh to approximate an iso surface . it starts with a triangulation of a sphere and next applies a series of deformations to this triangulation to transform it into the required surface . these deformations leave the topology invariant , so the final iso surface should be homeomorphic with a sphere . the algorithm is adaptive in the sense that the lengths of the sides of the triangles in the mesh vary with the local curvature of the underlying surface . a quantitative analysis of the accuracy of the algorithm is given along with an empirical comparison with earlier algorithms .
improving the static analysis of embedded languages via partial evaluation . <eos> programs in embedded languages contain invariants that are not automatically detected or enforced by their host language . we show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language . we demonstrate the effectiveness of this technique in improving the results of a value flow analysis .
increasing internet capacity using local search . <eos> open shortest path first ( ospf ) is one of the most commonly used intra domain internet routing protocol . traffic flow is routed along shortest paths , splitting flow evenly at nodes where several outgoing links are on shortest paths to the destination . the weights of the links , and thereby the shortest path routes , can be changed by the network operator . the weights could be set proportional to the physical lengths of the links , but often the main goal is to avoid congestion , i . e . overloading of links , and the standard heuristic recommended by cisco ( a major router vendor ) is to make the weight of a link inversely proportional to its capacity . we study the problem of optimizing ospf weights for a given a set of projected demands so as to avoid congestion . we show this problem is np hard , even for approximation , and propose a local search heuristic to solve it . we also provide worst case results about the performance of ospf routing vs . an optimal multi commodity flow routing . our numerical experiments compare the results obtained with our local search heuristic to the optimal multi commodity flow routing , as well as simple and commonly used heuristics for setting the weights . experiments were done with a proposed next generation at t worldnet backbone as well as synthetic internetworks .
a nonmonotonic observation logic . <eos> a variant of reiter ' s default logic is proposed as a logic for reasoning with ( defeasible ) observations . traditionally , default rules are assumed to represent generic information and the facts are assumed to represent specific information about the situation , but in this paper , the specific information derives from defeasible observations represented by ( normal free ) default rules , and the facts represent ( hard ) background knowledge . whenever the evidence underlying some observation is more refined than the evidence underlying another observation , this is modelled by means of a priority between the default rules representing the observations . we thus arrive at an interpretation of prioritized normal free default logic as an observation logic , and we propose a semantics for this observation logic . finally , we discuss how the proposed observation logic relates to the multiple extension problem and the problem of sensor fusion .
self stabilizing clock synchronization in the presence of byzantine faults . <eos> we initiate a study of bounded clock synchronization under a more severe fault model than that proposed by lamport and melliar smith <digit> . realistic aspects of the problem of synchronizing clocks in the presence of faults are considered . one aspect is that clock synchronization is an on going task , thus the assumption that some of the processors never fail is too optimistic . to cope with this reality , we suggest self stabilizing protocols that stabilize in any ( long enough ) period in which less than a third of the processors are faulty . another aspect is that the clock value of each processor is bounded . a single transient fault may cause the clock to reach the upper bound . therefore , we suggest a bounded clock that wraps around when appropriate . we present two randomized self stabilizing protocols for synchronizing bounded clocks in the presence of byzantine processor failures . the first protocol assumes that processors have a common pulse , while the second protocol does not . a new type of distributed counter based on the chinese remainder theorem is used as part of the first protocol .
plugging haskell in . <eos> extension languages enable users to expand the functionality of an application without touching its source code . commonly , these languages are dynamically typed languages , such as lisp , python , or domain specific languages , which support runtime plugins via dynamic loading of components . we show that haskell can be comfortably used as a statically typed extension language for both haskell and foreign language applications supported by the haskell ffi , and that it can perform type safe dynamic loading of plugins using dynamic types . moreover , we discuss how plugin support is especially useful to applications where haskell is used as an embedded domain specific language ( edsl ) . we explain how to realise type safe plugins using dynamic types , runtime compilation , and dynamic linking , exploiting infrastructure provided by the glasgow haskell compiler . we demonstrate the practicability of our approach with several applications that serve as running examples .
an event detection algebra for reactive systems . <eos> in reactive systems , execution is driven by external events to which the system should respond with appropriate actions . such events can be simple , but systems are often supposed to react to sophisticated situations involving a number of simpler events occurring in accordance with some pattern . a systematic approach to handle this type of systems is to separate the mechanism for detecting composite events from the rest of the application logic . in this paper , we present an event algebra for composite event detection . we show a number of algebraic laws that facilitate formal reasoning , and justify the algebra semantics by showing to what extent the operators comply with intuition . finally , we present an implementation of the algebra , and identify a large subset of expressions for which detection can be performed with bounded resources .
a methodology for generating verified combinatorial circuits . <eos> high level programming languages offer significant expressivity but provide little or no guarantees about resource use . resource bounded languages such as hardware description languages provide strong guarantees about the runtime behavior of computations but often lack mechanisms that allow programmers to write more structured , modular , and reusable programs . to overcome this basic tension in language design , recent work advocated the use of resource aware programming ( rap ) languages , which take into account the natural distinction between the development platform and the deployment platform for resource constrained software . this paper investigates the use of rap languages for the generation of combinatorial circuits . the key challenge that we encounter is that the rap approach does not safely admit a mechanism to express a posteriori ( post generation ) optimizations . the paper proposes and studies the use of abstract interpretation to overcome this problem . the approach is illustrated using an in depth analysis of the fast fourier transform ( fft ) . the generated computations are comparable to those generated by fftw .
adaptive offloading for pervasive computing . <eos> pervasive computing lets users continuously and consistently access an application on heterogeneous devices . however , delivering complex applications on resource constrained mobile devices such as cell phones is challenging . application or system based adaptations attempt to address the problem , but often at the cost of considerable degradation to application fidelity . the solution is to dynamically partition the application and offload part of the application execution data to a powerful nearby surrogate . this allows delivery of the application in a pervasive computing environment without significant fidelity degradation or expensive application rewriting . runtime offloading must adapt to different application execution patterns and resource fluctuations in the pervasive computing environment . this offloading inference engine adaptively solves two key decision making problems in runtime offloading timely triggering of offloading and efficient partitioning of applications . both trace driven simulations and prototype experiments confirm the effectiveness of this adaptive offloading system .
an experimental comparison of min cut max flow algorithms for energy minimization in vision . <eos> after <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , minimum cut maximum flow algorithms on graphs emerged as an increasingly useful tool for exact or approximate energy minimization in low level vision . the combinatorial optimization literature provides many min cut max flow algorithms with different polynomial time complexity . their practical efficiency , however , has to date been studied mainly outside the scope of computer vision . the goal of this paper is to provide an experimental comparison of the efficiency of min cut max flow algorithms for applications in vision . we compare the running times of several standard algorithms , as well as a new algorithm that we have recently developed . the algorithms we study include both goldberg tarjan style push relabel methods and algorithms based on ford fulkerson style augmenting paths . we benchmark these algorithms on a number of typical graphs in the contexts of image restoration , stereo , and segmentation . in many cases , our new algorithm works several times faster than any of the other methods , making near real time performance possible . an implementation of our max flow min cut algorithm is available upon request for research purposes .
towards proving strong direct product theorems . <eos> a fundamental question of complexity theory is the direct product question . a famous example is yao ' s xor lemma , in which one assumes that some function f is hard on average for small circuits ( meaning that every circuit of some fixed size s which attempts to compute f is wrong on a non negligible fraction of the inputs ) and concludes that every circuit of size s ' only has a small advantage over guessing randomly when computing . . . f ( xk ) on independently chosen x1 , . . . , xk . all known proofs of this lemma have the property that s ' < s . in words , the circuit which attempts to compute fk is smaller than the circuit which attempts to compute f on a single input this paper addresses the issue of proving strong direct product assertions , that is , ones in which s ' ks and is in particular larger than s . we study the question of proving strong direct product question for decision trees and communication protocols .
the complexity of synchronous iterative do all with crashes . <eos> the ability to cooperate on common tasks in a distributed setting is key to solving a broad range of computation problems ranging from distributed search such as seti to distributed simulation and multi agent collaboration . do all , an abstraction of such cooperative activity , is the problem of performing n tasks in a distributed system of p failure prone processors . many distributed and parallel algorithms have been developed for this problem and several algorithm simulations have been developed by iterating do all algorithms . the efficiency of the solutions for do all is measured in terms of work complexity where all processing steps taken by all processors are counted . work is ideally expressed as a function of n , p , and f , the number of processor crashes . however the known lower bounds and the upper bounds for extant algorithms do not adequately show how work depends on f . we present the first non trivial lower bounds for do all that capture the dependence of work on n , p and f . for the model of computation where processors are able to make perfect load balancing decisions locally , we also present matching upper bounds . we define the r iterative do all problem that abstracts facts the repeated use of do all such as found in typical algorithm simulations . our f sensitive analysis enables us to derive tight bounds for r iterative do all work ( that are stronger than the r fold work complexity of a single do all ) . our approach that models perfect load balancing allows for the analysis of specific algorithms to be divided into two parts ( i ) the analysis of the cost of tolerating failures while performing work under free load balancing , and ( ii ) the analysis of the cost of implementing load balancing . we demonstrate the utility and generality of this approach by improving the analysis of two known efficient algorithms . we give an improved analysis of an efficient message passing algorithm . we also derive a tight and complete analysis of the best known do all algorithm for the synchronous shared memory model . finally we present a new upper bound on simulations of synchronous shared memory algorithms on crash prone processors .
nested semantics over finite trees are equationally hard . <eos> this paper studies nested simulation and nested trace semantics over the language bccsp , a basic formalism to express finite process behaviour . it is shown that none of these semantics affords finite ( in ) equational axiomatizations over bccsp . in particular , for each of the nested semantics studied in this paper , the collection of sound , closed ( in ) equations over a singleton action set is not finitely based .
resolution for label based formulas in hierarchical representation . <eos> order sorted logic <unk> many and partially ordered sorts as a sort hierarchy . in the field of knowledge representation and reasoning , it is useful to develop reasoning systems fbr terminological knowledge , together with assertional knowledge . however , the expression of sort hierarchies can not sufficiently capture the lexical diversity of terminological knowledge . in addition to sorts , various kinds of symbols constants , functions and predicates are semantically and hierarchically associated with each other . this is because natural language words identifying these symbols can be employed in the description of terminological knowledge . in this paper , we present a label based language for consistently handling the variety of hierarchical relationships among symbol names . for this language we develop a sorted resolution system whose reasoning power is enhanced by adding hierarchical inference rules with labeled substitutions .
unit disk graph approximation . <eos> finding a good embedding of a unit disk graph given by its connectivity information is a problem of practical importance in a variety of fields . in wireless ad hoc and sensor networks , such an embedding can be used to obtain virtual coordinates . in this paper , we prove a non approximability result for the problem of embedding a given unit disk graph . particularly , we show that if non neighboring nodes are not allowed to be closer to each other than distance <digit> , then two neighbors can be as far apart as <digit> <digit> , where goes to <digit> as n goes to infinity , unless p np . we further show that finding a realization of a d quasi unit disk graph with d <digit> <digit> is np hard .
debugging larch shared language specifications . <eos> the checkability designed into the lsl ( larch shared language ) is described , and two tools that help perform the checking are discussed . lp ( the larch power ) is the principal debugging tool . its design and development have been motivated primarily by work on lsl , but it also has other uses ( e . g . reasoning about circuits and concurrent algorithms ) . because of these other uses , and because they also tend to use lp to analyze larch interface specifications , the authors have tried not to make lp too lsl specific . instead , they have chosen to build a second tool , <unk> ( the lsl checker ) , to serve as a front end to lp . <unk> checks the syntax and static semantics of lsl specifications and generates lp proof obligations from their claims . these proof obligations fall into three categories consistency ( that a specification does not contradict itself ) , theory containment ( that a specification has intended consequences ) , and relative completeness ( that a set of operators is adequately defined ) . an extended example illustrating how lp is used to debug lsl specifications is presented .
exploiting medium access diversity in rate adaptive wireless lans . <eos> recent years have seen the growing popularity of multi rate wireless network devices ( e . g . , <digit> . 11a cards ) that can exploit variations in channel conditions and improve overall network throughput . concurrently , rate adaptation schemes have been developed that selectively increase data transmissions on a link when it offers good channel quality . in this paper , we propose a medium access diversity ( mad ) scheme that leverages the benefits of rate adaptation schemes by aggressively exploiting multiuser diversity . the basic mechanism of mad is to obtain instantaneous channel condition information from multiple receivers and selectively transmit data to a receiver that improves the overall throughput of the network , while maintaining temporal fairness among multiple data flows . we identify and address the challenges in the design and implementation of mad ' s three phases channel probing , data transmission , and receiver scheduling . we also use analytical models to examine the tradeoff between network performance improvement and overhead of channel probing , and derive an asymptotic performance bound for the receiver scheduling algorithms used by mad . results from the analysis and the extensive simulations demonstrate that , on average , mad can improve the overall throughput of ieee <digit> . <digit> wireless lans by <digit> % as compared with the best existing rate adaptation scheme .
arithmetic circuits and polynomial replacement systems . <eos> this paper addresses the problems of counting proof trees ( as introduced by venkateswaran and tompa ) and counting proof circuits , a related but seemingly more natural question . these problems lead to a common generalization of straight line programs which we call polynomial replacement systems prss . we contribute a classification of these systems and we investigate their complexity . diverse problems falling within the scope of this study include , for example , counting proof circuits and evaluating cup , circuits over the natural numbers . a number of complexity results are obtained , including a proof that counting proof circuits is <unk> complete .
tight bounds for testing bipartiteness in general graphs . <eos> in this paper we consider the problem of testing bipartiteness of general graphs . the problem has previously been studied in two models , one most suitable for dense graphs and one most suitable for bounded degree graphs . roughly speaking , dense graphs can be tested for bipartiteness with constant complexity , while the complexity of testing bounded degree graphs is tilde theta ( sqrt n ) , where n is the number of vertices in the graph ( and tilde theta ( f ( n ) ) means theta ( f ( n ) cdot rm polylog ( f ( n ) ) ) ) . thus there is a large gap between the complexity of testing in the two cases . in this work we bridge the gap described above . in particular , we study the problem of testing bipartiteness in a model that is suitable for all densities . we present an algorithm whose complexity is tilde o ( min ( sqrt n , n <digit> m ) ) , where m is the number of edges in the graph , and we match it with an almost tight lower bound .
a column approximate minimum degree ordering algorithm . <eos> sparse gaussian elimination with partial pivoting computes the factorization of a sparse matrix a , where the row ordering p is selected during factorization using standard partial pivoting with row interchanges . the goal is to select a column preordering , q , based solely on the nonzero pattern of a , that limits the worst case number of nonzeros in the factorization . the fill in also depends on p , but q is selected to reduce an upper bound on the fill in for any subsequent choice of p . the choice of q can have a dramatic impact on the number of nonzeros in l and u . one scheme for determining a good column ordering for a is to compute a symmetric ordering that reduces fill in in the cholesky factorization of ata . a conventional minimum degree ordering algorithm would require the sparsity structure of ata to be computed , which can be expensive both in terms of space and time since ata may be much denser than a . an alternative is to compute q directly from the sparsity structure of a this strategy is used by matlab ' s colmmd preordering algorithm . a new ordering algorithm , colamd , is presented . it is based on the same strategy but uses a better ordering heuristic . colamd is faster and computes better orderings , with fewer nonzeros in the factors of the matrix .
rounding algorithms for a geometric embedding of minimum multiway cut . <eos> given an undirected graph with edge costs and a subset ofk <digit> nodes <unk> , a multiway , ork way , cut is a subset of the edges whose removal disconnects each terminal from the others . the multiway cut problem is to find a minimum cost multiway cut . this problem is max snp hard . recently , calinescu et al . ( calinescu , g . , h . karloff , y . rabani . <digit> . an improved approximation algorithm for multiway cut . j . comput . system sci . <digit> ( <digit> ) <digit> <digit> ) gave a novel geometric relaxation of the problem and a rounding scheme that produced a ( <digit> <digit> <digit> k ) approximation algorithm . in this paper , we study their geometric relaxation . in particular , we study the worst case ratio between the value of the relaxation and the value of the minimum multicut ( the so called integrality gap of the relaxation ) . fork <digit> , we show the integrality gap is <digit> <digit> , giving tight upper and lower bounds . that is , we exhibit a family of graphs with integrality gaps arbitrarily close to <digit> <digit> and give an algorithm that finds a cut of value <digit> <digit> times the relaxation value . our lower bound shows that this is the best possible performance guarantee for any algorithm based purely on the value of the relaxation . our upper bound meets the lower bound and improves the factor of <digit> <digit> shown by calinescu et al . for allk , we show that there exists a rounding scheme with performance ratio equal to the integrality gap , and we give explicit constructions of polynomial time rounding schemes that lead to improved upper bounds . fork <digit> and <digit> , our best upper bounds are based on computer constructed rounding schemes ( with computer proofs of correctness ) . for <unk> we give an algorithm with performance ratio <digit> . <digit> e k . our results were discovered with the help of computational experiments that we also describe here .
hard equality constrained integer knapsacks . <eos> we consider the following integer feasibility problem given positive integer <unk> , a1 , . . . , a n , with gcd ( a1 , . . . , a n does there exist a <unk> z n <unk> x a0 we prove that if the <unk> , . . . , a <unk> a certain decomposable structure , then the frobenius number associated <unk> , . . . , a n , i . e . , the largest value <unk> for <unk> x a0 does not have a nonnegative integer solution , is close to a known upper bound . in the instances we consider , we <unk> to be the frobenius number . furthermore , we show that the decomposable structure <unk> , . . . , a <unk> the solution of a lattice reformulation of our problem almost trivial , since the number of lattice hyperplanes that intersect the polytope resulting from the reformulation in the direction of the last coordinate is going to be very small . for branch and bound such instances are difficult to solve , since they are infeasible and have large values <unk> a i , <digit> i n . we illustrate our results by some computational examples .
pictorial structures for object recognition . <eos> in this paper we present a computationally efficient framework for part based modeling and recognition of objects . our work is motivated by the pictorial structure models introduced by fischler and elschlager . the basic idea is to represent an object by a collection of parts arranged in a deformable configuration . the appearance of each part is modeled separately , and the deformable configuration is represented by spring like connections between pairs of parts . these models allow for qualitative descriptions of visual appearance , and are suitable for generic recognition problems . we address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples , presenting efficient algorithms in both cases . we demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images .
constraint programming viewed as rule based programming . <eos> we study here a natural situation when constraint programming can be entirely reduced to rule based programming . to this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first order formulas . then we consider constraint satisfaction problems that are based on predefined , explicitly given constraints . to solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules , combined with labeling . we consider two types of rule here . the first type , that we call equality rules , leads to a new notion of local consistency , called rule consistency that turns out to be weaker than arc consistency for constraints of arbitrary arity ( called hyper arc consistency in marriott stuckey ( <digit> ) ) . for boolean constraints rule consistency coincides with the closure under the well known propagation rules for boolean constraints . the second type of rules , that we call membership rules , yields a rule based characterization of arc consistency . to show feasibility of this rule based approach to constraint programming , we show how both types of rules can be automatically generated , as chr rules of <unk> ( <digit> ) . this yields an implementation of this approach to programming by means of constraint logic programming . we illustrate the usefulness of this approach to constraint programming by discussing various examples , including boolean constraints , two typical examples of many valued logics , constraints dealing with waltz ' s language for describing polyhedral scenes , and allen ' s qualitative approach to temporal logic .
typing constraint logic programs . <eos> we present a prescriptive type system with parametric polymorphism and subtyping for constraint logic programs . the aim of this type system is to detect programming errors statically . it introduces a type discipline for constraint logic programs and modules , while maintaining the capabilities of performing the usual coercions between constraint domains , and of typing meta programming predicates , thanks to the exibility of subtyping . the property of subject reduction expresses the consistency of a prescriptive type system w . r . t . the execution model if a program is well typed , then all derivations starting from a well typed goal are again well typed . that property is proved w . r . t . the abstract execution model of constraint programming which proceeds by accumulation of constraints only , and w . r . t . an enriched execution model with type constraints for substitutions . we describe our implementation of the system for type checking and type inference . we report our experimental results on type checking iso prolog , the ( constraint ) libraries of sicstus prolog and other prolog programs .
a theory of normed simulations . <eos> in existing simulation proof techniques , a single step in a lower level specification may be simulated by an extended execution fragment in a higher level one . as a result , it is cumbersome to mechanize these techniques using general purpose theorem provers . moreover , it is undecidable whether a given relation is a simulation , even if tautology checking is decidable for the underlying specification logic . this article studies various types of < i > normed simulations < i > . in a normed simulation , each step in a lower level specification can be simulated by at most one step in the higher level one , for any related pair of states . in earlier work we demonstrated that normed simulations are quite useful as a vehicle for the formalization of refinement proofs via theorem provers . here we show that normed simulations also have pleasant theoretical properties ( <digit> ) under some reasonable assumptions , it is decidable whether a given relation is a normed forward simulation , provided tautology checking is decidable for the underlying logic ( <digit> ) at the semantic level , normed forward and backward simulations together form a complete proof method for establishing behavior inclusion , provided that the higher level specification has finite invisible nondeterminism .
a statistical analysis of the long run node spatial distribution in mobile ad hoc networks . <eos> in this paper , we analyze the node spatial distribution of mobile wireless ad hoc networks . characterizing this distribution is of fundamental importance in the analysis of many relevant properties of mobile ad hoc networks , such as connectivity , average route length , and network capacity . in particular , we have investigated under what conditions the node spatial distribution resulting after a large number of mobility steps resembles the uniform distribution . this is motivated by the fact that the existing theoretical results concerning mobile ad hoc networks are based on this assumption . in order to test this hypothesis , we performed extensive simulations using two well known mobility models the random waypoint model , which resembles intentional movement , and a brownian like model , which resembles nonintentional movement . our analysis has shown that in brownian like motion the uniformity assumption does hold , and that the intensity of the concentration of nodes in the center of the deployment region that occurs in the random waypoint model heavily depends on the choice of some mobility parameters . for extreme values of these parameters , the uniformity assumption is impaired .
adaptive dissemination of data in time critical asymmetric communication environments . <eos> the proliferation of new data intensive applications in asymmetric communication environments has led to an increasing interest in the development of push based techniques , in which the information is broadcast to a large population of clients in order to achieve the most efficient use of the limited server and communication resources . it is important to note that quite often the data that is broadcast is time critical in nature . most of the related current research focuses on a pure push based approach ( broadcast disks model ) , where the transmission of data is done without allowing explicit requests from the users . more recently , some bidirectional models incorporating a low capacity uplink channel have been proposed in order to increase the functionality of the broadcast disks model . however , the impact of integration of the uplink channel has been investigated using only static client profiles or ignoring the existence of time sensitive data . none of the existing models integrates all the characteristics needed to perform effectively in a real world , dynamic time critical asymmetric communication environment . in this paper we present an adaptive data dissemination model and the associated on line scheduling algorithms . these improve the functionality and performance of bidirectional broadcast models , maximizing the total number of satisfied users in asymmetric communication environments with dynamic client profiles and time requirements ( e . g . , mobile systems ) . this is achieved by means of dynamic adaptation of the broadcast program to the needs of the users , taking into account the bandwidth constraints inherent in asymmetric communication environments and the deadline requirements of the user requests . performance is evaluated by simulation of a real time asymmetric communication environment
wait free synchronization . <eos> a wait free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps , regardless of the execution speeds of the other processes . the problem of constructing a wait free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms , concurrent data structures , and multiprocessor architectures . first , we introduce a simple and general technique , based on reduction to a concensus protocol , for proving statements of the form , there is no wait free implementation of x by y . we derive a hierarchy of objects such that no object at one level has a wait free implementation in terms of objects at lower levels . in particular , we show that atomic read write registers , which have been the focus of much recent attention , are at the bottom of the hierarchy <unk> can not be used to construct wait free implementations of many simple and familiar data types . moreover , classical synchronization primitives such <unk> set and fetch add , while more powerful than read and write , are also computationally weak , as are the standard message passing primitives . second , nevertheless , we show that there do exist simple universal objects from which one can construct a wait free implementation of any sequential object .
short signatures from the weil pairing . <eos> we introduce a short signature scheme based on the computational diffiehellman assumption on certain elliptic and hyperelliptic curves . for standard security parameters , the signature length is about half that of a dsa signature with a similar level of security . our short signature scheme is designed for systems where signatures are typed in by a human or are sent over a low bandwidth channel . we survey a number of properties of our signature scheme such as signature aggregation and batch verification .
towards provable security for ad hoc routing protocols . <eos> we propose a formal framework for the security analysis of on demand source routing protocols for wireless ad hoc networks . our approach is based on the well known simulation paradigm that has been proposed to prove the security of cryptographic protocols . our main contribution is the application of the simulation based approach in the context of ad hoc routing . this involves a precise definition of a real world model , which describes the real operation of the protocol , and an ideal world model , which captures what the protocol wants to achieve in terms of security . both models take into account the peculiarities of wireless communications and ad hoc routing . then , we give a formal definition of routing security in terms of indistinguishability of the two models from the point of view of honest parties . we demonstrate the usefulness of our approach by analyzing two secure ad hoc routing protocols , srp and ariadne . this analysis leads to the discovery of as yet unknown attacks against both protocols . finally , we propose a new ad hoc routing protocol and prove it to be secure in our model .
location diversity in anonymity networks . <eos> anonymity networks have long relied on diversity of node location for protection against attacks typically an adversary who can observe a larger fraction of the network can launch a more effective attack . we investigate the diversity of two deployed anonymity networks , mixmaster and tor , with respect to an adversary who controls a single internet administrative domain . specifically , we implement a variant of a recently proposed technique that passively estimates the set of administrative domains ( also known as autonomous systems , or ases ) between two arbitrary end hosts without having access to either end of the path . using this technique , we analyze the as level paths that are likely to be used in these anonymity networks . we find several cases in each network where multiple nodes are in the same administrative domain . further , many paths between nodes , and between nodes and popular endpoints , traverse the same domain .
( optimal ) duplication is not elementary recursive . <eos> in <digit> , asperti and mairson proved that the cost of reducing a term using an optimal reducer ( a la <unk> ) can not be bound by any elementary function in the number of shared beta steps . we prove in this paper that an analogous result holds for lamping ' s abstract algorithm . that is , there is no elementary function in the number of shared beta steps bounding the number of duplication steps of the optimal reducer . this theorem vindicates the oracle of lamping ' s algorithm as the culprit for the negative result of asperti and mairson . the result is obtained using as a technical tool elementary affine logic .
on learning monotone dnf under product distributions . <eos> we show that the class of monotone <digit> < sup > o ( log < i > n < i > ) < sup > term dnf formulae can be pac learned in polynomial time under the uniform distribution from random examples only . this is an exponential improvement over the best previous polynomial time algorithms in this model , which could learn monotone o ( log < sup > <digit> < sup > < i > n < i > ) term dnf . we also show that various classes of small constant depth circuits which compute monotone functions are pac learnable in polynomial time under the uniform distribution . all of our results extend to learning under any constant bounded product distribution .
efficient incremental algorithms for dynamic detection of likely invariants . <eos> dynamic detection of likely invariants is a program analysis that generalizes over observed values to hypothesize program properties . the reported program properties are a set of likely invariants over the program , also known as an operational abstraction . operational abstractions are useful in testing , verification , bug detection , refactoring , comparing behavior , and many other tasks . previous techniques for dynamic invariant detection scale poorly or report too few properties . incremental algorithms are attractive because they process each observed value only once and thus scale well with data sizes . previous incremental algorithms only checked and reported a small number of properties . this paper takes steps toward correcting this problem . the paper presents two new incremental algorithms for invariant detection and compares them analytically and experimentally to two existing algorithms . furthermore , the paper presents four optimizations and shows how to implement them in the context of incremental algorithms . the result is more scalable invariant detection that does not sacrifice functionality .
reusable cryptographic fuzzy extractors . <eos> we show that a number of recent definitions and constructions of fuzzy extractors are not adequate for multiple uses of the same fuzzy secret a major shortcoming in the case of biometric applications . we propose two particularly stringent security models that specifically address the case of fuzzy secret reuse , respectively from an outsider and an insider perspective , in what we call a chosen perturbation attack . we characterize the conditions that fuzzy extractors need to satisfy to be secure , and present generic constructions from ordinary building blocks . as an illustration , we demonstrate how to use a biometric secret in a remote fuzzy authentication protocol that does not require any storage on the client ' s side .
id based encryption for complex hierarchies with applications to forward security and broadcast encryption . <eos> a forward secure encryption scheme protects secret keys from exposure by evolving the keys with time . forward security has several unique requirements in hierarchical identity based encryption ( hibe ) scheme ( <digit> ) users join dynamically ( <digit> ) encryption is joining time oblivious ( <digit> ) users evolve secret keys autonomously . we present a scalable forward secure hibe ( fs hibe ) scheme satisfying the above properties . we also show how our fs hibe scheme can be used to construct a forward secure public key broadcast encryption scheme , which protects the secrecy of prior transmissions in the broadcast encryption setting . we further generalize fs hibe into a collusion resistant multiple hierarchical id based encryption scheme , which can be used for secure communications with entities having multiple roles in role based access control . the security of our schemes is based on the bilinear diffie hellman assumption in the random oracle model .
models of software development environments . <eos> a general model of software development environments that consists of structures , mechanisms , and policies is presented . the advantage of this model is that it distinguishes intuitively those aspects of an environment that are useful in comparing and contrasting software development environments . four classes of environments the individual , the family , the city . and the state are characterized by means of a sociological metaphor based on scale . the utility of the taxonomy is that it delineates the important classes of interactions among software developers and exposes the ways in which current software development environments inadequately support the development of large systems . the generality of the model is demonstrated by its application to a previously published taxonomy that categorizes environments according to how they relate to language centered , structure oriented , toolkit , and method based environments .
reliable solution of special event location problems for odes . <eos> computing the solution of the initial value problem in ordinary differential equations ( odes ) may be only part of a larger task . one such task is finding where an algebraic function of the solution ( an event function ) has a root ( an event occurs ) . this is a task which is difficult both in theory and in software practice . for certain useful kinds of event functions , it is possible to avoid two fundamental difficulties . it is described how to achieve the reliable solutions of such problems in a way that allows the capability to be grafted onto popular codes for the initial value problem .
mixed finite element approximation of incompressible mhd problems based on weighted regularization . <eos> we introduce and analyze a new mixed finite element method for the numerical approximation of stationary incompressible magneto hydrodynamics ( mhd ) problems in polygonal and polyhedral domains . the method is based on standard inf sup stable elements for the discretization of the hydrodynamic unknowns and on nodal elements for the discretization of the magnetic variables . in order to achieve convergence in non convex domains , the magnetic bilinear form is suitably modified using the weighted regularization technique recently developed in numer . math . <digit> ( <digit> ) <digit> . we first discuss the well posedness of this approach and establish a novel existence and uniqueness result for non linear mhd problems with small data . we then derive quasi optimal error bounds for the proposed finite element method and show the convergence of the approximate solutions in non convex domains . the theoretical results are confirmed in a series of numerical experiments for a linear two dimensional oseen type mhd problem , demonstrating that weighted regularization is indispensable for the resolution of the strongest magnetic singularities .
i o efficient dynamic planar point location . <eos> we present an i o efficient dynamic data structure for point location in a general planar subdivision . our structure uses o ( < i > n b < i > ) disk blocks of size < i > b < i > to store a subdivision of size < i > n < i > . queries can be answered in o ( log < inf > < i > b < i > < inf > < sup > <digit> < sup > < i > n < i > ) i os in the worst case , and insertions and deletions can be performed in o ( log < inf > < i > b < i > < inf > < sup > <digit> < sup > < i > n < i > ) and o ( log < inf > < i > b < i > < inf > < i > n < i > ) i os amortized , respectively . part of our data structure is based on an external version of the so called logarithmic method that allows for efficient dynamization of static external memory data structures with certain characteristics . another important part of our structure is an external data structure for vertical ray shooting among line segments in the plane with endpoints on < i > b < i > lines , developed using an external version of dynamic fractional cascading . we believe that these methods could prove helpful in the development of other dynamic external memory data structures .
on relations between counting communication complexity classes . <eos> we develop upper and lower bound arguments for counting acceptance modes of communication protocols . a number of separation results for counting communication complexity classes is established . this extends the investigation of the complexity of communication between two processors in terms of complexity classes initiated by babai et al . ( proceedings of the 27th ieee focs , <digit> , pp . <digit> <digit> ) and continued in several papers ( e . g . , j . comput . system sci . <digit> ( <digit> ) <digit> <digit> ( <digit> ) <digit> proceedings of the 36th ieee focs , <digit> , pp . <digit> <digit> ) . in particular , it will be shown that for all pairs of distinct primes < i > p < i > and < i > q < i > the communication complexity classes mod < inf > < i > p < i > < inf > < i > p < i > < sup > cc < sup > and mod < inf > < i > q < i > < inf > < i > p < i > < sup > cc < sup > are incomparable with regard to inclusion . the same is true for pp < sup > cc < sup > and mod < inf > < i > m < i > < inf > < i > p < i > < sup > cc < sup > , for any number < i > m < i > <digit> . moreover , non determinism and modularity are incomparable to a large extend . on the other hand , if < i > m < i > < i > p < i > < inf > <digit> < inf > < i > l < i > < inf > <digit> < inf > ' . . . ' < i > p < i > < inf > r < inf > < i > l < i > < inf > r < inf > is the prime decomposition of < i > m < i > <digit> , then the complexity classes mod < inf > < i > m < i > < inf > < i > p < i > < sup > cc < sup > and mod < inf > < i > ( m ) < i > < inf > < i > p < i > < sup > cc < sup > coincide , where < i > ( m ) < i > < i > p < i > < inf > <digit> < inf > ' . . . ' < i > p < i > < inf > r < inf > . the results are obtained by characterizing the modular and probabilistic communication complexity in terms of the minimum rank of matrices ranging over certain equivalence classes . methods from algebra and analytic geometry are used . this paper is the completely revised and strongly extended version of the conference paper damm et al . ( proc . 9th ann . stacs , pp . <digit> <digit> ) where a subset of the results was presented .
dynamically adapting registration areas to user mobility and call patterns for efficient location management in pcs networks . <eos> in this paper , we propose an extension to the personal communication services ( pcs ) location management protocol which uses dynamically overlapped registration areas . the scheme is based on monitoring the aggregate mobility and call pattern of the users during each reconfiguration period and adapting to the mobility and call patterns by either expanding or shrinking registration areas at the end of each reconfiguration period . we analytically characterize the trade off resulting from the inclusion or exclusion of a cell in a registration area in terms of expected change in aggregate database access cost and signaling overhead . this characterization is used to guide the registration area adaption in a manner in which the signaling and database access load on any given location register ( lr ) does not exceed a specified limit . our simulation results show that it is useful to dynamically adapt the registration areas to the aggregate mobility and call patterns of the mobile units when the mobility pattern exhibits locality . for such mobility and call patterns , the proposed scheme can greatly reduce the average signaling and database access load on lrs . further , the cost of adapting the registration areas is shown to be low in terms of memory and communication requirements .
software trace cache . <eos> this paper explores the use of compiler optimizations which optimize the layout of instructions in memory . the target is to enable the code to make better use of the underlying hardware resources regardless of the specific details of the processor architecture in order to increase fetch performance . the software trace cache ( stc ) is a code layout algorithm with a broader target than previous layout optimizations . we target not only an improvement in the instruction cache hit rate , but also an increase in the effective fetch width of the fetch engine . the stc algorithm organizes basic blocks into chains trying to make sequentially executed basic blocks reside in consecutive memory positions , then maps the basic block chains in memory to minimize conflict misses in the important sections of the program . we evaluate and analyze in detail the impact of the stc , and code layout optimizations in general , on the three main aspects of fetch performance the instruction cache hit rate , the effective fetch width , and the branch prediction accuracy . our results show that layout optimized codes have some special characteristics that make them more amenable for high performance instruction they have a very high rate of not taken branches and execute long chains of sequential instructions also , they make very effective use of instruction cache lines , mapping only useful instructions which will execute close in time , increasing both spatial and temporal locality .
random walks on truncated cubes and sampling <digit> <digit> knapsack solutions . <eos> we solve an open problem concerning the mixing time of symmetric random walk on the n dimensional cube truncated by a hyperplane , showing that it is polynomial in n . as a consequence , we obtain a fully polynomial randomized approximation scheme for counting the feasible solutions of a <digit> <digit> knapsack problem . the results extend to the case of any fixed number of hyperplanes . the key ingredient in our analysis is a combinatorial construction we call a balanced almost uniform permutation , which is of independent interest .
detecting global predicates in distributed systems with clocks . <eos> this paper proposes a framework for detecting global state predicates in systems of processes with approximately synchronized real time clocks . timestamps from these clocks are used to define two orderings on events definitely occurred before and possibly occurred before . these orderings lead naturally to definitions of <digit> distinct detection modalities , i . e . , <digit> meanings of predicate held during a computation , namely poss < sup > < i > db < i > < sup > ( possibly held ) , definitely held ) , and inst ( definitely held in a specific global state ) . this paper defines these modalities and gives efficient algorithms for detecting them . the algorithms are based on algorithms of garg and waldecker , alagar and venkatesan , cooper and marzullo , and fromentin and raynal . complexity analysis shows that under reasonable assumptions , these real time clock based detection algorithms are less expensive than detection algorithms based on lamport ' s happened before ordering . sample applications are given to illustrate the benefits of this approach .
superstabilizing mutual exclusion . <eos> a superstabilizing protocol is a protocol that ( i ) is self stabilizing , meaning that it can recover from an arbitrarily severe transient fault and ( ii ) can recover from a local transient fault while satisfying a passage predicate during recovery . this paper investigates the possibility of superstabilizing protocols for mutual exclusion in a ring of processors , where a local fault consists of any transient fault at a single processor the passage predicate specifies that there be at most one token in the ring , with the single exception of a spurious token colocated with the transient fault . the first result of the paper is an impossibility theorem for a class of superstabilizing mutual exclusion protocols . two unidirectional protocols are then presented to show that conditions for impossibility can independently be relaxed so that superstabilization is possible using either additional time or communication registers . a bidirectional protocol subsequently demonstrates that superstabilization in < i > o < i > ( <digit> ) time is possible . all three superstabilizing protocols are optimal with respect to the number of communication registers used .
secure reliable multicast protocols in a wan . <eos> a secure reliable multicast protocol enables a process to send a message to a group of recipients such that all correct destinations receive the same message , despite the malicious efforts of fewer than a third of the total number of processes , including the sender . this has been shown to be a useful tool in building secure distributed services , albeit with a cost that typically grows linearly with the size of the system . for very large networks , for which this is prohibitive , we present two approaches for reducing the cost first , we show a protocol whose cost is on the order of the number of tolerated failures . secondly , we show how relaxing the consistency requirement to a probabilistic guarantee can reduce the associated cost , effectively to a constant .
communication based prevention of useless checkpoints in distributed computations . <eos> a useless checkpoint is a local checkpoint that can not be part of a consistent global checkpoint . this paper addresses the following problem . given a set of processes that take ( basic ) local checkpoints in an independent and unknown way , the problem is to design communication induced checkpointing protocols that direct processes to take additional local ( forced ) checkpoints to ensure no local checkpoint is useless . the paper first proves two properties related to integer timestamps which are associated with each local checkpoint . the first property is a necessary and sufficient condition that these timestamps must satisfy for no checkpoint to be useless . the second property provides an easy timestamp based determination of consistent global checkpoints . then , a general communication induced checkpointing protocol is proposed . this protocol , derived from the two previous properties , actually defines a family of timestamp based communication induced checkpointing protocols . it is shown that several existing checkpointing protocols for the same problem are particular instances of the general protocol . the design of this general protocol is motivated by the use of communication induced checkpointing protocols in consistent global checkpoint based distributed applications such as the detection of stable or unstable properties and the determination of distributed breakpoints .
constraint based structuring of network protocols . <eos> the complexity of designing protocols has led to compositional techniques for designing and verifying protocols . we propose a technique based on the notion of parallel composition of protocols . we view a composite protocol as an interleaved execution of the component protocols subject to a set of constraints . using the constraints as building blocks , we define several constraint based structures with each structure combining the properties of the component protocols in a different way . for instance , the component protocols of a multifunction protocol can be structured so that the composite protocol performs all the individual functions concurrently or performs only one of them depending on the order of initiation of the component protocols . we provide inference rules to infer safety and liveness properties of the composite protocol . some properties are derived from those of the component protocols while others are derived from the structuring mechanism ( the set of constraints ) used to combine the component protocols .
byzantine quorum systems . <eos> quorum systems are well known tools for ensuring the consistency and availability of replicated data despite the benign failure of data repositories . in this paper we consider the arbitrary ( byzantine ) failure of data repositories and present the first study of quorum system requirements and constructions that ensure data availability and consistency despite these failures . we also consider the load associated with our quorum systems , i . e . , the minimal access probability of the busiest server . for services subject to arbitrary failures , we demonstrate quorum systems over < i > n < i > servers with a load of < i > o < i > ( <digit> < i > n < i > ) , thus meeting the lower bound on load for benignly fault tolerant quorum systems . we explore several variations of our quorum systems and extend our constructions to cope with arbitrary client failures .
keeping track of the latest gossip in a distributed system . <eos> we tackle a natural problem from distributed computing , involving time stamps . let < i > p < i > < i > p < i > < inf > <digit> < inf > , < i > p < i > < inf > <digit> < inf > , . . . , < i > p < i > < inf > n < inf > be a set of computing agents or processes which synchronize with each other from time to time and exchange information about themselves and others . the gossip problem is the following whenever a set < i > p < i > < i > p < i > meets , the processes in < i > p < i > must decide amongst themselves which of them has the latest information , direct or indirect , about each agent < i > p < i > in the system . we propose an algorithm to solve this problem which is finite state and local . formally , this means that our algorithm can be implemented as an asynchronous automation .
efficient leader election using sense of direction . <eos> this paper presents a protocol for leader election in complete networks with a sense of direction . sense of direction provides nodes the capability of distinguishing between their incident links according to a global scheme . we propose a protocol for leader election which requires < i > o ( n ) < i > messages and < i > o < i > ( log < i > n < i > ) time . the protocol is message optimal and the time complexity is a significant improvement over currently known protocols for this problem .
randomized naming using wait free shared variables . <eos> a naming protocol assigns unique names ( keys ) to every process out of a set of communicating processes . we construct a randomized wait free naming protocol using wait free atomic read write registers ( shared variables ) as process intercommunication primitives . each process has its own private register and can read all others . the addresses names each one uses for the others are possibly different processes < i > p < i > and < i > q < i > address the register of process < i > r < i > in a way not known to each other . for < i > n < i > processes and > <digit> , the protocol uses a name space of size ( <digit> ) < i > n < i > and < i > o < i > ( < i > n < i > log < i > n < i > log log < i > n < i > ) running time ( read writes to shared bits ) with probability at least <digit> < i > o < i > ( <digit> ) , and < i > o < i > ( < i > n < i > log < sup > <digit> < sup > < i > n < i > ) overall expected running time . the protocol is based on the wait free implementation of a novel test <unk> object that randomly and fast selects a winner from a set of < i > q < i > contenders with probability at least in the face of the strongest possible adaptive adversary .
communication lower bounds for distributed memory matrix multiplication . <eos> we present lower bounds on the amount of communication that matrix multiplication algorithms must perform on a distributed memory parallel computer . we denote the number of processors by < i > p < i > and the dimension of square matrices by < i > n < i > . we show that the most widely used class of algorithms , the so called two dimensional ( 2d ) algorithms , are optimal , in the sense that in any algorithm that only uses < i > o < i > ( < i > n < i > < sup > <digit> < sup > < i > p < i > ) words of memory per processor , at least one processor must send or receive ( < i > n < i > < sup > <digit> < sup > < i > p < i > < sup > <digit> <digit> < sup > ) words . we also show that algorithms from another class , the so called three dimensional ( 3d ) algorithms , are also optimal . these algorithms use replication to reduce communication . we show that in any algorithm that uses < i > o < i > ( < i > n < i > < sup > <digit> < sup > < i > p < i > < sup > <digit> <digit> < sup > ) words of memory per processor , at least one processor must send or receive ( < i > n < i > < sup > <digit> < sup > < i > p < i > < sup > <digit> <digit> < sup > ) words . furthermore , we show a continuous tradeoff between the size of local memories and the amount of communication that must be performed . the 2d and 3d bounds are essentially instantiations of this tradeoff . we also show that if the input is distributed across the local memories of multiple nodes without replication , then ( < i > n < i > < sup > <digit> < sup > ) words must cross any bisection cut of the machine . all our bounds apply only to conventional ( < i > n < i > < sup > <digit> < sup > ) algorithms . they do not apply to strassen ' s algorithm or other < i > o < i > ( < i > n < i > < sup > <digit> < sup > ) algorithms .
approximation algorithms for partial covering problems . <eos> we study a generalization of covering problems called partial covering . here we wish to cover only a desired number of elements , rather than covering all elements as in standard covering problems . for example , in < i > k < i > partial set cover , we wish to choose a minimum number of sets to cover at least < i > k < i > elements . for < i > k < i > partial set cover , if each element occurs in at most < i > f < i > sets , then we derive a primal dual < i > f < i > approximation algorithm ( thus implying a <digit> approximation for < i > k < i > partial vertex cover ) in polynomial time . without making any assumption about the number of sets an element is in , for instances where each set has cardinality at most three , we obtain an approximation of <digit> <digit> . we also present better than <digit> approximation algorithms for < i > k < i > partial vertex cover on bounded degree graphs , and for vertex cover on expanders of bounded average degree . we obtain a polynomial time approximation scheme for < i > k < i > partial vertex cover on planar graphs , and for covering < i > k < i > points in < i > r < sup > d < sup > < i > by disks .
on exact selection of minimally unsatisfiable subformulae . <eos> a minimally unsatisfiable subformula ( mus ) is a subset of clauses of a given cnf formula which is unsatisfiable but becomes satisfiable as soon as any of its clauses is removed . the selection of a mus is of great relevance in many practical applications . this expecially holds when the propositional formula encoding the application is required to have a well defined satisfiability property ( either to be satisfiable or to be unsatisfiable ) . while selection of a mus is a hard problem in general , we show classes of formulae where this problem can be solved efficiently . this is done by using a variant of farkas ' lemma and solving a linear programming problem . successful results on real world contradiction detection problems are presented .
crossing number , pair crossing number , and expansion . <eos> the < i > crossing number < i > cr ( < i > g < i > ) of a graph < i > g < i > is the minimum possible number of edge crossings in a drawing of < i > g < i > in the plane , while the < i > pair crossing number < i > pcr ( < i > g < i > ) is the smallest number of pairs of edges that cross in a drawing of < i > g < i > in the plane . while cr ( < i > g < i > ) pcr ( < i > g < i > ) holds trivially , it is not known whether a strict inequality can ever occur ( this question was raised by mohar and pach and tth ) . we aim at bounding cr ( < i > g < i > ) in terms of pcr ( < i > g < i > ) . using the methods of leighton and rao , bhatt and leighton , and even , guha and schieber , we prove that one of the main steps is an analogy of the well known lower bound cr ( < i > g < i > ) ( < i > b < i > ( < i > g < i > ) < sup > <digit> < sup > ) < i > o < i > ( <unk> ( < i > g < i > ) ) , where < i > b ( g ) < i > is the < i > bisection width < i > of < i > g < i > , that is , the smallest number of edges that have to be removed so that no component of the resulting graph has more than <digit> <digit> < i > n < i > vertices . we show that we also prove by similar methods that a graph < i > g < i > with crossing number log < sup > <digit> < sup > < i > n < i > has a nonplanar subgraph on at most < i > o < i > ( < i > nm < i > log < sup > <digit> < sup > < i > n < i > < i > k < i > ) vertices , where < i > m < i > is the number of edges , is the maximum degree in < i > g < i > , and < i > c < i > is a suitable sufficiently large constant .
scenario based comparison of source tracing and dynamic source routing protocols for ad hoc networks . <eos> we present source tracing as a new viable approach to routing in ad hoc networks in which routers communicate the second to last hop and distance in preferred paths to destinations . we introduce a table driven protocol ( best ) in which routers maintain routing information for all destinations , and an on demand routing protocol ( dst ) in which routers maintain routing information for only those destinations to whom they need to forward data . simulation experiments are used to compare these protocols with dsr , which has been shown to incur less control overhead that other on demand routing protocols . the simulations show that dst requires far less control packets to achieve comparable or better average delays and percentage of packet delivered than dsr , and that best achieves comparable results to dsr while maintaining routing information for all destinations .
building minority language corpora by learning to generate web search queries . <eos> the web is a source of valuable information , but the process of collecting , organizing , and effectively utilizing the resources it contains is difficult . we describe corpusbuilder , an approach for automatically generating web search queries for collecting documents matching a minority concept . the concept used for this paper is that of text documents belonging to a minority natural language on the web . individual documents are automatically labeled as relevant or nonrelevant using a language filter , and the feedback is used to learn what query lengths and inclusion exclusion term selection methods are helpful for finding previously unseen documents in the target language . our system learns to select good query terms using a variety of term scoring methods . using < i > odds ratio < i > scores calculated over the documents acquired was one of the most consistently accurate query generation methods . to reduce the number of estimated parameters , we parameterize the query length using a gamma distribution and present empirical results with learning methods that vary the time horizon used when learning from the results of past queries . we find that our system performs well whether we initialize it with a whole document or with a handful of words elicited from a user . experiments applying the same approach to multiple languages are also presented showing that our approach generalizes well across several languages regardless of the initial conditions .
collaborative filtering with maximum entropy . <eos> the authors describe a novel maximum entropy ( maxent ) approach for generating online recommendations as a user navigates through a collection of documents . they show how to handle high dimensional sparse data and represent it as a collection of ordered sequences of document requests . this representation and the maxent approach have several advantages ( <digit> ) you can naturally model long term interactions and dependencies in the data sequences ( <digit> ) you can query the model quickly once it is learned , which makes the method applicable to high volume web servers and ( <digit> ) you obtain empirically high quality recommendations . although maxent learning is computationally infeasible if implemented in the straightforward way , the authors explored data clustering and several algorithmic techniques to make learning practical even in high dimensions . they present several methods for combining the predictions of maxent models learned in different clusters . they conducted offline tests using over six months ' worth of data from researchindex , a popular online repository of over <digit> , <digit> computer science documents . they show that their maxent algorithm is one of the most accurate recommenders , as compared to such techniques as correlation , a mixture of markov models , a mixture of multinomial models , individual similarity based recommenders currently available on researchindex , and even various combinations of current researchindex recommenders .
privacy preserving data mining . <eos> data mining is under attack from privacy advocates because of a misunderstanding about what it actually is and a valid concern about how it ' s generally done . this article shows how technology from the security community can change data mining for the better , providing all its benefits while still maintaining privacy .
selection of views to materialize in a data warehouse . <eos> a data warehouse stores materialized views of data from one or more sources , with the purpose of efficiently implementing decision support or olap queries . one of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse . the goal is to select an appropriate set of views that minimizes total query response time and the cost of maintaining the selected views , given a limited amount of resource , e . g . , materialization time , storage space , etc . in this article , we have developed a theoretical framework for the general problem of selection of views in a data warehouse . we present polynomial time heuristics for a selection of views to optimize total query response time under a disk space constraint , for some important special cases of the general data warehouse scenario , viz . <digit> ) an and view graph , where each query view has a unique evaluation , e . g . , when a multiple query optimizer can be used to general a global evaluation plan for the queries , and <digit> ) an or view graph , in which any view can be computed from any one of its related views , e . g . , data cubes . we present proofs showing that the algorithms are guaranteed to provide a solution that is fairly close to ( within a constant factor ratio of ) the optimal solution . we extend our heuristic to the general and or view graphs . finally , we address in detail the view selection problem under the maintenance cost constraint and present provably competitive heuristics .
efficient learning equilibrium . <eos> we introduce efficient learning equilibrium ( ele ) , a normative approach to learning in noncooperative settings . in ele , the learning algorithms themselves are required to be in equilibrium . in addition , the learning algorithms must arrive at a desired value after polynomial time , and a deviation from the prescribed ele becomes irrational after polynomial time . we prove the existence of an ele ( where the desired value is the expected payoff in a nash equilibrium ) and of a pareto ele ( where the objective is the maximization of social surplus ) in repeated games with perfect monitoring . we also show that an ele does not always exist in the imperfect monitoring case . finally , we discuss the extension of these results to general sum stochastic games .
proving convergence of self stabilizing systems using first order rewriting and regular languages . <eos> in the framework of self stabilizing systems , the convergence proof is generally done by exhibiting a measure that strictly decreases until a legitimate configuration is reached . the discovery of such a measure is very specific and requires a deep understanding of the studied transition system . in contrast we propose here a simple method for proving convergence , which regards self stabilizing systems as string rewrite systems , and adapts a procedure initially designed by dershowitz for proving termination of string rewrite systems . in order to make the method terminate more often , we also propose an adapted procedure that manipulates schemes , i . e . regular sets of words , and incorporates a process of scheme generalization . the interest of the method is illustrated on several nontrivial examples .
exponential lower bound for <digit> query locally decodable codes via a quantum argument . <eos> a locally decodable code ( ldc ) encodes n bit strings x in m bit codewords c ( x ) in such a way that one can recover any bit xi from a corrupted codeword by querying only a few bits of that word . we use a quantum argument to prove that ldcs with <digit> classical queries require exponential <digit> ( n ) . previously , this was known only for linear codes ( goldreich et al . , in proceedings of 17th ieee conference on computation complexity , <digit> , pp . <digit> <digit> ) . the proof proceeds by showing that a <digit> query ldc can be decoded with a single quantum query , when defined in an appropriate sense . it goes on to establish an exponential lower bound on any ' l query locally quantum decodable code ' . we extend our lower bounds to non binary alphabets and also somewhat improve the polynomial lower bounds by katz and trevisan for ldcs with more than <digit> queries . furthermore , we show that q quantum queries allow more succinct ldcs than the best known ldcs with q classical queries . finally , we give new classical lower bounds and quantum upper bounds for the setting of private information retrieval . in particular , we exhibit a quantum <digit> server private information retrieval ( pir ) scheme with o ( n3 <digit> ) qubits of communication , beating the o ( n1 <digit> ) bits of communication of the best known classical <digit> server pir .
a tight bound on approximating arbitrary metrics by tree metrics . <eos> in this paper , we show that any n point metric space can be embedded into a distribution over dominating tree metrics such that the expected stretch of any edge is o ( log n ) . this improves upon the result of bartal who gave a bound of o ( log n log log n ) . moreover , our result is existentially tight there exist metric spaces where any tree embedding must have distortion ( log n ) distortion . this problem lies at the heart of numerous approximation and online algorithms including ones for group steiner tree , metric labeling , buy at bulk network design and metrical task system . our result improves the performance guarantees for all of these problems .
load balancing scatter operations for grid computing . <eos> we present solutions to statically load balance scatter operations in parallel codes run on grids . our load balancing strategy is based on the modification of the data distributions used in scatter operations . we study the replacement of scatter operations with parameterized scatters , allowing custom distributions of data . the paper presents ( <digit> ) a general algorithm which finds an optimal distribution of data across processors ( <digit> ) a quicker guaranteed heuristic relying on hypotheses on communications and computations ( <digit> ) a policy on the ordering of the processors . experimental results with an mpi scientific code illustrate the benefits obtained from our load balancing .
spatial gossip and resource location protocols . <eos> the dynamic behavior of a network in which information is changing continuously over time requires robust and efficient mechanisms for keeping nodes updated about new information . gossip protocols are mechanisms for this task in which nodes communicate with one another according to some underlying deterministic or randomized algorithm , exchanging information in each communication step . in a variety of contexts , the use of randomization to propagate information has been found to provide better reliability and scalability than more regimented deterministic approaches . in many settings , such as a cluster of distributed computing hosts , new information is generated at individual nodes , and is most interesting to nodes that are nearby . thus , we propose distance based propagation bounds as a performance measure for gossip mechanisms a node at distance d from the origin of a new piece of information should be able to learn about this information with a delay that grows slowly with d , and is independent of the size of the network . for nodes arranged with uniform density in euclidean space , we present natural gossip mechanisms , called spatial gossip , that satisfy such a guarantee new information is spread to nodes at distance d , with high probability , in o ( log1 steps . such a bound combines the desirable qualitative features of uniform gossip , in which information is spread with a delay that is logarithmic in the full network size , and deterministic flooding , in which information is spread with a delay that is linear in the distance and independent of the network size . our mechanisms and their analysis resolve a conjecture of demers et al . <digit> . we further show an application of our gossip mechanisms to a basic resource location problem , in which nodes seek to rapidly learn the location of the nearest copy of a resource in a network . this problem , which is of considerable practical importance , can be solved by a very simple protocol using spatial gossip , whereas we can show that no protocol built on top of uniform gossip can inform nodes of their approximately nearest resource within poly logarithmic time . the analysis relies on an additional useful property of spatial gossip , namely that information travels from its source to sinks along short paths not visiting points of the network far from the two nodes .
fast monte carlo algorithms for finding low rank approximations . <eos> we consider the problem of approximating a given m n matrix a by another matrix of specified rank k , which is smaller than m and n . the singular value decomposition ( svd ) can be used to find the best such approximation . however , it takes time polynomial in m , n which is prohibitive for some modern applications . in this article , we develop an algorithm that is qualitatively faster , provided we may sample the entries of the matrix in accordance with a natural probability distribution . in many applications , such sampling can be done efficiently . our main result is a randomized algorithm to find the description of a matrix d of rank at most k so that holds with probability at least <digit> ( where verbar verbar f is the frobenius norm ) . the algorithm takes time polynomial in k , <digit> epsi , log ( <digit> ) only and is independent of m and n . in particular , this implies that in constant time , it can be determined if a given matrix of arbitrary size has a good low rank approximation .
on the multiplicity of parts in a random composition of a large integer . <eos> in this paper we study the following question posed by h . s . wilf what is , asymptotically as n rightarrow infty , the probability that a randomly chosen part size in a random composition of an integer n has multiplicity m more specifically , given positive integers n and m , suppose that a composition lambda of n is selected uniformly at random and then , out of the set of part sizes in lambda , a part size j is chosen uniformly at random . let p ( a n ( m ) ) be the probability that j has multiplicity m . we show that for fixed m , p ( a n ( m ) ) goes to <digit> at the rate <digit> ln n . a more careful analysis uncovers an unexpected result ( ln n ) p ( a n ( m ) ) does not have a limit but instead oscillates around the value <digit> m as n to infty . this work is a counterpart of a recent paper of corteel , pittel , savage , and wilf , who studied the same problem in the case of partitions rather than compositions .
approximation algorithms for the <digit> extension problem . <eos> in the <digit> extension problem , we are given a weighted graph with some nodes marked as terminals and a semimetric on the set of terminals . our goal is to assign the rest of the nodes to terminals so as to minimize the sum , over all edges , of the product of the edge ' s weight and the distance between the terminals to which its endpoints are assigned . this problem generalizes the multiway cut problem of dahlhaus et al . siam j . comput . , <digit> ( <digit> ) , pp . <digit> <digit> and is closely related to the metric labeling problem introduced by kleinberg and tardos proceedings of the 40th ieee annual symposium on foundations of computer science , new york , <digit> , pp . <digit> <digit> . we present approximation algorithms for sc <digit> extension . in arbitrary graphs , we present a o ( log k ) approximation algorithm , k being the number of terminals . we also give o ( <digit> ) approximation guarantees for weighted planar graphs . our results are based on a natural metric relaxation of the problem previously considered by karzanov european j . combin . , <digit> ( <digit> ) , pp . <digit> <digit> . it is similar in flavor to the linear programming relaxation of garg , vazirani , and yannakakis siam j . comput . , <digit> ( <digit> ) , pp . <digit> <digit> for the multicut problem , and similar to relaxations for other graph partitioning problems . we prove that the integrality ratio of the metric relaxation is at least c sqrt lg k for a positive c for infinitely many k . our results improve some of the results of kleinberg and tardos , and they further our understanding on how to use metric relaxations .
homotopies for intersecting solution components of polynomial systems . <eos> we show how to use numerical continuation to compute the intersection c a cap b of two algebraic sets a and b , where a , b , and c are numerically represented by witness sets . en route to this result , we first show how to find the irreducible decomposition of a system of polynomials restricted to an algebraic set . the intersection of components a and b then follows by considering the decomposition of the diagonal system of equations u restricted to u , v in a times b . an offshoot of this new approach is that one can solve a large system of equations by finding the solution components of its subsystems and then intersecting these . it also allows one to find the intersection of two components of the two polynomial systems , which is not possible with any previous numerical continuation approach .
an open and shut typecase . <eos> two different ways of defining ad hoc polymorphic operations commonly occur in programming languages . with the first form polymorphic operations are defined inductively on the structure of types while with the second form polymorphic operations are defined for specific sets of types . in intensional type analysis operations are defined by induction on the structure of types . therefore no new cases are necessary for user defined types , because these types are equivalent to their underlying structure . however , intensional type analysis is closed to extension , as the behavior of the operations can not be differentiated for the new types , thus destroying the distinctions that these types are designed to express . haskell type classes on the other hand define polymorphic operations for sets of types . operations defined by class instances are considered open the programmer can add instances for new types without modifying existing code . however , the operations must be extended with specialized code for each new type , and it may be tedious or even impossible to add extensions that apply to a large universe of new types . both approaches have their benefits , so it is important to let programmers decide which is most appropriate for their needs . in this paper , we define a language that supports both forms of ad hoc polymorphism , using the same basic constructs .
associated types with class . <eos> haskell ' s type classes allow ad hoc overloading , or type indexing , of functions . a natural generalisation is to allow type indexing of data types as well . it turns out that this idea directly supports a powerful form of abstraction called associated types , which are available in c using traits classes . associated types are useful in many applications , especially for self optimising libraries that adapt their data representations and algorithms in a type directed manner . in this paper , we introduce and motivate associated types as a rather natural generalisation of haskell ' s existing type classes . formally , we present a type system that includes a type directed translation into an explicitly typed target language akin to system f the existence of this translation ensures that the addition of associated data types to an existing haskell compiler only requires changes to the front end .
ergodicity and throughput bounds of petri nets with unique consistent firing count vector . <eos> ergodicity and throughput bound characterization are addressed for a subclass of timed and stochastic petri nets , interleaving qualitative and quantitative theories . the nets considered represent an extension of the well known subclass of marked graphs , defined as having a unique consistent firing count vector , independently of the stochastic interpretation of the net model . in particular , persistent and mono t semiflow net subclasses are considered . upper and lower throughput bounds are computed using linear programming problems defined on the incidence matrix of the underlying net . the bounds proposed depend on the initial marking and the mean values of the delays but not on the probability distributions ( thus including both the deterministic and the stochastic cases ) . from a different perspective , the considered subclasses of synchronized queuing networks thus , the proposed bounds can be applied to these networks .
multiple resolution segmentation of textured images . <eos> a multiple resolution algorithm is presented for segmenting images into regions with differing statistical behavior . in addition , an algorithm is developed for determining the number of statistically distinct regions in an image and estimating the parameters of those regions . both algorithms use a causal gaussian autoregressive model to describe the mean , variance , and spatial correlation of the image textures . together , the algorithms can be used to perform unsupervised texture segmentation . the multiple resolution segmentation algorithm first segments images at coarse resolution and then progresses to finer resolutions until individual pixels are classified . this method results in accurate segmentations and requires significantly less computation than some previously known methods . the field containing the classification of each pixel in the image is modeled as a markov random field . segmentation at each resolution is then performed by maximizing the a posteriori probability of this field subject to the resolution constraint . at each resolution , the a posteriori probability is maximized by a deterministic greedy algorithm which iteratively chooses the classification of individual pixels or pixel blocks . the unsupervised parameter estimation algorithm determines both the number of textures and their parameters by minimizing a global criterion based on the aic information criterion . clusters corresponding to the individual textures are formed by alternately estimating the cluster parameters and repartitioning the data into those clusters . concurrently , the number of distinct textures is estimated by combining clusters until a minimum of the criterion is reached .
fitting parameterized three dimensional models to images . <eos> model based recognition and motion tracking depend upon the ability to solve for projection and model parameters that will best fit a <digit> d model to matching <digit> d image features . the author extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation , variable dimensions , or surface deformations . numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters . the levenberg marquardt method is used to always ensure convergence of the solution . these techniques allow model based vision to be used for a much wider class of problems than was possible with previous methods . their application is demonstrated for tracking the motion of curved , parameterized objects .
an object oriented framework for the integration of interactive animation techniques . <eos> we present an interactive modeling and animation system that facilitates the integration of a variety of simulation and animation paradigms . this system permits the modeling of diverse objects that change in shape , appearance , and behaviour over time . our system thus extends modeling tools to include animation controls . changes can be effected by various methods of control , including scripted , gestural , and behavioral specification . the system is an extensible testbed that supports research in the interaction of disparate control methods embodied in controller objects . this paper discusses some of the issues involved in modeling such interactions and the mechanisms implemented to provide solutions to some of these issues . the system ' s object oriented architecture uses delegation hierarchies to let objects change all of their attributes dynamically . objects include displayable objects , controllers , cameras , lights , renderers , and user interfaces . techniques used to obtain interactive performance include the use of data dependency networks , lazy evaluation , and extensive caching to exploit inter and intra frame coherency .
piecewise surface flattening for non distorted texture mapping . <eos> this paper introduces new techniques for interactive piecewise flattening of parametric <digit> d surfaces , leading to a non distorted , hence realistic , texture mapping . cuts are allowed on the mapped texture and we make a compromise between discontinuities and distortions . these techniques are based on results from differential geometry , more precisely on the notion of geodesic curvature isoparametric curves of the surface are mapped , in a constructive way , onto curves in the texture plane with preservation of geodesic curvature at each point . as an application , we give a concrete example which is a first step towards an efficient and robust cad tool for shoe modeling .
minimal order loop free routing strategy . <eos> a multiorder routing strategy is developed which is loop free even in the presence of link node failures . unlike most conventional methods in which the same routing strategy is applied indiscriminately to all nodes in the network , nodes under this proposal may adopt different routing strategies according to the network structure . formulas are developed to determine the minimal order of routing strategy for each node to eliminate looping completely . a systematic procedure for striking a compromise between the operational overhead and network adaptability is proposed . several illustrative examples are presented .
refinement methods for geometric bounds in constructive solid geometry . <eos> in constructive solid geometry , geometric solids are represented as trees whose leaves are labeled by primitive solids and whose internal nodes are labeled by set theoretic operations . a bounding function in this context is an upper or lower estimate on the extent of the constituent sets such bounds are commonly used to speed up algorithms based on such trees . we introduce the class of totally consistent bounding functions , which have the desirable properties of allowing surprisingly good bounds to be built quickly . both outer and inner bounds can be refined using a set of rewrite rules , for which we give some complexity and convergence results . we have implemented the refinement rules for outer bounds within a solid modeling system , where they have proved especially useful for intersection testing in three and four dimensions . our implementations have used boxes as bounds , but different classes ( shapes ) of bounds are also explored . the rewrite rules are also applicable to relatively slow , exact operations , which we explore for their theoretical insight , and to general boolean algebras . results concerning the relationship between these bounds and active zones are also noted .
distributed representations , simple recurrent networks , and grammatical structure . <eos> in this paper three problems for a connectionist account of language are considered <digit> . what is the nature of linguistic representations <digit> . how can complex structural relationships such as constituent structure be represented <digit> . how can the apparently open ended nature of language be accommodated by a fixed resource system using a prediction task , a simple recurrent network ( srn ) is trained on <unk> sentences which contain multiply embedded relative clauses . principal component analysis of the hidden unit activation patterns reveals that the network solves the task by developing complex distributed representations which encode the relevant grammatical relations and hierarchical constituent structure . differences between the srn state representations and the more traditional pushdown store are discussed in the final section .
embedding complete binary trees into butterfly networks . <eos> the authors present embeddings of complete binary trees into butterfly networks with or without wrap around connections . let m be an even integer and q m ( log m ) <digit> . the authors show how to embed a <digit> sup q <digit> <digit> node complete binary tree t ( q ) into a ( m <digit> ) <digit> sup m <digit> node wrap around butterfly b sub w ( m <digit> ) with a dilation of <digit> , and how to embed t ( q ) into a ( m <digit> ) <digit> sup m <digit> node wrap around butterfly b sub w ( m <digit> ) with an optimal dilation of <digit> . they also present an embedding of a wrap around butterfly b sub w ( m ) into a ( m <digit> ) <digit> sup m node no wrap around butterfly b ( m ) with a dilation of <digit> . using this embedding it is shown that t ( q ) can be embedded into a no wrap butterfly b ( m <digit> ) ( resp . b ( m <digit> ) ) with a dilation of <digit> ( resp . <digit> ) .
a packaging system for heterogeneous execution environments . <eos> a packaging system that allows diverse software components to be easily interconnected within heterogeneous programming environments is described . interface software and stubs are generated for programmers automatically once the programmers express their application ' s geometry in a few simple rules and module interconnection language attributes . by generating custom interface code for each application , based on analysis and extraction of interfacing requirements , the system is able to produce executables whose run time performance is comparable to manually integrated applications . the system is implemented within the unix environment .
using program slicing in software maintenance . <eos> program slicing is applied to the software maintenance problem by extending the notion of a program slice ( that originally required both a variable and line number ) to a decomposition slice , one that captures all computation on a given variable , i . e . , is independent of line numbers . using the lattice of single variable decomposition slices ordered by set inclusion , it is shown how a slice based decomposition for programs can be formed . one can then delineate the effects of a proposed change by isolating those effects in a single component of the decomposition . this gives maintainers a straightforward technique for determining those statements and variables which may be modified in a component and those which may not . using the decomposition , a set of principles to prohibit changes which will interfere with unmodified components is provided . these semantically consistent changes can then be merged back into the original program in linear time .
adaptive programming . <eos> an adaptive program is one that changes its behavior base on the current state of its environment . this notion of adaptivity is formalized , and a logic for reasoning about adaptive programs is presented . the logic includes several composition operators that can be used to define an adaptive program in terms of given constituent programs programs resulting from these compositions retain the adaptive properties of their constituent programs . the authors begin by discussing adaptive sequential programs , then extend the discussion to adaptive distributed programs . the relationship between adaptivity and self stabilization is discussed . a case study for constructing an adaptive distributed program where a token is circulated in a ring of processes is presented .
performance prediction and evaluation of parallel processing on a numa multiprocessor . <eos> the efficiency of the basic operations of a numa ( nonuniform memory access ) multiprocessor determines the parallel processing performance on a numa multiprocessor . the authors present several analytical models for predicting and evaluating the overhead of interprocessor communication , process scheduling , process synchronization , and remote memory access , where network contention and memory contention are considered . performance measurements to support the models and analyses through several numerical examples have been done on the bbn gp1000 , a numa shared memory multiprocessor . analytical and experimental results give a comprehensive understanding of the various effects , which are important for the effective use of numa shared memory multiprocessor . the results presented can be used to determine optimal strategies in developing an efficient programming environment for a numa system .
optimal weight assignment for signature generation . <eos> previous work on superimposed coding has been characterized by two aspects . first , it is generally assumed that signatures are generated from logical text blocks of the same size that is , each block contains the same number of unique terms after stopword and duplicate removal . we call this approach the fixed size block ( fsb ) method , since each text block has the same size , as measured by the number of unique terms contained in it . second , with only a few exceptions <digit> , <digit> , <digit> , <digit> , <digit> , most previous work has assumed that each term in the text contributes the same number of ones to the signature ( i . e . , the weight of the term signatures is fixed ) . the main objective of this paper is to derive an optimal weight assignment that assigns weights to document terms according to their occurrence and query frequencies in order to minimize the false drop probability . the optimal scheme can account for both uniform and nonuniform occurence and query frequencies , and the signature generation method is still based on hashing rather than on table lookup . furthermore , a new way of generating signatures , the fixed weight block ( <unk> ) method , is introduced . <unk> controls the weight of every signature to a constant , whereas in fsb , only the expected signature weight is constant . we have shown that <unk> has a lower false drop probability than that of the fsb method , but its storage overhead is slightly higher . other advantages of <unk> are that the optimal weight assignment can be obtained analytically without making unrealistic assumptions and that the formula for computing the term signature weights is simple and efficient .
a lingua franca for concurrent logic programming . <eos> two of the more important concurrent logic programming languages with nonflat guards are ghc and parlog . they balance the requirements of having clean semantics and providing good control facilities rather differently , and their respective merits are compared and contrasted . since concurrent logic programming would benefit from both , but neither language is able to express all the programs expressible in the other language , a lingua franca of these languages is defined and justified . a method is given for translating ghc and parlog to and from it . the method preserves the arities and execution conditions of each clause . it enables a lingua franca implementation to support both languages transparently , and to provide a simple concurrent logic programming language suitable for programming in its own right .
a control flow normalization algorithm and its complexity . <eos> a single method for normalizing the control flow of programs to facilitate program transformations , program analysis , and automatic parallelization is presented . while previous methods result in programs whose control flowgraphs are reducible , programs normalized by this technique satisfy a stronger condition than reducibility and are therefore simpler in their syntax and structure than with previous methods . in particular , all control flow cycles are normalized into single entry , single exit while loops and all gotos are eliminated . furthermore , the method avoids problems of code replication that are characteristic of node splitting techniques . this restructuring obviates the control dependence graph , since afterwards control dependence relations are manifest in the syntax tree of the program . transformations that effect this normalization are presented , and the complexity of the method is studied .
what are race conditions . <eos> in shared memory parallel programs that use explicit synchronization , race conditions result when accesses to shared memory are not properly synchronized . race conditions are often considered to be manifestations of bugs , since their presence can cause the program to behave unexpectedly . unfortunately , there has been little agreement in the literature as to precisely what constitutes a race condition . two different notions have been implicitly considered one pertaining to programs intended to be deterministic ( which we call general races ) and the other to nondeterministic programs containing critical sections ( which we call data races ) . however , the differences between general races and data races have not yet been recognized . this paper examines these differences by characterizing races using a formal model and exploring their properties . we show that two variations of each type of race exist feasible general races and data races capture the intuitive notions desired for debugging and apparent races capture less accurate notions implicitly assumed by most dynamic race detection methods . we also show that locating feasible races is an np hard problem , implying that only the apparent races , which are approximations to feasible races , can be detected in practice . the complexity of dynamically locating apparent races depends on the type of synchronization used by the program . apparent races can be exhaustively located efficiently only for weak types of synchronization that are incapable of implementing mutual exclusion . this result has important implications since we argue that debugging general races requires exhaustive race detection and is inherently harder than debugging data races ( which requires only partial race detection ) . programs containing data races can therefore be efficiently debugged by locating certain easily identifiable races . in contrast , programs containing general races require more complex debugging techniques .
a multimodel methodology for qualitative model engineering . <eos> qualitative models arising in artificial intelligence domain often concern real systems that are difficult to represent with traditional means . however , some promise for dealing with such systems is offered by research in simulation methodology . such research produces models that combine both continuous and discrete event formalisms . nevertheless , the aims and approaches of the ai and the simulation communities remain rather mutually ill understood . consequently , there is a need to bridge theory and methodology in order to have a uniform language when either analyzing or reasoning about physical systems . this article introduces a methodology and formalism for developing multiple , cooperative models of physical systems of the type studied in qualitative physics . the formalism combines discrete event and continuous models and offers an approach to building intelligent machines capable of physical modeling and reasoning .
on workload characterization of relational database environments . <eos> a relational database workload analyzer ( <unk> ) is developed to characterize the workload in a db2 environment . this is applied to study a production db2 system where a structured query language ( sql ) trace for a two hour interval and an image copy of the database catalog were obtained . the results of the workload study are summarized . the structure and complexity of sql statements , the makeup and run time behavior of transactions queries , and the composition of relations and views are discussed . the results obtained provide the important information needed to build a benchmark workload to evaluate the alternative design tradeoffs of database systems .
detecting unsafe error recovery schedules . <eos> a mechanism for modeling timing , precedence , and data consistency constraints on concurrently executing processes is presented . the model allows durations and intervals between events to be specified . an algorithm is provided to detect schedules which may be unsafe with respect to the constraints . this work , motivated by the design and validation of autonomous error recovery strategies on the galileo spacecraft , appears to be applicable to a variety of asynchronous real time systems .
vectorized software for bessel function evaluation . <eos> a suite of computer programs for the evaluation of bessel functions and modified bessel functions of orders zero and one for a vector of real arguments is described . distinguishing characteristics of these programs are that ( a ) they are portable across a wide range of machines , and ( b ) they are vectorized in the case when multiple function evaluations are to be performed . the performance of the new programs are compared with software from the <unk> collection of fullerton on which the new software is based .
a file system for continuous media . <eos> the continuous media file system , cmfs , supports real time storage and retrieval of continuous media data ( digital audio and video ) on disk . cmfs clients read or write files in sessions , each with a guaranteed minimum data rate . multiple sessions , perhaps with different rates , and non real time access can proceed concurrently . cmfs addresses several interrelated design issues real time semantics fo sessions , disk layout , an acceptance test for new sessions , and disk scheduling policy . we use simulation to compare different design choices .
the convergence of td ( <unk> ) for general <unk> . <eos> the method of temporal differences ( td ) is one way of making consistent predictions about the future . this paper uses some analysis of watkins ( <digit> ) to extend a convergence theorem due to sutton ( <digit> ) from the case which only uses information from adjacent time steps to that involving information from arbitrary ones . it also considers how this version of td behaves in the face of linearly dependent representations for <unk> that it still converges , but to a different answer from the least mean squares algorithm . finally it adapts watkins ' theorem that cal q learning , his closely related prediction and action learning method , converges with probability one , to demonstrate this strong form of convergence for a slightly modified version of td .
tools for building asynchronous servers to support speech and audio applications . <eos> distributed client server models are becoming increasingly prevalent in multimedia systems and advanced user interface design . a multimedia application , for example , may play and record audio , use speech recognition input , and use a window system for graphical i o . the software architecture of such a system can be simplified if the application communicates to multiple servers ( e . g . , audio servers , recognition servers ) that each manage different types of input and output . this paper describes tools for rapidly prototyping distributed asynchronous servers and applications , with an emphasis on supporting highly interactive user interfaces , temporal media , and multi modal i o .
declarative programming of graphical interfaces by visual examples . <eos> graphical user interfaces ( gui ) provide intuitive and easy means for users to communicate with computers . however , construction of gui software requires complex programming that is far from being intuitive . because of the semantic gap between the textual application program and its graphical interface , the programmer himself must conceptually maintain the correspondence between the textual programming and the graphical image of the resulting interface . instead , we propose a programming environment based on the programming by visual example ( pbve ) scheme , which allows the gui designers to program visual interfaces for their applications by drawing the example visualization of application data with a direct manipulation interface . our system , trip3 , realizes this with ( <digit> ) the bi directional translation model between the ( abstract ) application data and the pictorial data of the gui , and ( <digit> ) the ability to generate mapping rules for the translation from example application data and its corresponding example visualization . the latter is made possible by the use of generalization of visual examples , where the system is able to automatically generate generalized mapping rules from a given set of examples .
a subexponential bound for linear programming . <eos> we present a simple randomized algorithm which solves linear programs with n constraints and d variables in expected o ( nde ( d ln ( n <digit> ) ) <digit> <digit> ) time in the unit cost model ( where we count the number of arithmetic operations on the numbers in the input ) . the expectation is over the internal randomizations performed by the algorithm , and holds for any input . the algorithm is presented in an abstract framework , which facilitates its application to several other related problems . the algorithm has been presented in a previous work by the authors shw , but its analysis and the subexponential complexity bound are new .
the decoupled simulation model for virtual reality systems . <eos> the virtual reality user interface style allows the user to manipulate virtual objects in a 3d environment using 3d input devices . this style is best suited to application areas where traditional two dimensional styles fall short , but the current programming effort required to produce a vr application is somewhat large . we have built a toolkit called mr , which facilitates the development of vr applications . the toolkit provides support for distributed computing , head mounted displays , room geometry , performance monitoring , hand input devices , and sound feedback . in this paper , the architecture of the toolkit is outlined , the programmer ' s view is described , and two simple applications are described .
lessons learned from suit , the simple user interface toolkit . <eos> in recent years , the computer science community has realized the advantages of guis ( graphical user interfaces ) . because high quality guis are difficult to build , support tools such as uimss , ui toolkits , and interface builders have been developed . although these tools are powerful , they typically make two assumptions first , that the programmer has some familiarity with the gui model , and second , that he is willing to invest several weeks becoming proficient with the tool . these tools typically operate only on specific platforms , such as dos , the macintosh , or unix x windows .
lexical ambiguity and information retrieval . <eos> lexical ambiguity is a pervasive problem in natural language processing . however , little quantitative information is available about the extent of the problem or about the impact that it has on information retrieval systems . we report on an analysis of lexical ambiguity in information retrieval test collections and on experiments to determine the utility of word meanings for separating relevant from nonrelevant documents . the experiments show that there is considerable ambiguity even in a specialized database . word senses provide a significant separation between relevant and nonrelevant documents , but several factors contribute to determining whether disambiguation will make an improvement in performance . for example , resolving lexical ambiguity was found to have little impact on retrieval effectiveness for documents that have many words in common with the query . other uses of word sense disambiguation in an information retrieval context are discussed .
estimation and enhancement of real time software reliability through mutation analysis . <eos> a simulation based method for obtaining numerical estimates of the reliability of n version , real time software is proposed . an extended stochastic petri net is used to represent the synchronization structure of n versions of the software , where dependencies among versions are modeled through correlated sampling of module execution times . the distributions of execution times are derived from automatically generated test cases that are based on mutation testing . since these test cases are designed to reveal software faults , the associated execution times and reliability estimates are likely to be conservative . experimental results using specifications for nasa ' s planetary lander control software suggest that mutation based testing could hold greater potential for enhancing reliability than the desirable but perhaps unachievable goal of independence among n versions . nevertheless , some support for n version enhancement of high quality , mutation tested code is also offered . mutation analysis could also be valuable in the design of fault tolerant software systems .
programming and verifying real time systems by means of the synchronous data flow language lustre . <eos> the benefits of using a synchronous data flow language for programming critical real time systems are investigated . these benefits concern ergonomy ( since the dataflow approach meets traditional description tools used in this domain ) and ability to support formal design and verification methods . it is shown , using a simple example , how the language lustre and its associated verification tool <unk> , can be used to design a program , to specify its critical properties , and to verify these properties . as the language lustre and its uses have already been discussed in several papers , emphasis is put on program verification .
automatic recognition of tractability in inference relations . <eos> a procedure is given for recognizing sets of inference rules that generate polynomial time decidable inference relations . the procedure can automatically recognize the tractability of the inference rules underlying congruence closure . the recognition of tractability for that particular rule set constitutes mechanical verification of a theorem originally proved independently by kozen and shostak . the procedure is algorithmic , rather than heuristic , and the class of automatically recognizable tractable rule sets can be precisely characterized . a series of examples of rule sets whose tractability is nontrivial , yet machine recognizable , is also given . the technical framework developed here is viewed as a first step toward a general theory of tractable inference relations .
fast algorithms for generating discrete random variates with changing distributions . <eos> one of the most fundamental operations when simulating a stochastic discrete event dynamic system is the generation of a nonuniform discrete random variate . the simplest form of this operation can be stated as follows generate a random variable x that is distributed over the integers <digit> , <digit> , , n such that are fixed nonnegative numbers . the well known alias algorithm is available to accomplish this task in o ( <digit> ) time . a more difficult problem is to generate variates for x when the ai ' s are changing with time . we present three rejection based algorithms for this task , and for each algorithm we characterize the performance in terms of acceptance probability and the expected effort to generate a variate . we show that , under fairly unrestrictive conditions , the long run expected effort is o ( <digit> ) . applications to markovian queuing networks are discussed . we also compare the three algorithms with competing schemes appearing in the literature .
coordinating rule based software processes with esp . <eos> esp is a language for modeling rule based software processes that take place in a distributed software development environment . it is based on polis , an abstract coordination model that relies on multiple tuple spaces , i . e . , collections of tuples a la linda . polis extends linda aiming at the specification and coordination of logically distributed systems . esp ( extended shared prolog ) combines the polis mechanisms to deal with concurrency and distribution , with the logic programming language prolog , to deal with rules and deduction . such a combination of a coordination model and a logic language provides a powerful framework in which experiments about rule based software process programming can be performed and evaluated .
mutation analysis using mutant schemata . <eos> mutation analysis is a powerful technique for assessing and improving the quality of test data used to unit test software . unfortunately , current automated mutation analysis systems suffer from severe performance problems . this paper presents a new method for performing mutation analysis that uses program schemata to encode all mutants for a program into one metaprogram , which is subsequently compiled and run at speeds substantially higher than achieved by previous interpretive systems . preliminary performance improvements of over <digit> % are reported . this method has the additional advantages of being easier to implement than interpretive systems , being simpler to port across a wide range of hardware and software platforms , and using the same compiler and run time support system that is used during development and or deployment .
analysis of or parallel execution models . <eos> we discuss fundamental limitations of or parallel execution models of nondeterministic programming languages . or parallelism corresponds to the execution of different nondeterministic computational paths in parallel . a natural way to represent the state of ( parallel ) execution of a nondeterministic program is by means of an or parallel tree . we identify three important criteria that underlie the design of or parallel implementations based on the or parallel tree constant time access to variables , constant time task creation , and constant time task switching , where the term constant time means that the time for these operations is independent of the number of nodes in the or parallel tree , as well as the size of each node . we prove that all three criteria can not be simultaneously satisfied by any or parallel execution model based on a finite number of processors but unbounded memory . we discuss in detail the application of our result to the class of logic programming languages and show how our result can serve as a useful way to categorize the various or parallel methods proposed in this field . we also discuss the suitability of different or parallel implemenation strategies for different parallel architectures .
the logical data model . <eos> we propose an object oriented data model that generalizes the relational , hierarchical , and network models . a database scheme in this model is a directed graph , whose leaves represent data and whose internal nodes represent connections among the data . instances are constructed from objects , which have separate names and values . we define a logic for the model , and describe a nonprocedural query language that is based on the logic . we also describe an algebraic query language and show that it is equivalent to the logical language .
parallelizing algorithms for symbolic computation using maple . <eos> maple ( speak parallel maple ) is a portable system for parallel symbolic computation . the system is built as an interface between the parallel declarative programming language strand and the sequential computer algebra system maple , thus providing the elegance of strand and the power of the existing sequential algorithms in maple .
template driven interfaces for numerical subroutines . <eos> this paper describes a set of interfaces for numerical subroutines . typing a short ( often one line ) description allows one to solve problems in application domains including least squares data fitting , differential equations , minimization , root finding , and integration . our approach of template driven programming makes it easy to build such an interface a simple one takes a few hours to construct , while a few days suffice to build the most complex program we describe .
a weighted nearest neighbor algorithm for learning with symbolic features . <eos> in the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values . in such domains , the examples can be treated as points and distance metrics can use standard definitions . in symbolic domains , a more sophisticated treatment of the feature space is required . we introduce a nearest neighbor algorithm for learning in domains with symbolic features . our algorithm calculates distance tables that allow it to produce real valued distances between instances , and attaches weights to the instances to further modify the structure of feature space . we show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers predicting protein secondary structure , identifying dna promoter sequences , and pronouncing english text . direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains . in addition , our algorithm has advantages in training speed , simplicity , and perspicuity . we conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here .
a visual execution model for ada tasking . <eos> a visual execution model for ada tasking can help programmers attain a deeper understanding of the tasking semantics . it can illustrate subtleties in semantic definitions that are not apparent in natural language design . we describe a contour model of ada tasking that depicts asynchronous tasks ( threads of control ) , relationships between the environments in which tasks execute , and the manner in which tasks interact . the use of this high level execution model makes it possible to see what happens during execution of a program . the paper provides an introduction to the contour model of ada tasking and demonstrates its use .
a conceptual framework for evolving software processes . <eos> software processes are complex entities that may last for long periods of time and are carried out through the interaction of humans and computerized tools . they need to continuously evolve in order to cope with different kinds of changes or customizations both in the organization and in the technologies used to support software production activities . in recent years , many software process support technologies have been developed , and have currently been further extended and used in trial projects . moreover , some research prototypes have generated commercial products , that are marketed and currently used in industrial organizations . despite these significant efforts and results , however , there is still little conceptual characterization and assessment of the properties of software processes and related support environments . it is difficult to compare and assess existing approaches . even a common characterization of the problems to be addressed seems to be problematic and difficult to achieve . this is particularly true when we consider the process evolution problem , for which it does not seem that a common view of the issue has been established yet . this paper aims at proposing a conceptual framework to describe and assess flexible and evolving software processes . it is based on the assumption that a software process is composed of two main components a software production process to carry out software production activities , and a software meta process to improve and evolve the whole software process . the general requirements and properties of the process domain are first discussed , and the meta process concept is introduced . then , we discuss several process related concepts and , in particular , the relationship between the meta process and the rest of the software process . methods and technologies needed to support the meta process are highlighted and discussed . finally , we apply the resulting framework to an example , in order to show the potential and expected benefits of the proposed approach .
a comparison of adaptive wormhole routing algorithms . <eos> improvement of message latency and network utilization in torus interconnection networks by increasing adaptivity in wormhole routing algorithms is studied . a recently proposed partially adaptive algorithm and four new fully adaptive routing algorithms are compared with the well known e cube algorithm for uniform , hotspot , and local traffic patterns . our simulations indicate that the partially adaptive north last algorithm , which causes unbalanced traffic in the network , performs worse than the nonadaptive e cube routing algorithm for all three traffic patterns . another result of our study is that the performance does not necessarily improve with full adaptivity . in particular , a commonly discussed fully adaptive routing algorithm , which uses 2n virtual channels per physical channel of a k ary n cube , performs worse than e cube for uniform and hotspot traffic patterns . the other three fully adaptive algorithms , which give priority to messages based on distances traveled , perform much better than the e cube and partially adaptive algorithms for all three traffic patterns . one of the conclusions of this study is that adaptivity , full or partial , is not necessarily a benefit in wormhole routing .
a survey of x protocol multiplexors . <eos> an x multiplexor allows a single x window system client to be displayed and interacted with on several x servers simultaneously . such a service is necessary for the construction of a computer supported cooperative work ( cscw ) environment such as <unk> ( joint viewing and tele operation service ) which is being implemented within race ii project <unk> . this paper describes several existing x multiplexors and evaluates their usefulness for <unk> .
random walks in weyl chambers and the decomposition of tensor powers . <eos> we consider a class of random walks on a lattice , introduced by gessel and zeilberger , for which the reflection principle can be used to count the number of k step walks between two points which stay within a chamber of a weyl group . we prove three independent results about such reflectable walks first , a classification of all such walks semi second , many determinant formulas for walk numbers and their generating functions semi third , an equality between the walk numbers and the multiplicities of irreducibles in the kth tensor power of certain lie group representations associated to the walk types . our results apply to the defining representations of the classical groups , as well as some spin representations of the orthogonal groups .
a new theory of deadlock free adaptive routing in wormhole networks . <eos> the theoretical background for the design of deadlock free adaptive routing <unk> wormhole networks is developed . the author proposes some basic definitions and <unk> . these create the conditions to verify that an adaptive algorithm <unk> free , even when there are cycles in the channel dependency graph . two <unk> are also proposed . the first supplies algorithms with a high degree offreedom , without increasing the number of physical channels . the second methodology <unk> for the design of fault tolerant algorithms . some examples are given to show theapplication of the methodologies . simulations show the performance improvement thatcan be achieved by designing the routing algorithms with the new theory .
theory and practice of vector quantizers trained on small training sets . <eos> examines how the performance of a memoryless vector quantizer changes as a function of its training set size . specifically , the authors study how well the training set distortion predicts test distortion when the training set is a randomly drawn subset of blocks from the test or training image ( s ) . using the vapnik chervonenkis ( vc ) dimension , the authors derive formal bounds for the difference of test and training distortion of vector quantizer codebooks . the authors then describe extensive empirical simulations that test these bounds for a variety of codebook sizes and vector dimensions , and give practical suggestions for determining the training set size necessary to achieve good generalization from a codebook . the authors conclude that , by using training sets comprising only a small fraction of the available data , one can produce results that are close to the results obtainable when all available data are used .
a nested graph model for the representation and manipulation of complex objects . <eos> three recent trends in database research are object oriented and deductive databases and graph based user interfaces . we draw these trends together in a data model we call the hypernode model . the single data structure of this model is the hypernode , a graph whose nodes can themselves be graphs . hypernodes are typed , and types , too , are nested graphs . we give the theoretical foundations of hypernodes and types , and we show that type checking is tractable . we show also how conventional type forming operators can be simulated by our graph types , including cyclic types . the hypernode model comes equipped with a rule based query language called hyperlog , which is complete with respect to computation and update . we define the operational semantics of hyperlog and show that the evaluation can be performed efficiently . we discuss also the use of hyperlog for supporting database browsing , an essential feature of hypertext databases . we compare our work with other graph based data <unk> previous graph based models , the hypernode model provides inherent support for data abstraction via its nesting of graphs . finally , we briefly discuss the implementation of a dbms based on the hypernode model .
a really temporal logic . <eos> we introduce a temporal logic for the specification of real time systems . our logic , tptl , employs a novel quantifier construct for referencing time the freeze quantifier binds a variable to the time of the local temporal context .
the elusive atomic register . <eos> we present a construction of a single writer , multiple reader atomic register from single writer , single reader atomic registers . the complexity of our construction is asymptotically optimal o ( m2 shared single writer , single reader safe bits are required to construct a single writer , m reader , n bit atomic register .
parallel linear programming in fixed dimension almost surely in constant time . <eos> for any fixed dimension ) time , information processing letters , v . <digit> n . <digit> , p . <digit> <digit> , january <digit> , <digit>
explaining type errors in polymorphic languages . <eos> strongly typed languages present programmers with compile time feedback about the type correctness of programs . errors during polymorphic type checking take the form of a unification failure for two types . finding the source of the type error in the code is often difficult because the error may occur far from the spot where the inconsistency is detected . as functional languages use more and more complex type systems , the difficulty of interpreting and locating these errors will increase . to locate the source of type errors the programmer must unravel the long chain of deductions and type instantiations made during type reconstruction . this paper describes an approach that maintains the deductive steps of type inference and the reasons for type instantiations . the approach could be used in an interactive system to guide the programmer to the source of a type error or to explain why the compiler assigned a particular type to an expression .
executable interprocedural slices . <eos> the notion of a program slice , originally introduced by mark weiser , is useful in program debugging , automatic parallelization , program integration , and software maintenance . a slice of a program is taken with respect to a program point p and a variable x the slice consists of all statements of the program that might affect the value of x at point p . an interprocedural slice is a slice of an entire program , where the slice crosses the boundaries of procedure calls .
precise and efficient groundness analysis for logic programs . <eos> we show how precise groundness information can be extracted from logic programs . the idea is to use abstract interpretation with boolean functions as approximations to groundness dependencies between variables . this idea is not new , and different classes of boolean functions have been used . we argue , however , that one class , the positive functions , is more suitable than others . positive boolean functions have a certain property which we ( inspired by a . langen ) call condensation . this property allows for rapid computation of groundness information .
semantics of constraint logic programs with optimization . <eos> many applications of constraint logic programming ( clp ) languages require not only testing if a set of constraints is satisfiable , but also finding the optimal solution which satisfies them . unfortunately , the standard declarative semantics for clp languages does not consider optimization but only constraint satisfaction . here we give a model theoretic semantics for optimization , which is a simple extension of the standard semantics , and a corresponding operational semantics , which may be efficiently implemented .
dynamic nurbs with geometric constraints for interactive sculpting . <eos> this article develops a dynamic generalization of the nonuniform rational b spline ( nurbs ) model . nurbs have become a defacto standard in commercial modeling systems because of their power to represent free form shapes as well as common analytic shapes . to date , however , they have been viewed as purely geometric primitives that require the user to manually adjust multiple control points and associated weights in order to design shapes . dynamic nurbs , or d nurbs , are physics based models that incorporate mass distributions , internal deformation energies , and other physical quantities into the popular nurbs geometric substrate . using d nurbs , a modeler can interactively sculpt curves and surfaces and design complex shapes to required specifications not only in the traditional indirect fashion , by adjusting control points and weights , but also through direct physical manipulation , by applying simulated forces and local and global shape constraints . d nurbs move and deform in a physically intuitive manner in response to the user ' s direct manipulations . their dynamic behavior results from the numerical integration of a set of nonlinear differential equations that automatically evolve the control points and weights in response to the applied forces and constraints . to derive these equations , we employ lagrangian mechanics and a finite element like discretization . our approach supports the trimming of d nurbs surfaces using d nurbs curves . we demonstrate d nurbs models and constraints in applications including the rounding of solids , optimal surface fitting to unstructured data , surface design from cross sections , and free form deformation . we also introduce a new technique for 2d shape metamorphosis using constrained d nurbs surfaces .
multiresolution stochastic hybrid shape models with fractal priors . <eos> 3d shape modeling has received enormous attention in computer graphics and computer vision over the past decade . several shape modeling techniques have been proposed in literature , some are local ( distributed parameter ) while others are global ( lumped parameter ) in terms of the parameters required to describe the shape . hybrid models that combine both ends of this parameter spectrum have been in vogue only recently . however , they do not allow a smooth transition between the two extremes of this parameter spectrum .
complexity restricted advice functions . <eos> the authors consider uniform subclasses of the nonuniform complexity classes defined by karp and lipton l ' <unk> . math . , <digit> ( <digit> ) via the notion of advice functions . these subclasses are obtained by restricting the complexity of computing correct advice . also , the effect of allowing advice functions of limited complexity to depend on the input rather than on the input ' s length is investigated . among other results , using the notions described above , new characterizations of ( a ) np np cap sparse , ( b ) np with a restricted access to an np oracle , and ( c ) the odd levels of the boolean hierarchy are given . as a consequence , it is shown that every set that is nondeterministically truth table reducible to sat in the sense of rich j . comput . system sci . , <digit> ( <digit> ) , pp . <digit> <digit> is already deterministically truth table reducible to sat . furthermore , it turns out that the np reduction classes of bounded versions of this reducibility coincide with the odd levels of the boolean hierarchy .
generating linear extensions fast . <eos> one of the most important sets associated with a poset cal p is its set of linear extensions , e ( cal p ) . this paper presents an algorithm to generate all of the linear extensions of a poset in constant amortized time , that is , in time o ( e ( cp ) ) , where e ( cp ) . the fastest previously known algorithm for generating the linear extensions of a poset runs in time o ( n cdot e ( cp ) ) , where n is the number of elements of the poset . the algorithm presented here is the first constant amortized time algorithm for generating a naturally defined class of combinatorial objects for which the corresponding counting problem is p complete . furthermore , it is shown that linear extensions can be generated in constant amortized time where each extension differs from its predecessor by one or two adjacent transpositions . the algorithm is practical and can be modified to count linear extensions efficiently and to compute p ( x < y ) , for all pairs x , y , in time o ( n <digit> e ( cal p ) ) .
computational complexity of sparse rational interpolation . <eos> the authors analyze the computational complexity of sparse rational interpolation , and give the first deterministic algorithm for this problem with singly exponential bounds on the number of arithmetic operations .
tight upper and lower bounds on the path length of binary trees . <eos> the external path length of a tree t is the sum of the lengths of the paths from the root to each external node . the maximal path length difference , delta , is the difference between the lengths of the longest and shortest such paths . tight lower and upper bounds are proved on the external path length of binary trees with n external nodes and maximal path length difference delta is prescribed . in particular , an upper bound is given that , for each value of delta , can be exactly achieved for infinitely many values of n . this improves on the previously known upper bound that could only be achieved up to a factor proportional to n . an elementary proof of the known upper bound is also presented as a preliminary result . moreover , a lower bound is proved that can be exactly achieved for each value of n and delta leq n <digit> .
the complexity of decision versus search . <eos> a basic question about np is whether or not search reduces in polynomial time to decision . this paper indicates that the answer is negative under a complexity assumption ( that deterministic and nondeterministic double exponential time are unequal ) a language in np for which search does not reduce to decision is constructed . these ideas extend in a natural way to interactive proofs and program checking . under similar assumptions , the authors present languages in np for which it is harder to prove membership interactively than it is to decide this membership , and languages in np that are not checkable .
piecewise linear interpolation between polygonal slices . <eos> in this paper we present a new technique for piecewise linear surface reconstruction from a series of parallel polygonal cross sections . this is an important problem in medical imaging , surface reconstruction from topographic data , and other applications . we reduce the problem , as in most previous works , to a series of problems of piecewise linear interpolation between each pair of successive slices . our algorithm uses a partial curve matching technique for matching parts of the contours , an optimal triangulation of <digit> d polygons for resolving the unmatched parts , and a minimum spanning tree heuristic for interpolating between non simply connected regions . unlike previous attempts at solving this problem , our algorithm seems to handle successfully any kind of data . it allows multiple contours in each slice , with any hierarchy of contour nesting , and avoids the introduction of counter intuitive bridges between contours , proposed in some earlier papers to handle interpolation between multiply connected regions . experimental results on various complex examples , involving actual medical imaging data , are presented , and show the good and robust performance of our algorithm .
improvements to graph coloring register allocation . <eos> we describe two improvements to chaitin style graph coloring register allocators . the first , optimistic coloring , uses a stronger heuristic to find a k coloring for the interference graph . the second extends chaitin ' s treatment of rematerialization to handle a larger class of values . these techniques are complementary . optimistic coloring decreases the number of procedures that require spill code and reduces the amount of spill code when spilling is unavoidable . rematerialization lowers the cost of spilling some values . this paper describes both of the techniques and our experience building and using register allocators that incorporate them . it provides a detailed description of optimistic coloring and rematerialization . it presents experimental data to show the performance of several versions of the register allocator on a suite of fortran programs . it discusses several insights that we discovered only after repeated implementation of these allocators .
model checking and modular verification . <eos> we describe a framework for compositional verification of finite state processes . the framework is based on two ideas a subset of the logic ctl for which satisfaction is preserved under composition , and a preorder on structures which captures the relation between a component and a system containing the component . satisfaction of a formula in the logic corresponds to being below a particular structure ( a tableau for the formula ) in the preorder . we show how to do assume guarantee style reasoning within this framework . additionally , we demonstrate efficient methods for model checking in the logic and for checking the preorder in several special cases . we have implemented a system based on these methods , and we use it to give a compositional verification of a cpu controller .
coordinating first order multiparty interactions . <eos> a first order multiparty interaction is an abstraction mechanism that defines communication among a set of formal process roles . actual processes participate in a first order interaction by enroling into roles , and execution of the interaction can proceed when all roles are filled by distinct processes . as in csp , <unk> statements can serve as guards in alternative commands . the <unk> guard scheduling problem then is to enable the execution of first order interactions through the judicious scheduling of roles to processes that are currently ready to execute <unk> guards .
controlled grammatic ambiguity . <eos> a new approach to ambiguity of context free grammars is presented , and within this approach the ll and lr techniques are generalized to solve the following problems for large classes of ambiguous grammars
reducing indirect function call overhead in c programs . <eos> modern computer architectures increasingly depend on mechanisms that estimate future control flow decisions to increase performance . mechanisms such as speculative execution and prefetching are becoming standard architectural mechanisms that rely on control flow prediction to prefetch and speculatively execute future instructions . at the same time , computer programmers are increasingly turning to object oriented languages to increase their productivity . these languages commonly use run time dispatching to implement object polymorphism . dispatching is usually implemented using an indirect function call , which presents challenges to existing control flow prediction techniques .
faster approximation algorithms for the unit capacity concurrent flow problem with applications to routing and finding sparse cuts . <eos> this paper describes new algorithms for approximately solving the concurrent multicommodity flow problem with uniform capacities . these algorithms are much faster than algorithms discovered previously . besides being an important problem in its own right , the uniform capacity concurrent flow problem has many interesting applications . leighton and rao used uniform capacity concurrent flow to find an approximately sparsest cut in a graph and thereby approximately solve a wide variety of graph problems , including minimum feedback arc set , minimum cut linear arrangement , and minimum area layout . however , their method appeared to be impractical as it required solving a large linear program . this paper shows that their method might be practical by giving an o ( m <digit> log m ) expected time randomized algorithm for their concurrent flow problem on an m edge graph . raghavan and thompson used uniform capacity concurrent flow to solve approximately a channel width minimization problem in very large scale integration . an randomized algorithm and an o ( k min n , k ( m n log n ) log k ) deterministic algorithm is given for this problem when the channel width is omega ( log n ) , where k denotes the number of wires to be routed in an n node , m edge network .
implementing complex elementary functions using exception handling . <eos> algorithms are developed for reliable and accurate evaluations of the complex elementary functions required in fortran <digit> and fortran <digit> , namely , cabs , <unk> , <unk> , clog , <unk> , and ccos . the algorithms are presented in a pseudocode that has a convenient exception handling facility . a tight error bound is derived for each algorithm . corresponding fortran programs for an ieee environment have also been developed to illustrate the practicality of the algorithms , and these programs have been tested very carefully to help confirm the correctness of the algorithms and their error bounds . the results of these tests are included in the paper , but the fortran programs are not .
using genetic algorithms for concept learning . <eos> in this article , we explore the use of genetic algorithms ( gas ) as a key element in the design and implementation of robust concept learning systems . we describe and evaluate a ga based system called gabil that continually learns and refines concept classification rules from its interaction with the environment . the use of gas is motivated by recent studies showing the effects of various forms of bias built into different concept learning systems , resulting in systems that perform well on certain concept classes ( generally , those well matched to the biases ) and poorly on others . by incorporating a ga as the underlying adaptive search mechanism , we are able to construct a concept learning system that has a simple , unified architecture with several important features . first , the system is surprisingly robust even with minimal bias . second , the system can be easily extended to incorporate traditional forms of bias found in other concept learning systems . finally , the architecture of the system encourages explicit representation of such biases and , as a result , provides for an important additional feature the ability to dynamically adjust system bias . the viability of this approach is illustrated by comparing the performance of gabil with that of four other more traditional concept learners ( <unk> , c4 . <digit> , <unk> , and <unk> ) on a variety of target concepts . we conclude with some observations about the merits of this approach and about possible extensions .
extracting refined rules from knowledge based neural networks . <eos> neural networks , despite their empirically proven abilities , have been little used for the refinement of existing knowledge because this task requires a three step process . first , knowledge must be inserted into a neural network . second , the network must be refined . third , the refined knowledge must be extracted from the network . we have previously described a method for the first step of this process . standard neural learning techniques can accomplish the second step . in this article , we propose and empirically evaluate a method for the final , and possibly most difficult , step . our method efficiently extracts symbolic rules from trained neural networks . the four major results of empirical tests of this method are that the extracted rules <digit> ) closely reproduce the accuracy of the network from which they are extracted semi <digit> ) are superior to the rules produced by methods that directly refine symbolic rules semi are superior to those produced by previous techniques for extracting rules from trained neural networks semi and are human comprehensible . thus , this method demonstrates that neural networks can be used to effectively refine symbolic knowledge . moreover , the rule extraction technique developed herein contributes to the understanding of how symbolic and connectionist approaches to artificial intelligence can be profitably integrated .
factorization of matrix polynomials with symmetries . <eos> an n times n matrix polynomial l ( lambda ) ( with real or complex coefficients ) is called self adjoint if factorizations of selfadjoint and symmetric matrix polynomials of the form are studied , where d is a constant matrix and m ( lambda ) is a matrix polynomial . in particular , the minimal possible size of d is described in terms of the elementary divisors of l ( lambda ) and ( sometimes ) signature of the hermitian values of l ( lambda ) .
combining static and dynamic scheduling on distributed memory multiprocessors . <eos> loops are a large source of parallelism for many numerical applications . an important issue in the parallel execution of loops is how to schedule them so that the workload is well balanced among the processors . most existing loop scheduling algorithms were designed for shared memory multiprocessors , with uniform memory access costs . these approaches are not suitable for distributed memory multiprocessors where data locality is a major concern and communication costs are high . this paper presents a new scheduling algorithm in which data locality is taken into account . our approach combines both worlds , static and dynamic scheduling , in a two level ( overlapped ) fashion . this way data locality is considered and communication costs are limited . the performance of the new algorithm is evaluated on a cm <digit> message passing distributed memory multiprocessor .
using virtual lines to enhance locality exploitation . <eos> because the spatial locality of numerical codes is significant , the potential for performance improvements is important . however , large cache lines can not be used in current on chip data caches because of the important pollution they breed . in this paper , we propose a hardware design , called the virtual line scheme , that allows the utilization of large virtual cache lines when fetching data from memory for better exploitation of spatial locality , while the actual physical cache line is smaller than currently found cache lines for better exploitation of temporal locality . simulations show that a <digit> % to <digit> % reduction of the average memory access time can be obtained for a <digit> cycle memory latency . it is also shown how simple software informations can be used to significantly decrease memory traffic , a flaw associated with the utilization of large cache lines .
an approach to communication efficient data redistribution . <eos> we address the development of efficient methods for performing data redistribution of arrays on distributed memory machines . data redistribution is important for the distributed memory implementation of data parallel languages such as high performance fortran . an algebraic representation of regular data distributions is used to develop an analytical model for evaluating the communication cost of data redistribution . using this algebraic representation and the analytical model , an approach to communication efficient data redistribution is developed . implementation results on the intel ipsc <digit> are reported .
measure , stochasticity , and the density of hard languages . <eos> the main theorem of this paper is that , for every real number alpha < <digit> ( e . g . , alpha <digit> . <digit> ) , only a measure <digit> subset of the languages decidable in exponential time are leq p n alpha tt reducible to languages that are not exponentially dense . thus every leq p n alpha tt hard language for e is exponentially dense . this strengthens watanabe ' s <digit> result , that every leq p ( o log n ) tt hard language for e is exponentially dense . the combinatorial technique used here , the sequentially most frequent query selection , also gives a new , simpler proof of watanabe ' s result . the main theorem also has implications for the structure of np under strong hypotheses . ogiwara and watanabe ( <digit> ) have shown that the hypothesis p ne np implies that every leq p btt hard language for np is nonsparse ( i . e . , not polynomially sparse ) . their technique does not appear to allow significant relaxation of either the query bound or the sparseness criterion . it is shown here that a stronger hypothesis namely , that np does not have measure <digit> in exponential time implies the stronger conclusion that , for every real alpha < <digit> , every leq p n alpha tt hard language for np is exponentially dense . evidence is presented that this stronger hypothesis is reasonable . the proof of the main theorem uses a new , very general weak stochasticity theorem , ensuring that almost every language in e is statistically unpredictable by feasible deterministic algorithms , even with linear nonuniform advice .
cache interference phenomena . <eos> the impact of cache interferences on program performance ( particularly numerical codes , which heavily use the memory hierarchy ) remains unknown . the general knowledge is that cache interferences are highly irregular , in terms of occurrence and intensity . in this paper , the different types of cache interferences that can occur in numerical loop nests are identified . an analytical method is developed for detecting the occurrence of interferences and , more important , for computing the number of cache misses due to interferences . simulations and experiments on real machines show that the model is generally accurate and that most interference phenomena are captured . experiments also show that cache interferences can be intense and frequent . certain parameters such as array base addresses or dimensions can have a strong impact on the occurrence of interferences . modifying these parameters only can induce global execution time variations of <digit> % and more . applications of these modeling techniques are numerous and range from performance evaluation and prediction to enhancement of data locality optimizations techniques .
an exploratory evaluation of three interfaces for browsing large hierarchical tables of contents . <eos> three different interfaces were used to browse a large ( <digit> items ) table of contents . a fully expanded stable interface , expand contract interface , and <unk> interface were studied in a between groups experiment with <digit> novice participants . nine timed fact retrieval tasks were performed each task is analyzed and discussed separately . we found that both the expand contract and <unk> interfaces produced significantly faster times than the stable interface for many tasks using this large hierarchy other advantages of the expand contract and <unk> interfaces over the stable interface are discussed . the animation characteristics of the expand contract interface appear to play a major role . refinements to the <unk> and expand contract interfaces are suggested . a predictive model for measuring navigation effort of each interface is presented .
fixpoint computation for polyvariant static analyses of higher order applicative programs . <eos> this paper presents an optimized general purpose algorithm for polyvariant , static analyses of higher order applicative programs . a polyvariant analysis is a very accurate form of analysis that produces many more abstract descriptions for a program than does a conventional analysis . it may also compute intermediate abstract descriptions that are irrelevant to the final result of the analysis . the optimized algorithm addresses this overhead while preserving the accuracy of the analysis . the algorithm is also parameterized over both the abstract domain and degree of polyvariance . we have implemented an instance of our algorithm and evaluated its performance compared to the unoptimized algorithm . our implementation runs significantly faster on average than the other algorithm for benchmarks reported here .
a framework for expressing the relationships between multiple views in requirements specification . <eos> composite systems are generally comprised of heterogeneous components whose specifications are developed by many development participants . the requirements of such systems are invariably elicited from multiple perspectives that overlap , complement , and contradict each other . furthermore , these requirements are generally developed and specified using multiple methods and notations , respectively . it is therefore necessary to express and check the relationships between the resultant specification fragments . we deploy multiple viewpoints that hold partial requirements specifications , described and developed using different representation schemes and development strategies . we discuss the notion of inter viewpoint communication in the context of this viewpoints framework , and propose a general model for viewpoint interaction and integration . we elaborate on some of the requirements for expressing and enacting inter viewpoint relationships the vehicles for consistency checking and inconsistency management . finally , though we use simple fragments of the requirements specification method core to illustrate various components of our work , we also outline a number of larger case studies that we have used to validate our framework . our computer based viewpoints support environment , the viewer , is also briefly described .
prototyping a process monitoring experiment . <eos> features are often the basic unit of development for a very large software system and represent long term efforts , spanning up to several years from inception to actual use . developing an experiment to monitor ( by means of sampling ) such lengthy processes requires a great deal of care in order to minimize casts and to maximize benefits . just as prototyping is often a necessary auxiliary step in a large scale , long term development effort , so , too , is prototyping a necessary step in the development of a large scale , long term process monitoring experiment . therefore , we have prototyped our experiment using a representative process and reconstructed data from a large and rich feature development . this approach has yielded three interesting sets of results . first , we reconstructed a <digit> month time diary for the lead engineer of a feature composed of both hardware and software . these data represent the daily state ( where the lead engineer spent the majority of his time ) for a complete cycle of the development process . second , we found that we needed to modify our experimental design . our initial set of states did not represent the data as well as we had hoped . this is exemplified by the fact that the other category is too large . finally , the data provide evidence for both a waterfall view and an interactive , cyclic view of software development . we conclude that the prototyping effort is a necessary part of developing and installing any large scale process monitoring experiment .
serial array time slot interchangers and optical implementations . <eos> we consider time slot interchangers ( tsis ) which are built from <digit> spl times <digit> exchange switches and delays and which are useful for time division multiplexed ( tdm ) systems in telecommunications and pipelined systems such as time multiplexed optical multiprocessors . we formulate a general method for constructing tsis based on multistage interconnection networks in the space domain via space to time mapping . two types of tsis , time slot <unk> and time slot sorters , are considered . we review the time slot permuter based on the benes network , and obtain the spl <unk> tilde time slot permuter based on the bit controlled , self routing spl lambda permutation network . the time slot sorter , s sub n , is obtained from the batcher spatial sorting network . the generalized lambda time slot permuter spl <unk> <unk> q is obtained , in an algorithmic approach , by combining the idea of the spl <unk> tilde time slot permuter and q way bitonic decomposition ( q <digit> sup q ) . the numbers of switches , control complexities , and frame delays of these architectures are compared , and the problem of crosstalk in optical implementation is discussed . it is shown that control complexity can be traded against the number of switches .
request combining in multiprocessors with arbitrary interconnection networks . <eos> several techniques have been proposed to allow parallel access to a shared <unk> by combining requests . they have one or more of the following attributes requirements for a priori knowledge of the request to combine , restrictions on the <unk> messages in the network , or the use of sophisticated interconnection network nodes . we present a new method of combining requests that does not have the <unk> . we obtain this new method for request combining by developing <unk> scheme for the existing methods of request combining . this <unk> is facilitated by separating the request combining process into a two <unk> determining the combining set , which is the set of requests that participate ina combined access and distributing the results of the combined access to the <unk> the combining set . the classification of combining strategies is based upon <unk> component , processor elements , or interconnection network performs each ofthese tasks . our approach , which uses the interconnection network to establish <unk> set and the processor elements to distribute the results , lies in an <unk> of the design space . we also present simulation results to assess the benefits of theproposed approach .
fault tolerant routing in mesh architectures . <eos> it is important for a distributed computing system to be able to route messages <unk> faulty links or nodes may be present . we present a fault tolerant routingalgorithm that assures the delivery of every message as long as there is a path <unk> source and destination . the algorithm works on many common mesh <unk> as the torus and hexagonal mesh . the proposed scheme can also detect <unk> of a path between a pair of nodes in a finite amount of time . moreover , <unk> requires each node in the system to know only the state ( faulty or not ) of eachof its own links . the performance of the routing scheme is simulated for both square <unk> meshes while varying the physical distribution of faulty components . it <unk> that a shortest path between the source and destination of each message <unk> with a high probability , and , if a path exists , it is usually found very quickly .
improving generalization with active learning . <eos> active learning differs from learning from examples in that the learning algorithm assumes at least some control over what part of the input domain it receives information about . in some situations , active learning is provably more powerful than learning from examples alone , giving better generalization for a fixed number of training examples . in this article , we consider the problem of learning a binary concept in the absence of noise . we describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network . in selective sampling , a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers useful . we test our implementation , called an sg network , on three domains and observe significant improvement in generalization .
index transformation algorithms in a linear algebra framework . <eos> we present a linear algebraic formulation for a class of index transformations such <unk> code encoding and decoding , matrix transpose , bit reversal , vector reversal , shuffles , and other index or dimension permutations . this formulation unifies , simplifies , and can be used to derive algorithms for hypercube multiprocessors . we show how all the widely known properties of gray codes , and some not so well known properties as well , can be derived using this framework . using this framework , we relate hypercube communications algorithms to gauss jordan elimination on a matrix of <digit> ' s and <digit> ' s .
building information systems for mobile environments . <eos> it is expected that in the near future , tens of millions of users will have access to distributed information systems through wireless connections . the technical characteristics of the wireless medium and the resulting mobility of both data resources and data consumers raise new challenging questions regarding the development of information systems appropriate for mobile environments . in this paper , we report on the development of such a system . first , we describe the general architecture of the information system and the main considerations of our design . then , based on these considerations , we present our system support for maintaining the consistency of replicated data and for providing transaction schemas that account for the frequent but predictable disconnections , the mobility , and the vulnerability of the wireless environment .
line search algorithms with guaranteed sufficient decrease . <eos> the development of software for minimization problems is often based on a line search method . we consider line search methods that satisfy sufficient decrease and curvature conditions , and formulate the problem of determining a point that satisfies these two conditions in terms of finding a point in a set t ( mgr ) . we describe a search algorithm for this problem that produces a sequence of iterates that converge to a point in t ( mgr ) and that , except for pathological cases , terminates in a finite number of steps . numerical results for an implementation of the search algorithm on a set of test functions show that the algorithm terminates within a small number of iterations .
bounds and error estimates for radiosity . <eos> we present a method for determining a posteriori bounds and estimates for local and total errors in radiosity solutions . the ability to obtain bounds and estimates for the total error is crucial fro reliably judging the acceptability of a solution . realistic estimates of the local error improve the efficiency of adaptive radiosity algorithms , such as hierarchical radiosity , by indicating where adaptive refinement is necessary . first , we describe a hierarchical radiosity algorithm that computes conservative lower and upper bounds on the exact radiosity function , as well as on the approximate solution . these bounds account for the propagation of errors due to interreflections , and provide a conservative upper bound on the error . we also describe a non conservative version of the same algorithm that is capable of computing tighter bounds , from which more realistic error estimates can be obtained . finally , we derive an expression for the effect of a particular interaction on the total error . this yields a new error driven refinement strategy for hierarchical radiosity , which is shown to be superior to brightness weighted refinement .
a framework for the analysis of error in global illumination algorithms . <eos> in this paper we identify sources of error in global illumination algorithms and derive bounds for each distinct category . errors arise from three sources inaccuracies in the boundary data , discretization , and computation . boundary data consists of surface geometry , reflectance functions , and emission functions , all of which may be perturbed by errors in measurement or simulation , or by simplifications made for computational efficiency . discretization error is introduced by replacing the continuous radiative transfer equation with a finite dimensional linear system , usually by means of boundary elements and a corresponding projection method . finally , computational errors perturb the finite dimensional linear system through imprecise form factors , inner products , visibility , etc . , as well as by halting iterative solvers after a finite number of steps . using the error taxonomy introduced in the paper we examine existing global illumination algorithms and suggest new avenues of research .
spreadsheets for images . <eos> we describe a data visualization system based on spreadsheets . cells in our spreadsheet contain graphical objects such as images , volumes , or movies . cells may also contain widgets such as buttons , sliders , or curve editors . objects are displayed in miniature inside each cell . formulas for cells are written in a general purpose programming language ( tcl ) augmented with operators for array manipulation , image processing , and rendering . compared to flow chart visualization systems , spreadsheets are more expressive , morescalable , and easier to program . compared to conventional numerical spreadsheets , spreadsheets for images pose several unique design problems larger formulas , longer computation times , and more complicated intercelldependencies . in response to these problems , we have extended the spreadsheet paradigm in three ways formulas can display their results anywhere in the spreadsheet , cells can be selectively disabled , and multiple cells can be edited at once . we discuss these extensions and their implications , and we also point out some unexpected uses for our spreadsheets as a visual database browser , as a graphical user interface builder , as a smart clipboard for the desktop , and as a presentation tool .
generalization of lambert ' ' s reflectance model . <eos> lambert ' s model for body reflection is widely used in computer graphics . it is used extensively by rendering techniques such as radiosity and ray tracing . for several real world objects , however , lambert ' s model can prove to be a very inaccurate approximation to the body reflectance . while the brightness of a lambertian surface is independent of viewing direction , that of a rough surface increases as the viewing direction approaches the light source direction . in this paper , a comprehensive model is developed that predicts body reflectance from rough surfaces . the surface is modeled as a collection of lambertian facets . it is shown that such a surface is inherently non lambertian due to the foreshortening of the surface facets . further , the model accounts for complex geometric and radiometric phenomena such as masking , shadowing , and interreflections between facets . several experiments have been conducted on samples of rough diffuse surfaces , such as , plaster , sand , clay , and cloth . all these surface demonstrate significant deviation from lambertian behavior . the reflectance measurements obtained are in strong agreement with the reflectance predicted by the model .
free form shape design using triangulated surfaces . <eos> we present an approach to modeling with truly mutable yet completely controllable free form surfaces of arbitrary topology . surfaces may be pinned down at points and along curves , cut up and smoothly welded back together , and faired and reshaped in the large . this style of control is formulated as a constrained shape optimization , with minimization of squared principal curvatures yielding graceful shapes that are free of the parameterization worries accompanying many patch based approaches . triangulated point sets are used to approximate these smooth variational surfaces , bridging the gap between patch based and particle based representations . automatic refinement , mesh smoothing , and re triangulation maintain a good computational mesh as the surface shape evolves , and give sample points and surface features much of the freedom to slide around in the surface that oriented particles enjoy . the resulting surface triangulations are constructed and maintained in real time .
the <unk> approach to testing object oriented programs . <eos> this article describes a new approach to the unit testing of object oriented programs , a set of tools based on this approach , and two case studies . in this approach , each test case consists of a tuple of sequences of messages , along with tags indicating whether these sequences should put objects of the class under test into equivalent states and or return objects that are in equivalent states . tests are executed by sending the sequences to objects of the class under test , then invoking a user supplied equivalence checking mechanism . this approach allows for substantial automation of many aspects of testing , including test case generation , test driver generation , test execution , and test checking . experimental prototypes of tools for test generation and test execution are described . the test generation tool requires the availability of an algebraic specification of the abstract data type being tested , but the test execution tool can be used when no formal specification is available . using the test execution tools , case studies involving execution of tens of thousands of test cases , with various sequence lengths , parameters , and combinations of operations were performed . the relationships among likelihood of detecting an error and sequence length , range of parameters , and relative frequency of various operations were investigated for priority queue and sorted list implementations having subtle errors . in each case , long sequences tended to be more likely to detect the error , provided that the range of parameters was sufficiently large and likelihood of detecting an error tended to increase up to a threshold value as the parameter range increased .
multiresolution curves . <eos> we describe a multiresolution curve representation , based on wavelets , that conveniently supports a variety of operations smoothing a curve editing the overall form of a curve while preserving its details and approximating a curve within any given error tolerance for scan conversion . we present methods to support continuous levels of smoothing as well as direct manipulation of an arbitrary portion of the curve the control points , as well as the discrete nature of the underlying hierarchical representation , can be hidden from the user . the multiresolution representation requires no extra storage beyond that of the original control points , and the algorithms using the representation are both simple and fast .
a graphical interval logic for specifying concurrent systems . <eos> this article describes a graphical interval logic that is the foundation of a tool set supporting formal specification and verification of concurrent software systems . experience has shown that most software engineers find standard temporal logics difficult to understand and use . the objective of this article is to enable software engineers to specify and reason about temporal properties of concurrent systems more easily by providing them with a logic that has an intuitive graphical representation and with tools that support its use . to illustrate the use of the graphical logic , the article provides some specifications for an elevator system and proves several properties of the specifications . the article also describes the tool set and the implementation .
using particles to sample and control implicit surfaces . <eos> we present a new particle based approach to sampling and controlling implicit surfaces . a simple constraint locks a set of particles onto a surface while the particles and the surface move . we use the constraint to make surfaces follow particles , and to make particles follow surfaces . we implement control points for direct manipulation by specifying particle motions , then solving for surface motion that maintains the constraint . for sampling and rendering , we run the constraint in the order direction , creating floater particles that roam freely over the surface . local repulsion is used to make floaters spread evenly across the surface . by varying the radius of repulsion adaptively , and fissioning or killing particles based on the local density , we can achieve good sampling distributions very rapidly , and maintain them even in the face of rapid and extreme deformations and changes in surface topology .
piecewise smooth surface reconstruction . <eos> we present a general method for automatic reconstruction of accurate , concise , piecewise smooth surface models from scattered range data . the method can be used in a variety of applications such as reverse <unk> automatic generation of cad models from physical objects . novel aspects of the method are its ability to model surfaces of arbitrary topological type and to recover sharp features such as creases and corners . the method has proven to be effective , as demonstrated by a number of examples using both simulated and real data . a key ingredient in the method , and a principal contribution of this paper , is the introduction of a new class of piecewise smooth surface representations based on subdivision . these surfaces have a number of properties that make them ideal for use in surface reconstruction they are simple to implement , they can model sharp features concisely , and they can be fit to scattered range data using an unconstrained optimization procedure .
a clustering algorithm for radiosity in complex environments . <eos> we present an approach for accelerating hierarchical radiosity by clustering objects . previous approaches constructed effective hierarchies by subdividing surfaces , but could not exploit a hierarchical grouping on existing surfaces . this limitation resulted in an excessive number of initial links in complex environments . initial linking is potentially the most expensive portion of hierarchical radiosity algorithms , and constrains the complexity of the environments that can be simulated . the clustering algorithm presented here operates by estimating energy transfer between collections of objects while maintaining reliable error bounds on each transfer . two methods of bounding the transfers are employed with different tradeoffs between accuracy and time . in contrast with the o ( s2 ) time and space complexity of the initial linking in previous hierarchical radiosity algorithms , the new methods have complexities of o ( <unk> ) and o ( s ) for both time and space . using these methods we have obtained speedups of two orders of magnitude for environments of moderate complexity while maintaining comparable accuracy .
visual techniques for traditional and multimedia layouts . <eos> character user interfaces ( cui ) generally display only pieces of text and semi graphical objects , whereas graphical user interfaces ( gui ) display interaction objects ( io ) such as icons , check boxes , list boxes , radio buttons and push buttons . traditional gui do not often go beyond such existing io . multimedia gui add interactive objects such as pictures , images , video sequences that could serve as a base for sophisticated user interaction . all these types of user interfaces have in common the problem of determining a basic layout of io . the complexity of this problem is proportional to the variety of io accessible for the designer . this paper summarises visual techniques exported from the area of visual design to be further exploited for user interface . these visual techniques provide the designer a wide range of means for laying out io . a small set of guidelines for effectively applying these visual techniques is given .
an architecture for an extensible 3d interface toolkit . <eos> this paper presents the architecture for an extensible toolkit used in construction and rapid prototyping of three dimensional interfaces , interactive illustrations , and three dimensional widgets . the toolkit provides methods for the direct manipulation of 3d primitives which can be linked together through a visual programming language to create complex constrained behavior . features of the toolkit include the ability to visually build , encapsulate , and parameterize complex models , and impose limits on the models . the toolkit ' s constraint resolution technique is based on a dynamic object model similar to those in prototype delegation object systems . the toolkit has been used to rapidly prototype tools for mechanical modelling , scientific visualization , construct 3d widgets , and build mathematical illustrations .
extending a graphical toolkit for two handed interaction . <eos> multimodal interaction combines input from multiple sensors such as pointing devices or speech recognition systems , in order to achieve more fluid and natural interaction . two handed interaction has been used recently to enrich graphical interaction . building applications that use such combined interaction requires new software techniques and frameworks . using additional devices means that user interface toolkits must be more flexible with regard to input devices and event types . the possibility of parallel interactions must also be taken into account , with consequences on the structure of toolkits . finally , frameworks must be provided for the combination of events and status of several devices . this paper reports on the extensions we made to the direct manipulation interface toolkit whizz in order to experiment two handed interaction . these extensions range from structural adaptations of the toolkit to new techniques for specifying the time dependent fusion of events .
reducing memory traffic with cregs . <eos> array and pointer references are often ambiguous in that compile time analysis can not always determine if distinct references are to the same object . ambiguously aliased objects are not allocated to registers by conventional compilers due to the cost of the loads and stores required to keep register copies consistent with memory and each other . there are several hardware and software strategies that can be used to solve the ambiguous alias problem we have implemented one such scheme called cregs in a compiler and instruction level simulator . we present a modification to briggs ' optimistic coloring algorithm that allows us to allocate local and parameter arrays to cregs . the cregs register file operation and instruction set modifications required to implement this scheme are discussed . underlying hardware issues such as pipeline impact and chip area are briefly discussed . several benchmarks are compared in terms of dynamic instructions executed for two creg set sizes . the measured reduction in memory operations is significant , averaging <digit> % for the benchmarks shown .
data relocation and prefetching for programs with large data sets . <eos> numerical applications frequently contain nested loop structures that process large arrays of data . the execution of these loop structures often produces memory reference patterns that poorly utilize data caches . limited associativity and cache capacity result in cache conflict misses . also , non unit stride access patterns can cause low utilization of cache lines . data copying has been proposed and investigated in order to reduce cache conflict misses , but this technique has a high execution overhead since it performs the copy operations entirely in software . we propose a combined hardware and software technique called data relocation and prefetching which eliminates much of the overhead of data copying through the use of special hardware . furthermore , by relocating the data while performing software prefetching , the overhead of copying the data can be reduced further . experimental results for data relocation and prefetching are encouraging and show a large improvement in cache performance .
a high performance microarchitecture with hardware programmable functional units . <eos> this paper explores a novel way to incorporate hardware programmable resources into a processor microarchitecture to improve the performance of general purpose applications . through a coupling of compile time analysis routines and hardware synthesis tools , we automatically configure a given set of the hardware programmable functional units ( pfus ) and thus augment the base instruction set architecture so that it better meets the instruction set needs of each application . we refer to this new class of general purpose computers as programmable instruction set computers ( prisc ) . although similar in concept , the prisc approach differs from dynamically programmable microcode because in prisc we define entirely new primitive datapath operations . in this paper , we concentrate on the microarchitectural design of the simplest form of <unk> risc microprocessor with a single pfu that only evaluates combinational functions . we briefly discuss the operating system and the programming language compilation techniques that are needed to successfully build prisc and , we present performance results from a proof of concept study . with the inclusion of a single <digit> bit wide pfu whose hardware cost is less than that of a <digit> kilobyte sram , our study shows a <digit> % improvement in processor performance on the specint92 benchmarks .
accessing hyperdocuments through interactive dynamic maps . <eos> we propose a new navigation paradigm based on a spatial metaphor to help users access and navigate within large sets of documents . this metaphor is implemented by a computer artifact called an interactive dynamic map ( idm ) . an idm plays a role similar to the role of a real map with respect to physical space . two types of idms are computed from the documents topic idms represent the semantic contents of a set of documents while document idms visualize a subset of documents such as those resulting from a query . idms can be used for navigating , browsing , and querying . they can be made active , they can be customized and they can be shared among users . the article presents the shadocs document retrieval system and describes the role , use and generation of idms in shadocs .
an interaction engine for rich hypertexts . <eos> in semantically rich hypertexts it is attractive to enable presentation of a network of nodes and link at different levels of abstraction . it is also important that the user can interact with the hypertext using a command repertoire that reflects the chosen abstraction level . based on a characterization of rich hypertext we introduce the concept of an interaction engine that governs the separation between internal hypertext representation and external screen presentation . this separation is the key principle of the hyperpro system . the hyperpro interaction engine is based on simple rules for presentation , interpretation of events , and menu set up . much of the power of the interaction engine framework comes from the organization of these rules relative to the type of hierarchy of nodes and links , and relative to a hierarchy of so called interaction schemes . the primary application domain discussed in the paper is program development and program documentation .
interpreted collaboration protocols and their use in groupware prototyping . <eos> the correct and timely creation of systems for coordination of group work depends on the ability to express , analyze , and experiment with protocols for managing multiple work threads . we present an evolution of the trellis model that provides a formal basis for prototyping the coordination structure of a collaboration system . in trellis , group interaction protocols are represented separately from the interface processes that use them for coordination . protocols are interpreted ( rather than compiled into applications ) so group interactions can be changed as a collaborative task progresses . changes can be made either by a person editing the protocol specification on the fly or by a silent observation process that participates in an application solely to perform behavioral adaptations . trellis uniquely mixes hypermedia browsing with collaboration support . we term this combination a hyperprogram , and we say that a hyperprogram integrates the description of a collaborative task with the information required for that task . as illustration , we describe a protocol for a moderated meeting and show a trellis prototype conference tool controlled by this protocol .
a flexible object merging framework . <eos> the need to merge different versions of an object to a common state arises in collaborative computing due to several reasons including optimistic concurrency control , asynchronous coupling , and absence of access control . we have developed a flexible object merging framework that allows definition of the merge policy based on the particular application and the context of the collaborative activity . it performs automatic , semi automatic , and interactive merges , supports semantics determined merges , operates on objects with arbitrary structure and semantics , and allows fine grained specification of merge policies . it is based on an existing collaborative applications framework and consists of a merge matrix , which defines merge functions and their parameters and allows definition of multiple merge policies , and a merge algorithm , which performs the merge based on the results computed by the merge functions . in conjunction with our framework we introduce a set of merge policies for several useful kinds of merges we have identified . this paper motivates the need for a general approach to merging , identifies some important merging issues , surveys previous research in merging , identifies a list of merge requirements , describes our merging framework and illustrates it with examples , and evaluates the framework with respect to the requirements and other research efforts in merging objects .
a forum for supporting interactive presentations to distributed audiences . <eos> computer technology is available to build video based tools for supporting presentations to distributed audiences , but it is unclear how such an environment affects participants ' ability to interact and to learn . we built and tested a tool called forum that broadcasts live audio , video and slides from a speaker , and enables audiences to interact with the speaker and other audience members in a variety of ways . the challenge was to enable effective interactions while overcoming obstacles introduced by the distributed nature of the environment , the large size of the group , and the asymmetric roles of the participants . forum was most successful in enabling effective presentations in cases when the topic sparked a great deal of audience participation or when the purpose of the talk was mostly informational and did not require a great deal of interaction . we are exploring ways to enhance forum to expand the effectiveness of this technology .
tight bounds on oblivious chaining . <eos> the chaining problem is defined as follows . given values the chaining problem appears as a subproblem in many contexts . there are known algorithms that solve the chaining problem on crcw prams in o ( alpha ( n ) ) time , where alpha ( n ) is the inverse of ackerman ' s function , and is a very slowly growing function . the author studies a class of algorithms ( called oblivious algorithms ) for this problem . a simple oblivious chaining algorithm running in o ( alpha ( n ) ) time is presented . more importantly , the optimality of the algorithm is demonstrated by showing a matching lower bound for oblivious algorithms using n processors . the first steps toward a lower bound for all chaining algorithms are also provided by showing that any chaining algorithm that runs in two steps must use a superlinear number of processors . the proofs use prefix graphs and weak superconcentrators . an interesting connection between the two is demonstrated and this idea is used to obtain improved bounds on the size of prefix graphs .
using specialized procedures and specification based analysis to reduce the runtime costs of modularity . <eos> managing tradeoffs between program structure and program efficiency is one of the most difficult problems facing software engineers . decomposing programs into abstractions simplifies the construction and maintenance of software and results in fewer errors . however , the introduction of these abstractions often introduces significant inefficiencies . this paper describes a strategy for eliminating many of these inefficiencies . it is based upon providing alternative implementations of the same abstraction , and using information contained in formal specifications to allow a compiler to choose the appropriate one . the strategy has been implemented in a prototype compiler that incorporates theorem proving technology .
trap driven simulation with tapeworm ii . <eos> tapeworm ii is a software based simulation tool that evaluates the cache and tlb performance of multiple task and operating system intensive workloads . tapeworm resides in an os kernel and causes a host machine ' s hardware to drive simulations with kernel traps instead of with address traces , as is conventionally done . this allows tapeworm to quickly and accurately capture complete memory referencing behavior with a limited degradation in overall system performance . this paper compares trap driven simulation , as implemented in tapeworm , with the more common technique of trace driven memory simulation with respect to speed , accuracy , portability and flexibility .
reducing branch costs via branch alignment . <eos> several researchers have proposed algorithms for basic block reordering . we call these branch alignment algorithms . the primary emphasis of these algorithms has been on improving instruction cache locality , and the few studies concerned with branch prediction reported small or minimal improvements . as wide issue architectures become increasingly popular the importance of reducing branch costs will increase , and branch alignment is one mechanism which can effectively reduce these costs . in this paper , we propose an improved branch alignment algorithm that takes into consideration the architectural cost model and the branch prediction architecture when performing the basic block reordering . we show that branch alignment algorithms can improve a broad range of static and dynamic branch prediction architectures . we also show that a program performance can be improved by approximately <digit> % even when using recently proposed , highly accurate branch prediction architectures . the programs are compiled by any existing compiler and then transformed via binary transformations . when implementing these algorithms on a alpha axp <digit> up to a <digit> % reduction in total execution time is achieved .
inverse kinematics positioning using nonlinear programming for highly articulated figures . <eos> an articulated figure is often modeled as a set of rigid segments connected with joints . its configuration can be altered by varying the joint angles . although it is straight forward to compute figure configurations given joint angles ( forward kinematics ) , it is more difficult to find the joint angles for a desired configuration ( inverse kinematics ) . since the inverse kinematics problem is of special importance to an animator wishing to set a figure to a posture satisfying a set of positioning constraints , researchers have proposed several different approaches . however , when we try to follow these approaches in an interactive animation system where the object on which to operate is as highly articulated as a realistic human figure , they fail in either generality or performance . so , we approach this problem through nonlinear programming techniques . it has been successfully used since <digit> in the spatial constraint system within jack , a human figure simulation system developed at the university of pennsylvania , and proves to be satisfactorily efficient , controllable , and robust . a spatial constraint in our system involves two parts one constraint on the figure , the end effector , and one on the spatial environment , the goal . these two parts are dealt with separately , so that we can achieve a neat modular implementation . constraints can be added one at a time with appropriate weights designating the importance of this constraint relative to the others and are always solved as a group . if physical limits prevent satisfaction of all the constraints , the system stops with the ( possibly local ) optimal solution for the given weights . also , the rigidity of each joint angle can be controlled , which is useful for redundant degrees of freedom .
solution differentiability for nonlinear parametric control problems . <eos> perturbed nonlinear control problems with data depending on a vector parameter are considered . using second order sufficient optimality conditions , it is shown that the optimal solution and the adjoint multipliers are differentiable functions of the parameter . the proof exploits the close connections between solutions of a riccati differential equation and shooting methods for solving the associated boundary value problem . solution differentiability provides a firm theoretical basis for numerical feedback schemes that have been developed for computing neighbouring extremals . the results are illustrated by an example that admits two extremal solutions . second order sufficient conditions single out one optimal solution for which a sensitivity analysis is carried out .
robust indirect adaptive control of time varying plants with unmodeled dynamics and disturbances . <eos> it is shown that indirect pole zero placement adaptive controllers are robust for systems with time varying parameters as well as unmodeled dynamics and disturbances . a parameter estimator with projection is used . no special signal normalization is employed to ensure robustness . the nominal system parameters need only be bounded , and their variations need only be small in an average sense . this allows them to vary slowly with time , as well as to take large jumps occasionally . the adaptive controllers can also simultaneously tolerate small unmodeled dynamics , as well as bounded disturbances , with no restriction on the magnitude of the bound .
randomized algorithms for multiprocessor page migration . <eos> the page migration problem is to manage a globally addressed shared memory in a multiprocessor system . each physical page of memory is located at a given processor , and memory references to that page by other processors incur a cost proportional to the network distance . at times the page may migrate between processors at cost proportional to the distance times d , a page size factor . the problem is to schedule movements on line so that the total cost of memory references is within a constant factor c of the best off line schedule . an algorithm that does so is called c competitive . black and sleator gave <digit> competitive deterministic on line algorithms for uniform networks ( complete graphs with unit edge lengths ) and for trees with arbitrary edge lengths . no good deterministic algorithm is known for general networks with arbitrary edge lengths . randomized algorithms are presented for the migration problem that are both simple and better than <digit> competitive against an oblivious adversary . an algorithm for uniform graphs is given . it is approximately <digit> . <digit> competitive as d grows large . a second , more powerful algorithm that works on graphs with arbitrary edge distances is also given . this algorithm is approximately <digit> . <digit> competitive ( or , <digit> plus the golden ratio ) for large d . both these algorithms use random bits only during an initialization phase , and from then on run deterministically . the competitiveness of a very simple coin flipping algorithm is also examined .
powerlist a structure for parallel recursion . <eos> many data parallel <unk> fourier transform , batcher ' s sorting schemes , and the prefix <unk> recursive structure . we propose a data structure called powerlist that permits succinct descriptions of such algorithms , highlighting the roles of both parallelism and recursion . simple algebraic properties of this data structure can be <unk> to derive properties of these algorithms and to establish equivalence of different algorithms that solve the same problem .
distributed network computing over local atm networks . <eos> communication between processors has long been the bottleneck of distributed network computing . however , recent progress in switch based high speed local area networks ( lans ) may be changing this situation . asynchronous transfer mode ( atm ) is one of the most widely accepted and emerging high speed network standards which can potentially satisfy the communication needs of distributed network computing . in this paper , we investigate distributed network computing over local atm networks . we first study the performance characteristics involving end to end communication in an environment that includes several types of workstations interconnected via a fore systems ' asx <digit> atm switch . we then compare the communication performance of four different application programming interfaces ( apis ) . the four apis were fore systems atm api , bsd socket programming interface , sun ' s remote procedure call ( rpc ) , and the parallel virtual machine ( pvm ) message passing library . each api represents distributed programming at a different communication protocol layer . we evaluate parallel matrix multiplication over the local atm network . the experimental results show that network computing is promising over local atm networks .
invariants of six points and projective reconstruction from three uncalibrated images . <eos> <unk> are three projective invariants of a set of six points in general position in space . it is well known that these invariants can not be recovered from one image , however an invariant relationship does exist between space invariants and image invariants . this invariant relationship is first derived for a single image . then this invariant relationship is used to derive the space invariants , when multiple images are available . this paper establishes that the minimum number of images for computing these invariants is three , and the computation of invariants of six points from three images can have as many as three solutions . algorithms are presented for computing these invariants in closed form . the accuracy and stability with respect to image noise , selection of the triplets of images and distance between viewing positions are studied both through real and simulated images . applications of these invariants are also presented . both the results of faugeras <digit> and hartley et al . <digit> for projective reconstruction and <unk> method <digit> for epipolar geometry determination from two uncalibrated images with at least seven points are extended to the case of three uncalibrated images with only six points .
efficient image processing algorithms on the scan line array processor . <eos> <unk> develop efficient algorithms for low and intermediate level image processing on the scan line array processor , a simd machine consisting of a linear array of cells that processes images in a scan line fashion . for low level processing , we present algorithms for block dft , block dct , convolution , template matching , shrinking , and expanding which run in real time . by real time , we mean that , if the required processing is based on neighborhoods of size mm , then the output lines are generated at a rate of o ( m ) operations per line and a latency of o ( m ) scan lines , which is the best that can be achieved on this model . we also develop an algorithm for median filtering which runs in almost real time at a cost of o ( m rm log m ) time per scan line and a latency of bf lfloor underline m , , <digit> rfloor scan lines . for intermediate level processing , we present optimal algorithms for translation , histogram computation , scaling , and rotation . we also develop efficient algorithms for labelling the connected components and determining the convex hulls of multiple figures which run in o ( n rm log n ) and o ( n rm log 2n ) time , respectively . the latter algorithms are significantly simpler and easier to implement than those already reported in the literature for linear arrays .
a bayesian segmentation methodology for parametric image models . <eos> <unk> based image segmentation methods require some criterion for determining when to merge regions . this paper presents a novel approach by introducing a bayesian probability of homogeneity in a general statistical context . our approach does not require parameter estimation and is therefore particularly beneficial for cases in which estimation based methods are most prone to error when little information is contained in some of the regions and , therefore , parameter estimates are unreliable . we apply this formulation to three distinct parametric model families that have been used in past segmentation schemes implicit polynomial surfaces , parametric polynomial surfaces , and gaussian markov random fields . we present results on a variety of real range and intensity images .
designing programs that check their work . <eos> a program correctness checker is an algorithm for checking the output of a computation . that is , given a program and an instance on which the program is run , the checker certifies whether the output of the program on that instance is correct . this paper defines the concept of a program checker . it designs program checkers for a few specific and carefully chosen problems in the class fp of functions computable in polynomial time . problems in fp for which checkers are presented in this paper include sorting , matrix rank and gcd . it also applies methods of modern cryptography , especially the idea of a probabilistic interactive proof , to the design of program checkers for group theoretic computations . two structural theorems are proven here . one is a characterization of problems that can be checked . the other theorem establishes equivalence classes of problems such that whenever one problem in a class is checkable , all problems in the class are checkable .
developing a reflective model of collaborative systems . <eos> recent years have seen a shift in perception of the nature of hci and interactive systems . as interface work has increasingly become a focus of attention for the social sciences , we have expanded our appreciation of the importance of issues such as work practice , adaptation , and evolution in interactive systems . the reorientation in our view of interactive systems has been accompanied by a call for a new model of design centered around user needs and participation . this article argues that a new process of design is not enough and that the new view necessitates a similar reorientation in the structure of the systems we build . it outlines some requirements for systems that support a deeper conception of interaction and argues that the traditional system design techniques are not suited to creating such systems . finally , using examples from ongoing work in the design of an open toolkit for collaborative applications , it illustrates how the principles of computational reflection and metaobject protocols can lead us toward a new model based on open abstraction that holds great promise in addressing these issues .
a fortran <digit> environment for research and prototyping of enclosure algorithms for nonlinear equations and global optimization . <eos> an environment for general research into and prototyping of algorithms for reliable constrained and unconstrained global nonlinear optimization and reliable enclosure of all roots of nonlinear systems of equations , with or without inequality constraints , is being developed . this environment should be portable , easy to learn , use , and maintain , and sufficiently fast for some production work . the motivation , design principles , uses , and capabilities for this environment are outlined . the environment includes an interval data type , a symbolic form of automatic differentiation to obtain an internal representation for functions , a special technique to allow conditional branches with operator overloading and interval computations , and generic routines to give interval and noninterval function and derivative information . some of these generic routines use a special version of the backward mode of automatic differentiation . the package also includes dynamic data structures for exhaustive search algorithms .
closure analysis in constraint form . <eos> flow analyses of untyped higher order functional programs have in the past decade been presented by ayers , bondorf , consel , jones , heintze , sestoft , shivers , <unk> , wand , and others . the analyses are usually defined as abstract interpretations and are used for rather different tasks such as type recovery , globalization , and binding time analysis . the analyses all contain a global closure analysis that computes information about higher order control flow . sestoft proved in <digit> and <digit> that closure analysis is correct with respect to call by name and call by value semantics , but it remained open if correctness holds for arbitrary beta reduction . this article answers the question both closure analysis and others are correct with respect to arbitrary beta reduction . we also prove a subject reduction result closure information is still valid after beta reduction . the core of our proof technique is to define closure analysis using a constraint system . the constraint system is equivalent to the closure analysis of bondorf , which in turn is based on sestoft ' s .
effective cache prefetching on bus based multiprocessors . <eos> compiler directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high performance processors . however , prefetching is not without costs , particularly on a shared memory multiprocessor . prefetching can negatively affect bus utilization , overall cache miss rates , memory latencies and data sharing . we simulate the effects of a compiler directed prefetching algorithm , running on a range of bus based multiprocessors . we show that , despite a high memory latency , this architecture does not necessarily support prefetching well , in some cases actually causing performance degradations . we pinpoint several problems with prefetching on a shared memory architecture ( additional conflict misses , no reduction in the data sharing traffic and associated latencies , a multiprocessor ' s greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance ) and measure their effect on performance . we then solve those problems through architectural techniques and heuristics for prefetching that could be easily incorporated into a compiler ( <digit> ) victim caching , which eliminates most of the cache conflict misses caused by prefetching in a direct mapped cache , ( <digit> ) special prefetch algorithms for shared data , which significantly improve the ability of our basic prefetching algorithm to prefetch individual misses , and ( <digit> ) compiler based shared data restructuring , which eliminates many of the invalidation misses the basic prefetching algorithm does not predict . the combined effect of these improvements is to make prefetching effective over a much wider range of memory architectures .
combining analyses , combining optimizations . <eos> modern optimizing compilers use several passes over a program ' s intermediate representation to generate good code . many of these optimizations exhibit a phase ordering problem . getting the best code may require iterating optimizations until a fixed point is reached . combining these phases can lead to the discovery of more facts about the program , exposing more opportunities for optimization . this article presents a framework for describing optimizations . it shows how to combine two such frameworks and how to reason about the properties of the resulting framework . the structure of the frame work provides insight into when a combination yields better results . to make the ideas more concrete , this article presents a framework for combining constant propagation , value numbering , and unreachable code elimination . it is an open question as to what other frameworks can be combined in this way .
conjoining specifications . <eos> we show how to specify components of concurrent systems . the specification of a system is the conjunction of its components ' specifications . properties of the system are proved by reasoning about its components . we consider both the decomposition of a given system into parts , and the composition of given parts to form a system .
a complete calculus for the multialgebraic and functional semantics of nondeterminism . <eos> the current algebraic models for nondeterminism focus on the notion of possibility rather than necessity and consequently equate ( nondeterministic ) terms that one would intuitively not consider equal . furthermore , existing models for nondeterminism depart radically from the standard models for ( equational ) specifications of deterministic operators . one would prefer that a specification language for nondeterministic operators be based on an extension of the standard model concepts , preferably in such a way that the reasoning system for ( possibly nondeterministic ) operators becomes the standard equational one whenever restricted to the deterministic <unk> objective should be to minimize the departure from the standard frameworks . in this article we define a specification language for nondeterministic operators and multialgebraic semantics . the first complete reasoning system for such specifications is introduced . we also define a transformation of specifications of nondeterministic operators into derived specifications of deterministic ones , obtaining a computational semantics of nondeterministic specification by adopting the standard semantics of the derived specification as the semantics of the original one . this semantics turns out to be a refinement of multialgebra semantics . the calculus is shown to be sound and complete also with respect to the new semantics .
matrix powers in finite precision arithmetic . <eos> if a is a square matrix with spectral radius less than <digit> then a k to <digit> as k to infty , but the powers computed in finite precision arithmetic may or may not converge . we derive a sufficient condition for fl ( a k ) to <digit> as k to infty and a bound on norm fl ( a k ) , both expressed in terms of the jordan canonical form of a . examples show that the results can be sharp . we show that the sufficient condition can be rephrased in terms of a pseudospectrum of a when a is diagonalizable , under certain assumptions . our analysis leads to the rule of thumb that convergence or divergence of the computed powers of a can be expected according as the spectral radius computed by any backward stable algorithm is less than or greater than <digit> .
an experimental comparison of the nearest neighbor and nearest hyperrectangle algorithms . <eos> algorithms based on nested generalized exemplar ( nge ) theory ( salzberg , <digit> ) classify new data points by computing their distance to the nearest generalized exemplar ( i . e . , either a point or an axis parallel rectangle ) . they combine the distance based character of nearest neighbor ( nn ) classifiers with the axis parallel rectangle representation employed in many rule learning systems . an implementation of nge was compared to the k nearest neighbor ( knn ) algorithm in <digit> domains and found to be significantly inferior to knn in <digit> of them . several modifications of nge were studied to understand the cause of its poor performance . these show that its performance can be substantially improved by preventing nge from creating overlapping rectangles , while still allowing complete nesting of rectangles . performance can be further improved by modifying the distance metric to allow weights on each of the features ( salzberg , <digit> ) . best results were obtained in this study when the weights were computed using mutual information between the features and the output class . the best version of nge developed is a batch algorithm ( bnge <unk> ) that has no user tunable parameters . bnge <unk> ' s performance is comparable to the first nearest neighbor algorithm ( also incorporating feature weights ) . however , the k nearest neighbor algorithm is still significantly superior to bnge <unk> in <digit> of the <digit> domains , and inferior to it in only <digit> . we conclude that , even with our improvements , the nge approach is very sensitive to the shape of the decision boundaries in classification problems . in domains where the decision boundaries are axis parallel , the nge approach can produce excellent generalization with interpretable hypotheses . in all domains tested , nge algorithms require much less memory to store generalized exemplars than is required by nn algorithms .
approximate analysis of reader writer queues . <eos> we analyze the performance of queues that serve readers and writers . readers are served concurrently , while writers require exclusive service . we approximately analyze a first come first serve ( fcfs ) reader writer queue , and derive simple formulae for computing waiting times and capacity under the assumption of poisson arrivals and exponential service . we extend the analysis to handle a one writer queue , and a queue that includes write intention locks . the simple analyses that we present can be used as rules of thumb for designing concurrent systems .
evaluation of binarization methods for document images . <eos> <unk> paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast , variable background intensity and noise . <unk> method with the addition of the postprocessing step of yanowitz and <unk> method added performed the best and was also one of the fastest binarization methods .
when is the assignment bound tight for the asymmetric traveling salesman problem . <eos> we consider the probabilistic relationship between the value of a random asymmetric traveling salesman problem atsp ( m ) and the value of its assignment relaxation ap ( m ) . we assume here that the costs are given by an n times n matrix m whose entries are independently and identically distributed . we focus on the relationship between pr ( atsp ( m ) ap ( m ) ) and the probability p n that any particular entry is zero . if np n rightarrow infty with n then we prove that atsp ( m ) ap ( m ) with probability <digit> o ( <digit> ) . this is shown to be best possible in the sense that if np ( n ) rightarrow c , c > <digit> and constant , then pr ( atsp ( m ) ap ( m ) ) < <digit> phi ( c ) for some positive function phi . finally , if np n rightarrow <digit> then pr ( atsp ( m ) ap ( m ) ) rightarrow <digit> .
search problems in the decision tree model . <eos> the relative power of determinism , randomness , and nondeterminism for search problems in the boolean decision tree model is studied . it is shown that the gaps between the nondeterministic , the randomized , and the deterministic complexities can be arbitrarily large for search problems . an interesting connection of this model to the complexity of resolution proofs is also mentioned .
a domain specific software architecture for adaptive intelligent systems . <eos> a good software architecture facilitates application system development , promotes achievement of functional requirements , and supports system reconfiguration . we present a domain specific software architecture ( dssa ) that we have developed for a large application domain of adaptive intelligent systems ( ais ' s ) . the dssa provides a ) an ais reference architecture designed to meet the functional requirements shared by applications in this domain , b ) principles for decomposing expertise into highly reusable components , and c ) an application configuration method for selecting relevant components from a library and automatically configuring instances of those components in an instance of the architecture . the ais reference architecture incorporates features of layered , pipe and filter , and blackboard style architectures . we describe three studies demonstrating the utility of our architecture in the subdomain of mobile office robots and identify software engineering principles embodied in the architecture .
measured performance of data transmission over cellular telephone networks . <eos> recent developments in mobile communication and personal computer technology have laid a new foundation for mobile computing . performance of the data communication system as seen by an application program is a fundamental factor when communication infrastructure at the application layer is designed . this paper provides results of performance measurements of data transmission over two different cellular telephone networks , a digital gsm network and an analogue nmt network . since our emphasis is on performance as seen by application programs , we use the standard tcp ip protocols in the measurements . the performance is measured using three basic operations establishment of a wireless dial up connection , exchange of request reply messages , and bulk data transfer . the external conditions under which the measurements were carried out present a normal office environment when the field strength of the cellular link is good or fairly good .
model uncertainty in discrete event systems . <eos> earlier work concerning control of discrete event systems usually assumed that a correct model of the system to be controlled was available . a goal of this work is to provide an algorithm for determining the correct model from a set of models . the result of the algorithm is a finite language that can be used to test for the correct model or notification that the remaining models can not be controllably distinguished . we use the finite state machine model with controllable and uncontrollable events presented by ramadge and wonham .
analysis of costate discretizations in parameter estimation for linear evolution equations . <eos> a widely used approach to parameter identification is the output least squares formulation . numerical methods for solving the resulting minimization problem almost invariably require the computation of the gradient of the output least squares functional . when the identification problem involves time dependent distributed parameter systems ( or approximations thereof ) , numerical evaluation of the gradient can be extremely time consuming . the costate method can greatly reduce the cost of computing these gradients . however , questions have been raised concerning the accuracy and convergence of costate approximations , even when the numerical methods being used are known to converge rapidly on the forward problem . in this paper it is shown that the use of time marching schemes that yield high order accuracy on the forward problem does not necessarily lead to high order accurate costate approximations . in fact , in some cases these approximations do not converge at all . however , under certain circumstances , rapidly converging gradient approximations do result because of rapid weak star type convergence of the costate approximations . these issues are treated both theoretically and numerically .
a globally convergent successive approximation method for severely nonsmooth equations . <eos> this paper presents a globally convergent successive approximation method for solving f ( x ) <digit> where f is a continuous function . at each step of the method , f is approximated by a smooth function f k , with pa f k f pa rightarrow <digit> as k rightarrow infty . the direction f ' k ( x k ) <digit> f ( x k ) is then used in a line search on a sum of squares objective . the approximate function f k can be constructed for nonsmooth equations arising from variational inequalities , maximal monotone operator problems , nonlinear complementarity problems , and nonsmooth partial differential equations . numerical examples are given to illustrate the method .
tighter lower bounds on the exact complexity of string matching . <eos> this paper considers the exact number of character comparisons needed to find all occurrences of a pattern of length m in a text of length n using on line and general algorithms . for on line algorithms , a lower bound of about ( <digit> frac <digit> <digit> ( m <digit> ) ) cdot n character comparisons is obtained . for general algorithms , a lower bound of about ( <digit> frac <digit> m <digit> ) cdot n character comparisons is obtained . these lower bounds complement an on line upper bound of about ( <digit> frac <digit> <digit> ( m <digit> ) ) cdot n comparisons obtained recently by cole and hariharan . the lower bounds are obtained by finding patterns with interesting combinatorial properties . it is also shown that for some patterns off line algorithms can be more efficient than on line algorithms .
planar strong connectivity helps in parallel depth first search . <eos> this paper proves that for a strongly connected planar directed graph of size n , a depth first search tree rooted at a specified vertex can be computed in o ( log <digit> n ) time with n log n processors . previously , for planar directed graphs that may not be strongly connected , the best depth first search algorithm runs in o ( log <digit> n ) time with n processors . both algorithms run on a parallel random access machine that allows concurrent reads and concurrent writes in its shared memory , and in case of a write conflict , permits an arbitrary processor to succeed .
optimal file sharing in distributed networks . <eos> the following file distribution problem is considered given a network of processors represented by an undirected graph g ( v , e ) and a file size k , an arbitrary file w of k bits is to be distributed among all nodes of g . to this end , each node is assigned a memory device such that by accessing the memory of its own and of its adjacent nodes , the node can reconstruct the contents of w . the objective is to minimize the total size of memory in the network . this paper presents a file distribution scheme which realizes this objective for k gg log delta g , where delta g stands for the maximum degree in g for this range of k , the total memory size required by the suggested scheme approaches an integer programming lower bound on that size . the scheme is also constructive in the sense that given g and k , the memory size at each node in g , as well as the mapping of any file w into the node memory devices , can be computed in time complexity which is polynomial in k and v . furthermore , each node can reconstruct the contents of such a file w in o ( k <digit> ) bit operations . finally , it is shown that the requirement of k being much larger than log delta g is necessary in order to have total memory size close to the integer programming lower bound .
the complexity and distribution of hard problems . <eos> measure theoretic aspects of the leq rm p rm m reducibility structure of the exponential time complexity classes e dtime ( <digit> rm linear ) and e <digit> rm dtime ( <digit> rm polynomial ) are investigated . particular attention is given to the complexity ( measured by the size of complexity cores ) and distribution ( abundance in the sense of measure ) of languages that are leq rm p rm m hard for e and other complexity classes . tight upper and lower bounds on the size of complexity cores of hard languages are derived . the upper bound says that the leq rm p rm m hard languages for e are unusually simple , in the sense that they have smaller complexity cores than most languages in e . it follows that the leq rm p rm m complete languages for e form a measure <digit> subset of e ( and similarly in e <digit> ) . this latter fact is seen to be a special case of a more general theorem , namely , that it every pmr degree ( e . g . , the degree of all pmr complete languages for np ) has measure <digit> in e and in ep .
private computations over the integers . <eos> the subject of this work is the possibility of private distributed computations of n argument functions defined over the integers . a function f is t private if there exists a protocol for computing f , so that no coalition of at most t participants can infer any additional information from the execution of the protocol . it is known that over finite domains , every function can be computed left lfloor ( n <digit> ) <digit> right rfloor privately . some functions , like addition , are even n private . we prove that this result can not be extended to infinite domains . the possibility of privately computing f is shown to be closely related to the communication complexity of f . using this relation , we show , for example , that n argument addition is left lfloor ( n <digit> ) <digit> right rfloor private over the nonnegative integers , but not even <digit> private over all the integers . finally , a complete characterization of t private boolean functions over countable domains is given . a boolean function is <digit> private if and only if its communication complexity is bounded . this characterization enables us to prove that every boolean function falls into one of the following three categories it is either n private , left lfloor ( n <digit> ) <digit> right rfloor private but not left lceil n <digit> right rceil private , or not <digit> private .
a new approach to formal language theory by kolmogorov complexity . <eos> we present a new approach to formal language theory using kolmogorov complexity . the main results presented here are an alternative for pumping lemma ( s ) , a new characterization for regular languages , and a new method to separate deterministic context free languages and nondeterministic context free languages . the use of the new incompressibility arguments is illustrated by many examples . the approach is also successful at the high end of the chomsky hierarchy since one can quantify <unk> in terms of kolmogorov complexity .
eigenvalues and expansion of regular graphs . <eos> the spectral method is the best currently known technique to prove lower bounds on expansion . ramanujan graphs , which have asymptotically optimal second eigenvalue , are the best known explicit expanders . the spectral method yielded a lower bound of k <digit> on the expansion of linear sized subsets of k regular ramanujan graphs . we improve the lower bound on the expansion of ramanujan graphs to approximately k <digit> . moreover , we construct a family of k regular graphs with asymptotically optimal second eigenvalue and linear expansion equal to k <digit> . this shows that k <digit> is the best bound one can obtain using the second eigenvalue method . we also show an upper bound of roughly <digit> k <digit> on the average degree of linear sized induced subgraphs of ramanujan graphs . this compares positively with the classical bound 2k <digit> . as a byproduct , we obtain improved results on random walks on expanders and construct selection networks ( respectively , extrovert graphs ) of smaller size ( respectively , degree ) than was previously known .
on the minimality and global consistency of row convex constraint networks . <eos> constraint networks have been shown to be useful in formulating such diverse problems as scene labeling , natural language parsing , and temporal reasoning . given a constraint network , we often wish to ( i ) find a solution that satisfies the constraints and ( ii ) find the corresponding minimal network where the constraints are as explicit as possible . both tasks are known to be np complete in the general case . task ( <digit> ) is usually solved using a backtracking algorithm , and task ( ii ) is often solved only approximately by enforcing various levels of local consistency . in this paper , we identify a property of binary constraint called row convexity and show its usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee that a network is both minimal and globally consistent . globally consistent networks have the property that a solution can be found without backtracking . we show that one can test for the row convexity property efficiently and we show , by examining applications of constraint networks discussed in the literature , that our results are useful in practice . thus , we identify a class of binary constraint networks for which we can solve both tasks ( i ) and ( ii ) efficiently . finally , we generalize the results for binary constraint networks to networks with nonbinary constraints .
an optimal service policy for buffer systems . <eos> consider a switching component in a packet switching network , where messages from several incoming channels arrive and are routed to appropriate outgoing ports according to a service policy . one requirement in the design of such a system is to determine the buffer storage necessary at the input of each channel and the policy for serving these buffers that will prevent buffer overflow and the corresponding loss of messages . in this paper , a class of buffer service policies , called least time to reach bound ( <unk> ) , is introduced that guarantees no overflow , and for which the buffer size required at each input channel is independent of the number of channels and their relative speeds . further , the storage requirement is only twice the maximal length of a message in all cases , and as a consequence the class is shown to be optimal in the sense that any <unk> policy requires at least as much storage as <unk> .
approximating the minimum equivalent digraph . <eos> the meg ( minimum equivalent graph ) problem is the following given a directed graph , find a smallest subset of the edges that maintains all reachability relations between nodes . this problem is np hard this paper gives an approximation algorithm achieving a performance guarantee of about <digit> . <digit> in polynomial time . the algorithm achieves a performance guarantee of <digit> . <digit> in the time required for transitive closure . the heart of the meg problem is the minimum scss ( strongly connected spanning subgraph ) problem the meg problem restricted to strongly connected digraphs . for the minimum scss problem , the paper gives a practical , nearly linear time implementation achieving a performance guarantee of <digit> . <digit> . the algorithm and its analysis are based on the simple idea of contracting long cycles . the analysis applies directly to <digit> exchange , a general local improvement algorithm , showing that its performance guarantee is <digit> . <digit> .
stability of linear equations solvers in interior point methods . <eos> primal dual interior point methods for linear complementarity and linear programming problems solve a linear system of equations to obtain a modified newton step at each iteration . these linear systems become increasingly ill conditioned in the later stages of the algorithm , but the computed steps are often sufficiently accurate to be useful . we use error analysis techniques tailored to the special structure of these linear systems to explain this observation and examine how theoretically superlinear convergence of a path following algorithm is affected by the roundoff errors .
polynomial time membership comparable sets . <eos> this paper studies a notion called polynomial time membership comparable sets . for a function g , a set a is polynomial time g membership comparable if there is a polynomial time computable function f such that for any x <digit> , cdots , x m with m geq g ( max x <digit> , cdots , x m ) , outputs b in <digit> , <digit> m such that ( a ( x <digit> ) , cdots , a ( x m ) ) neq b . the following is a list of major results proven in the paper . <digit> . polynomial time membership comparable sets construct a proper hierarchy according to the bound on the number of arguments . <digit> . polynomial time membership comparable sets have polynomial size circuits . <digit> . for any function f and for any constant c > <digit> , if a set is leq p f ( n ) tt reducible to a p selective set , then the set is polynomial time ( <digit> c ) log f ( n ) membership comparable . <digit> . for any cal c chosen from rm pspace , up , fewp , np , c p , pp , mod <digit> p , mod <digit> , cdots , if cal c subseteq rm p mc ( c log n ) for some c < <digit> , then cal as a corollary of the last two results , it is shown that if there is some constant c < <digit> such that all of cal c are polynomial time n c truth table reducible to some p selective sets , then cal which resolves a question that has been left open for a long time .
a case study in simulating pcs networks using time warp . <eos> there has been rapid growth in the demand for mobile communications over the past few years . this has led to intensive research and development efforts for complex pcs ( personal communication service ) networks . capacity planning and performance modeling is necessary to maintain a high quality of service to the mobile subscriber while minimizing cost to the pcs provider . the need for flexible analysis tools and the high computational requirements of large pcs network simulations make it an excellent candidate for parallel simulation . here , we describe our experiences in developing two pcs simulation models on a general purpose distributed simulation platform based on the time warp mechanism . these models utilize two widely used approaches to simulating pcs networks ( i ) the call initiated and ( ii ) the portable initiated models . we discuss design decisions that were made in mapping these models to the time warp executive , and characterize the workloads resulting from these models in terms of factors such as communication locality and computation granularity . we then present performance measurements for their execution on a network of workstations . these measurements indicate that the call initiated model generally outperforms the portable initiated model , but is not able to capture phenomenon such as the busy line effect . moreover , these studies indicate that the high locality in large scale pcs network simulations make them well suited for execution on general purpose parallel and distributed simulation platforms .
probabilistic adaptive direct optimism control in time warp . <eos> in a distributed memory environment the communication overhead of time warp as induced by the rollback procedure due to overoptimistic progression of the simulation is the dominating performance factor . to limit optimism to an extent that can be justified from the inherent model parallelism , an optimism control mechanism is proposed , which by maintaining a history record of virtual time differences from the time stamps carried by arriving messages , and forecasting the timestamps of forthcoming messages , probabilistically delays the execution of scheduled events to avoid potential rollback and associated communication overhead ( antimessages ) . after investigating statistical forecast methods which express only the central tendency of the arrival process , we demonstrate that arrival processes in the context of time warp simulations of timed petri nets have certain predictable and consistent arima characteristics , which encourage the use of sophisticated and recursive forecast procedures based on those models . adaptiveness is achieved in two respects the synchronization behavior of logical processes automatically progressing and conservatively blocking , that is the most adequate for ( i ) the specific simulation model and ( ii ) the communication computation speed characteristics of the underlying execution platform .
super criticality revisited . <eos> critical path analysis has been suggested as a technique for establishing a lower bound on the completion times of parallel discrete event simulations . a protocol is super critical if there is at least one simulation that can complete in less than the critical path time using that protocol . previous studies have shown that several practical protocols are super critical while others are not . we present a sufficient condition to demonstrate that a protocol is super criticality . it has been claimed that super criticality requires independence of one or more messages ( states ) on events in the logical past of those messages ( states ) . we present an example which contradicts this claim and examine the implications of this contradiction on lower bounds .
buffer management in shared memory time warp systems . <eos> mechanisms for managing message buffers in time warp parallel simulations executing on cache coherent shared memory multiprocessors are studied . two simple buffer management strategies called the sender pool and receiver pool mechanisms are examined with respect to their efficiency , and in particular , their interaction with multiprocessor cache coherence protocols . measurements of implementations on a kendall square research ksr <digit> machine using both synthetic workloads and benchmark applications demonstrate that sender pools offer significant performance advantages over receiver pools . however , it is also observed that both schemes , especially the sender pool mechanism , are prone to severe performance degradations due to poor locality of reference in large simulations using substantial amounts of message buffer memory . a third strategy called the partitioned buffer pool approach is proposed that exploits the advantages of sender pools , but exhibits much better locality . measurements of this approach indicate that the partitioned pool mechanism yields substantially better performance than both the sender and receiver pool schemes for large scale , small granularity parallel simulation applications . the central conclusions from this study are ( <digit> ) buffer management strategies play an important role in determining the overall efficiency of multiprocessor based parallel simulators , and ( <digit> ) the partitioned buffer pool organization offers significantly better performance than the sender and receiver pool schemes . these studies demonstrate that poor performance may result if proper attention is not paid to realizing an efficient buffer management mechanism .
language constructs and transformation for hard real time systems . <eos> in practice , time critical portions of hard real time systems are still implemented in low level programming languages and manually tuned to meet all the timing requirements . without a real time language that supports an appropriate way of specifying timing constraints for a generic hard real time systems , and high precision timing analysis that is transparent to users , the users will ever suffer from the complex coding and analysis , particularly for systems requiring fast turnaround responses . in this paper , we propose novel language constructs that can be added to any imperative programming language so that the extended language provides users a way to specify relative timing constraints between arbitrary operations at instruction level . the compilation techniques unique to transformation of the proposed language are also presented as a part of charts , the compiler for hard real time systems , which generates a valid instruction sequence for a target execution model .
visualizing real time multivariate data using preattentive processing . <eos> a new method is presented for visualizing data as they are generated from real time applications . these techniques allow viewers to perform simple data analysis tasks such as detection of data groups and boundaries , target detection , and estimation . the goal is to do this rapidly and accurately on a dynamic sequence of data frames . our techniques take advantage of an ability of the human visual system called preattentive processing . preattentive processing refers to an initial organization of the visual system based on operations believed to be rapid , automatic , and spatially parallel . examples of visual features that can be detected in this way include hue , orientation , intensity , size , curvature , and line length . we believe that studies from preattentive processing should be used to assist in the design of visualization tools , especially those for which high speed target , boundary , and region detection are important . previous work has shown that results from research in preattentive processing can be used to build visualization tools that allow rapid and accurate analysis of individual , static data frames . we extend these techniques to a dynamic real time environment . this allows users to perform similar tasks on dynamic sequences of frames , exactly like those generated by real time systems such as visual interactive simulation . we studied two known preattentive features , hue and curvature . the primary question investigated was whether rapid and accurate target and boundary detection in dynamic sequences is possible using these features . behavioral experiments were run that simulated displays from our preattentive visualization tools . analysis of the results of the experiments showed that rapid and accurate target and boundary detection is possible with both hue and curvature . a second question , whether interactions occur between the two features in a real time environment , was answered positively .
production and playback of human figure motion for visual simulation . <eos> we describe a system for off line production and real time playback of motion for articulated human figures in 3d virtual environments . the key notion are ( <digit> ) the logical storage of full body motion in posture graphs , which provides a simple motion access method for playback , and ( <digit> ) mapping the motions of high dof figures to lower dof figures using slaving to provide human models at several levels of detail , both in geometry and articulation , for later playback . we present our system in the context of a simple problem animating human figures in a distributed simulation , using dis protocols for communicating the human state information . we also discuss several related techniques for real time animation of articulated figures in visual simulation .
active camera calibration for a head eye platform using the variable state dimension filter . <eos> <unk> correspondence presents a new technique for calibrating a camera mounted on a controllable head eye platform . it uses the trajectories of an arbitrary number of tracked corner features to improve the calibration parameter estimates over time , utilizing a novel variable state dimension form of recursive filter . no special visual stimuli are required and no assumptions are made about the structure of the scene , other than that it is stationary relative to the head . the algorithm runs at <digit> frames per second on a single inmos t805 transputer , and is fully integrated into a real time active vision system . updated calibration parameters are regularly passed to the vision modules that require them . although the algorithm requires an initial estimate of camera focal length , results are presented from real experiments demonstrating that convergence is achieved for initial errors up to <digit> % .
scale space properties of the multiscale morphological dilation erosion . <eos> abstracta multiscale morphological dilation erosion smoothing operation and its associated scale space expansion for multidimensional signals are proposed . properties of this smoothing operation are developed and , in particular a scale space monotonic property for signal extrema is demonstrated . scale space fingerprints from this approach have advantages over gaussian scale space fingerprints in that they are defined for negative values of the scale parameter have monotonic properties in two and higher dimensions , do not cause features to be shifted by the smoothing , and allow efficient computation . the application of reduced multiscale dilation erosion fingerprints to the surface matching of terrain is demonstrated .
genetic algorithms , operators , and dna fragment assembly . <eos> we study different genetic algorithm operators for one permutation problem associated with the human genome <unk> assembly of dna sequence fragments from a parent clone whose sequence is unknown into a consensus sequence corresponding to the parent sequence . the sorted order representation , which does not require specialized operators , is compared with a more traditional permutation representation , which does require specialized operators . the two representations and their associated operators are compared on problems ranging from 2k to 34k base pairs ( kb ) . edge recombination crossover used in conjunction with several specialized operators is found to perform best in these experiments semi these operators solved a 10kb sequence , consisting of <digit> fragments , with no manual intervention . natural building blocks in the problem are exploited at progressively higher levels through macro operators . this significantly improves performance .
unsupervised learning of multiple motifs in biopolymers using expectation maximization . <eos> the meme algorithm extends the expectation maximization ( em ) algorithm for identifying motifs in unaligned biopolymer sequences . the aim of meme is to discover new motifs in a set of biopolymer sequences where little or nothing is known in advance about any motifs that may be present . meme innovations expand the range of problems which can be solved using em and increase the chance of finding good solutions . first , subsequences which actually occur in the biopolymer sequences are used as starting points for the em algorithm to increase the probability of finding globally optimal motifs . second , the assumption that each sequence contains exactly one occurrence of the shared motif is removed . this allows multiple appearances of a motif to occur in any sequence and permits the algorithm to ignore sequences with no appearance of the shared motif , increasing its resistance to noisy data . third , a method for probabilistically erasing shared motifs after they are found is incorporated so that several distinct motifs can be found in the same set of sequences , both when different motifs appear in different sequences and when a single sequence may contain multiple motifs . experiments show that meme can discover both the crp and lexa binding sites from a set of sequences which contain one or both sites , and that meme can discover both the <digit> and <digit> promoter regions in a set of e . coli sequences .
evaluation and selection of biases in machine learning . <eos> in this introduction , we define the term bias as it is used in machine learning systems . we motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta bias spaces . recent research in the field of machine learning bias is summarized .
processor mapping techniques toward efficient data redistribution . <eos> <unk> time data redistribution can enhance algorithm performance in distributed memory machines . explicit redistribution of data can be performed between algorithm phases when a different data decomposition is expected to deliver increased performance for a subsequent phase of computation . redistribution , however , represents increased program overhead as algorithm computation is discontinued while data are exchanged among processor memories . in this paper , we present a technique that minimizes the amount of data exchange for block to cyclic ( c ) ( or vice versa ) redistributions of arbitrary number of dimensions . preserving the semantics of the target ( destination ) distribution pattern , the technique manipulates the data to logical processor mapping of the target pattern . when implemented on an ibm sp , the mapping technique demonstrates redistribution performance improvements of approximately <digit> % over traditional data to processor mapping . relative to the traditional mapping technique , the proposed method affords greater flexibility in specifying precisely which data elements are redistributed and which elements remain on processor .
a comparison of id3 and backpropagation for english text to speech mapping . <eos> the performance of the error backpropagation ( bp ) and id3 learning algorithms was compared on the task of mapping english text to phonemes and stresses . under the distributed output code developed by sejnowski and rosenberg , it is shown that bp consistently out performs id3 on this task by several percentage points . three hypotheses explaining this difference were explored ( a ) id3 is overfitting the training data , ( b ) bp is able to share hidden units across several output units and hence can learn the output units better , and ( c ) bp captures statistical information that id3 does not . we conclude that only hypothesis ( c ) is correct . by augmenting id3 with a simple statistical learning procedure , the performance of bp can be closely matched . more complex statistical procedures can improve the performance of both bp and id3 substantially in this domain .
weakly hard problems . <eos> a weak completeness phenomenon is investigated in the complexity class rm e rm dtime ( <digit> rm linear ) . according to standard terminology , a language h is leq rm p m hard for e if the set rm p m ( h ) , consisting of all languages a leq rm p m h , contains the entire class e . a language c is leq rm p m complete for e if it is leq rm p m hard for e and is also an element of e . generalizing this , a language h is weakly leq rm p m hard for e if the set rm p m ( h ) does not have measure <digit> in e . a language c is weakly leq rm p m complete for e if it is weakly leq rm p m hard for e and is also an element of e . the main result of this paper is the construction of a language that is weakly leq rm p m complete , but not leq rm p m complete , for e . the existence of such languages implies that previously known strong lower bounds on the complexity of weakly leq rm p m hard problems for e ( given by work of lutz , mayordomo , and juedes ) are indeed more general than the corresponding bounds for leq rm p m hard problems for e . the proof of this result introduces a new diagonalization method , called martingale diagonalization . using this method , one simultaneously develops an infinite family of polynomial time computable martingales ( betting strategies ) and a corresponding family of languages that defeat these martingales ( prevent them from winning too much money ) while also pursuing another agenda . martingale diagonalization may be useful for a variety of applications .
adaptive pattern matching . <eos> pattern matching is an important operation used in many applications such as functional programming , rewriting , and rule based expert systems . by preprocessing the patterns into a dfa like automaton , we can rapidly select the matching pattern ( s ) in a single scan of the relevant portions of the input term . this automaton is typically based on left to right traversal of the patterns . by adapting the traversal order to suit the set of input patterns , it is possible to considerably reduce the space and matching time requirements of the automaton . the design of such adaptive automata is the focus of this paper . we first formalize the notion of an adaptive traversal . we then present several strategies for synthesizing adaptive traversal orders aimed at reducing space and matching time complexity . in the worst case , however , the space requirements can be exponential in the size of the patterns . we show this by establishing an exponential lower bounds on space that is independent of the traversal order used . we then discuss an orthogonal approach to space minimization based on direct construction of optimal dag automata . finally , our work brings forth the impact of typing in pattern matching . in particular , we show that several important problems ( e . g . , lazy pattern matching in ml ) are computationally hard in the presence of type disciplines , whereas they can be solved efficiently in the untyped setting .
finding regular simple paths in graph databases . <eos> we consider the following problem given a labelled directed graph g and a regular expression r , find all pairs of nodes connected by a simple path such that the concatenation of the labels along the path satisfies r . the problem is motivated by the observation that many recursive queries in relational databases can be expressed in this form , and by the implementation of a query language , bf g , based on this observation . we show that the problem is in general intractable , but present an algorithm than runs in polynomial time in the size of the graph when the regular expression and the graph are free of conflicts . we also present a class of languages whose expressions can always be evaluated in time polynomial in the size of both the graph and the expression , and characterize syntactically the expressions for such languages .
efficient algorithms for atmospheric correction of remotely sensed data . <eos> remotely sensed imagery has been used for developing and validating various studies regarding land cover dynamics . however , the large amounts of imagery collected by the satellites are largely contaminated by the effects of atmospheric particles . the objective of atmospheric correction is to retrieve the surface reflectance from remotely sensed imagery by removing the atmospheric effects . we introduce a number of computational techniques that lead to a substantial speedup of an atmospheric correction algorithm based on using look up tables . excluding i o time , the previous known implementation processes one pixel at a time and requires about <digit> . <digit> seconds per pixel on a sparc <digit> machine , while our implementation is based on processing the whole image and takes about <digit> <digit> microseconds per pixel on the same machine . we also develop a parallel version of our algorithm that is scalable in terms of both computation and i o . experimental results obtained show that a thematic mapper ( tm ) image ( <digit> mb per band , <digit> bands need to be corrected ) can be handled in less than <digit> . <digit> minutes on a <digit> node cm <digit> machine , including i o time .
towards modeling the performance of a fast connected components algorithm on parallel machines . <eos> we present and analyze a portable , high performance algorithm for finding connected components on modern distributed memory multiprocessors . the algorithm is a hybrid of the classic dfs on the subgraph local to each processor and a variant of the shiloach vishkin pram algorithm on the global collection of subgraphs . we implement the algorithm in split c and measure performance on the the cray t3d , the meiko cs <digit> , and the thinking machines cm <digit> using a class of graphs derived from cluster dynamics methods in computational physics . on a <digit> processor cray t3d , the implementation outperforms all previous solutions by an order of magnitude . a characterization of graph parameters allows us to select graphs that highlight key performance features . we study the effects of these parameters and machine characteristics on the balance of time between the local and global phases of the algorithm and find that edge density , surface to volume ratio , and relative communication cost dominate performance . by understanding the effect of machine characteristics on performance , the study sheds light on the impact of improvements in computational and or communication performance on this challenging problem .
message passing versus distributed shared memory on networks of workstations . <eos> the message passing programs are executed with the parallel virtual machine ( pvm ) library and the shared memory programs are executed using treadmarks . the programs are water and barnes hut from the splash benchmark suite <digit> d fft , integer sort ( is ) and embarrassingly parallel ( ep ) from the nas benchmarks ilink , a widely used genetic linkage analysis program and successive over relaxation ( sor ) , traveling salesman ( tsp ) , and quicksort ( qsort ) . two different input data sets were used for water ( water <digit> and water <digit> ) , is ( is small and is large ) , and sor ( sor zero and sor nonzero ) . our execution environment is a set of eight hp735 workstations connected by a 100mbits per second fddi network . for water <digit> , ep , ilink , sor zero , and sor nonzero , the performance of treadmarks is within <digit> % of pvm . for is small , water <digit> , barnes hut , <digit> d fft , tsp , and qsort , differences are on the order of <digit> % to <digit> % . finally , for is large , pvm performs two times better than treadmarks . more messages and more data are sent in treadmarks , explaining the performance differences . this extra communication is caused by <digit> ) the separation of synchronization and data transfer , <digit> ) extra messages to request updates for data by the invalidate protocol used in treadmarks , accumulation for migratory data in treadmarks .
distributing a chemical process optimization application over a gigabit network . <eos> we evaluate the impact of a gigabit network on the implementation of a distributed chemical process optimization application . the optimization problem is formulated as a stochastic linear assignment problem and was solved using the thinking machines cm <digit> ( simd ) and the cray c <digit> ( vector ) computers at psc , and the intel iwarp ( mimd ) system at cmu , connected by the gigabit nectar testbed . we report our experience distributing the application across this heterogeneous set of systems and present measurements that show how the communication requirements of the application depend on the structure of the application . we use detailed traces to build an application performance model that can be used to estimate the elapsed time of the application for different computer system and network combinations . our results show that the application benefits from the high speed network , and that the need for high network throughput is increasing as computer systems get faster . we also observed that supporting high burst rates is critical , although structuring the application so that communication is overlapped with computation relaxes the bandwidth requirements .
detecting coarse grain parallelism using an interprocedural parallelizing compiler . <eos> this paper presents an extensive empirical evaluation of an interprocedural parallelizing compiler , developed as part of the stanford suif compiler system . the system incorporates a comprehensive and integrated collection of analyses , including privatization and reduction recognition for both array and scalar variables , and symbolic analysis of array subscripts . the interprocedural analysis framework is designed to provide analysis results nearly as precise as full inlining but without its associated costs . experimentation with this system shows that it is capable of detecting coarser granularity of parallelism than previously possible . specifically , it can parallelize loops that span numerous procedures and hundreds of lines of codes , frequently requiring modifications to array data structures such as privatization and reduction transformations . measurements from several standard benchmark suites demonstrate that an integrated combination of interprocedural analyses can substantially advance the capability of automatic parallelization technology .
relative debugging and its application to the development of large numerical models . <eos> because large scientific codes are rarely static objects , developers are often faced with the tedious task of accounting for discrepancies between new and old versions . in this paper , we describe a new technique called relative debugging that addresses this problem by automating the process of comparing a modified code against a correct reference code . we examine the utility of the relative debugging technique by applying a relative debugger called guard to a range of debugging problems in a large atmospheric circulation model . our experience confirms the effectiveness of the approach . using guard , we are able to validate a new sequential version of the atmospheric model , and to identify the source of a significant discrepancy in a parallel version in a short period of time .
lazy release consistency for hardware coherent multiprocessors . <eos> release consistency is a widely accepted memory model for distributed shared memory systems . eager release consistency represents the state of the art in release consistent protocols for hardware coherent multiprocessors , while lazy release consistency has been shown to provide better performance for software distributed shared memory ( dsm ) . several of the optimizations performed by lazy protocols have the potential to improve the performance of hardware coherent multiprocessors as well , but their complexity has precluded a hardware implementation . with the advent of programmable protocol processors it may become possible to use them after all . we present and evaluate a lazy release consistent protocol suitable for machines with dedicated protocol processors . this protocol admits multiple concurrent writers , sends write notices concurrently with computation , and delays invalidations until acquire operations . we also consider a lazier protocol that delays sending write notices until release operations . our results indicate that the first protocol outperforms eager release consistency by as much as <digit> % across a variety of applications . the lazier protocol , on the other hand , is unable to recoup its high synchronization overhead . this represents a qualitative shift from the dsm world , where lazier protocols always yield performance improvements . based on our results , we conclude that machines with flexible hardware support for coherence should use protocols based on lazy release consistency , but in a less ' ' aggressively lazy ' ' form than is appropriate for dsm .
an hpf compiler for the ibm sp2 . <eos> we describe phpf , an research prototype hpf compiler for the ibm sp series parallel machines . the compiler accepts as input fortran <digit> and fortran <digit> programs , augmented with hpf directives sequential loops are automatically parallelized . the compiler supports symbolic analysis of expressions . this allows parameters such as the number of processors to be unknown at compile time without significantly affecting performance . communication schedules and computation guards are generated in a parameterized form at compile time . several novel optimizations and improved versions of well known optimizations have been implemented in phpf to exploit parallelism and reduce communication costs . these optimizations include elimination of redundant communication using data availability analysis using collective communication new techniques for mapping scalar variables coarse grain wavefronting and communication reduction in multi dimensional shift communications . we present experimental results for some well known benchmark routines . the results show the effectiveness of the compiler in generating efficient code for hpf programs .
the synergetic effect of compiler , architecture , and manual optimizations on the performance of cfd on multiprocessors . <eos> this paper discusses the comprehensive performance profiling , improvement and benchmarking of a computational fluid dynamics code , one of the grand challenge applications , on three popular multiprocessors . in the process of analyzing performance we considered language , compiler , architecture , and algorithmic changes and quantified each of them and their incremental contribution to bottom line performance . we demonstrate that parallelization alone can not result in significant gains if the granularity of parallel threads and the effect of parallelization on data locality are not taken into account . unlike benchmarking studies that often focus on the performance or effectiveness of parallelizing compilers on specific loop kernels , we used the entire cfd code to measure the global effectiveness of compilers and parallel architectures . we probed the performance bottlenecks in each case and derived solutions which eliminate or neutralize the performance inhibiting factors . the major conclusion of our work is that overall performance is extremely sensitive to the synergetic effects of compiler optimizations , algorithmic and code tuning , and architectural idiosyncrasies .
the chinook hardware software co synthesis system . <eos> abstract designers of embedded systems are facing ever tighter constraints on design time , but computer aided design tools for embedded systems have not kept pace with these trends . the chinook co synthesis system addresses the automation of the most time consuming and error prone tasks in embedded controller design , namely the synthesis of interface hardware and software needed to integrate system components , the migration of functions between processors or custom logic , and the co simulation of the design before , during and after synthesis . this paper describes the principal elements of chinook and discuss its application to a variety of embedded designs .
optimal register assignment to loops for embedded code generation . <eos> abstract one of the challenging tasks in code generation for embedded systems is register assignment . when more live variables than registers exist , some variables are necessarily accessed from data memory . because loops are typically executed many times and are often time critical , good register assignment in loops is exceedingly important , since accessing data memory can degrade performance . the issue of finding an optimal register assignment to loops , one which minimizes the number of spills between registers and memory , has been open for some time . in this paper , we address this issue and present an optimal , but exponential , algorithm which assigns registers to loop bodies such that the resulting spill code is minimal . we also show that a heuristic modification performs as well as the exponential approach on typical loops from scientific code .
real time multi tasking in software synthesis for information processing systems . <eos> abstract software synthesis is a new approach which focuses on the support of embedded systems without the use of operating systems . compared to traditional design practices , a better utilization of the available time and hardware resources can be achieved , because the static information provided by the system specification is fully exploited and an application specific solution is automatically generated . on going research on a software synthesis approach for real time information processing systems is presented which starts from a concurrent process system specification and tries to automate the mapping of this description to a single processor . an internal representation model which is well suited for the support of concurrency and timing constraints is proposed , together with flexible execution models for multi tasking with real time constraints . the method is illustrated on a personal terminal receiver demodulator for mobile satellite communication .
controlling application grain size on a network of workstations . <eos> an important challenge in the area of distributed computing is to automate the selection of the parameters that control the distributed computation . a performance critical parameter is the grain size of the computation , i . e . , the interval between successive synchronization points in the application . this parameter is hard to select since it depends both on compile time ( loop structure and data dependences , computational complexity ) and run time components ( speed of compute nodes and network ) . on networks of workstations that are shared with other users , the run time parameters can change over time . as a result , it is also necessary to consider the interactions with dynamic load balancing , which is needed to achieve good performance in this environment . in this paper we present a method for automatically selecting the grain size of the computation consisting of nested do loops . the method is based on close cooperation between the compiler and the runtime system . we evaluate the method using both simulation and measurements for an implementation on the nectar multicomputer .
time constrained code compaction for dsps . <eos> abstract dsp algorithms are , in most cases , subject to hard real time constraints . in the case of programmable dsps , meeting those constraints must be ensured by appropriate code generation techniques . for processors offering instruction level parallelism , the task of code generation includes code compaction . the exact timing behavior of a dsp program is only known after compaction . therefore , real time constraints should be taken into account during the compaction phase . while most known dsp code generators rely on rigid heuristics for that phase , this paper proposes a novel approach to local code compaction based on an integer programming model , which obeys exact timing constraints . due to a general problem formulation , the model also obeys encoding restrictions and possible side effects .
synthesis of pipelined dsp accelerators with dynamic scheduling . <eos> abstract to construct complete systems on silicon , application specific dsp accelerators are needed to speed up the execution of high throughput dsp algorithms . in this paper , a methodology is presented to synthesize high throughput dsp functions into accelerator processors containing a datapath of highly pipelined , bit parallel hardware units . emphasis is put on the definition of a controller architecture that allows efficient run time schedules of these dsp algorithms on such highly pipelined data paths . the methodology is illustrated by means of an fft butterfly accelerator block .
an exact methodology for scheduling in a 3d design space . <eos> abstract this paper describes an exact solution methodology , implemented in rensselaer ' s voyager design space exploration system , for solving the scheduling problem in a <digit> dimensional ( 3d ) design space the usual 2d design space ( which trades off area and schedule length ) , plus a third dimension representing clock length . unlike design space exploration methodologies which rely on bounds or estimates , this methodology is guaranteed to find the globally optimal solution to the 3d scheduling problem . furthermore , this methodology efficiently prunes the search space , eliminating provably inferior design points through a careful selection of candidate clock lengths and tight bounds on the number of functional units of each type or on the schedule length .
power analysis and low power scheduling techniques for embedded dsp software . <eos> abstract this paper describes the application of a measurement based power analysis technique for an embedded dsp processor . an instruction level power model for the processor has been developed using this technique . significant points of difference have been observed between this model and the ones developed earlier for some general purpose commercial microprocessors . in particular , the effect of circuit state on the power cost of an instruction stream is more marked in the case of this dsp processor . in addition , the dsp processor has a special architectural feature that allows instructions to be packed into pairs . the energy reduction possible through the use of this feature is studied . the on chip booth multiplier on the processor is a major source of energy consumption for dsp programs . a micro architectural power model for the multiplier is developed and analyzed for further energy minimization . a scheduling algorithm incorporating these new techniques is proposed to reduce the energy consumed by dsp software . energy reductions varying from <digit> % to <digit> % have been observed for several example programs . these energy savings are real and have been verified through physical measurement .
bounded skew clock and steiner routing under elmore delay . <eos> we study the minimum cost bounded skew routing tree problem under the elmore delay model . we present two approaches to construct bounded skew routing trees ( i ) the boundary merging and embedding ( bme ) method which utilizes merging points that are restricted to the boundaries of merging regions , and ( ii ) the interior merging and embedding ( ime ) algorithm which employs a sampling strategy and dynamic programming to consider merging points that are interior to , rather than on the boundary of , the merging regions . our new algorithms allow accurate control of elmore delay skew , and show the utility of merging points inside merging regions .
optimal wire sizing and buffer insertion for low power and a generalized delay model . <eos> we present efficient , optimal algorithms for timing optimization by discrete wire sizing and buffer insertion . our algorithms are able to minimize dynamic power dissipation subject to given timing constraints . in addition , we compute the complete power delay tradeoff curve for added flexibility . we extend our algorithm to take into account the effect of signal slew on buffer delay which can contribute substantially to overall delay . the effectiveness of these methods is demonstrated experimentally .
hybrid decision diagrams . <eos> abstract functions that map boolean vectors into the integers are important for the design and verification of arithmetic circuits . mtbdds and bmds have been proposed for representing this class of functions . we discuss the relationship between these methods and describe a generalization called hybrid decision diagrams which is often much more concise . we show how to implement arithmetic operations efficiently for hybrid decision diagrams . in practice , this is one of the main limitations of bmds since performing arithmetic operations on functions expressed in this notation can be very expensive . in order to extend symbolic model checking algorithms to handle arithmetic properties , it is essential to be able to compute the bdd for the set of variable assignments that satisfy an arithmetic relation . in our paper , we give an efficient algorithm for this purpose . moreover , we prove that for the class of linear expressions , the time complexity of our algorithm is linear in the number of variables .
synthesizing petri nets from state based models . <eos> this paper presents a method to synthesize labeled petri nets from state based models . although state based models ( such as finite state machines ) are a powerful formalism to describe the behavior of sequential systems , they can not explicitly express the notions of concurrency , causality and conflict . petri nets can naturally capture these notions . the proposed method in based on deriving an elementary transition system ( ets ) from a specification model . previous work has shown that for any ets there exists a petri net with minimum transition count ( one transition for each label ) with a reachability graph isomorphic to the original ets . this paper presents the first known approach to obtain an ets from a non elementary ts and derive a place irredundant petri net . furthermore , by imposing constraints on the synthesis method , different classes of petri nets can be derived from the same reachability graph ( pure , free choice , unique choice ) . this method has been implemented and efficiently applied in different frameworks petri net composition , synthesis of petri nets from asynchronous circuits , and resynthesis of petri nets .
design verification via simulation and automatic test pattern generation . <eos> we present a simulation based method for combinational design verification that aims at complete coverage of specified design errors using conventional atpg tools . the error models used in prior research are examined and reduced to four types gate substitution errors ( gses ) , gate count errors ( gces ) , input count errors ( ices ) , and wrong input errors ( wies ) . conditions are derived for a gate to be completely testable for gses these conditions lead to small test sets for gses . near minimal test sets are also derived for gces . we analyze redundancy in design errors and relate this to single stuck line ( ssl ) redundancy . we show how to map all the foregoing error types into ssl faults , and describe an extensive set of experiments to evaluate the proposed method . our experiments demonstrate that high coverage of the modeled design errors can be achieved with small test sets .
diagnosis of realistic bridging faults with single stuck at information . <eos> abstract precise failure analysis requires accurate fault diagnosis . a previously proposed method for diagnosing bridging faults using single stuck at dictionaries was applied only to small circuits , produced large and imprecise diagnoses , and did not take into account the byzantine generals problem for bridging faults . we analyze the original technique and improve it by introducing the concepts of match restriction , match requirement , and failure recovery . our new technique , which requires no information other than that used by standard stuck at methods , produces diagnoses that are an order of magnitude smaller than those produced by the original technique and produces many fewer misleading diagnoses than that of traditional stuck at diagnosis .
instruction selection using binate covering for code size optimization . <eos> we address the problem of instruction selection in code generation for embedded dsp microprocessors . such processors have highly irregular data paths , and conventional code generation methods typically result in inefficient code . instruction selection can be formulated as directed acyclic graph ( dag ) covering . conventional methods for instruction selection use heuristics that break up the dag into a forest of trees and then cover them independently . this breakup can result in suboptimal solutions for the original dag . alternatively , the dag covering problem can be formulated as a binate covering problem , and solved exactly or heuristically using branch and bound methods . we show that optimal instruction selection on a dag in the case of accumulator based architectures requires a partial scheduling of nodes in the dag , and we augment the binate covering formulation to minimize spills and reloads . we show how the irregular data transfer costs of typical dsp data paths can be modeled in the binate covering formulation .
optimal wiresizing for interconnects with multiple sources . <eos> the optimal wiresizing problem for nets with multiple sources is studied under the distributed elmore delay model . we decompose such a net into a source subtree ( sst ) and a set of loading subtrees ( lsts ) , and show the optimal wiresizing solution satisfies a number of interesting properties , including the lst separability , the lst monotone property , the sst local monotone property and the general dominance property . furthermore , we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property . these properties lead to efficient algorithms to compute the lower and upper bounds of the optimal solutions . experiment results on nets from an intel processor layout show an interconnect delay reduction of up to <digit> . <digit> % when compared to the minimum width solution . in addition , the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy , when compared with the fixed grid based methods .
an iterative improvement algorithm for low power data path synthesis . <eos> we address the problem of minimizing power consumption in behavioral synthesis of data dominated circuits . the complex nature of power as a cost function implies that the effects of several behavioral synthesis tasks like module selection , clock selection , scheduling , and resource sharing on supply voltage and switched capacitance need to be considered simultaneously to fully derive the benefits of design space exploration at the behavior level . recent work has established the importance of behavioral synthesis in low power vlsi design . however , most of the algorithms that have been proposed separate these tasks and perform them sequentially , and are hence not able to explore the tradeoffs possible due to their interaction . we present an efficient algorithm for performing scheduling , clock selection , module selection , and resource allocation and assignment simultaneously with an aim of reducing the power consumption in the synthesized data path . the algorithm , which is based on an iterative improvement strategy , is capable of escaping local minima in its search for a low power solution . the algorithm considers diverse module libraries and complex scheduling constructs such as multicycling , chaining , and structural pipelining . we describe supply voltage and clock pruning strategies that significantly improve the efficiency of our algorithm by cutting down on the computational effort involved in exploring candidate supply voltages and clock periods that are unlikely to lead to the best solution . experimental results are reported to demonstrate the effectiveness of the algorithm . our techniques can be combined with other known methods of behavioral power optimization like data path replication and transformations , to result in a complete data path synthesis system for low power applications .
iteration abstraction in sather . <eos> sather extends the notion of an iterator in a powerful new way . we argue that iteration abstractions belong in class interfaces on an equal footing with routines . sather iterators were derived from clu iterators but are much more flexible and better suited for object oriented programming . we retain the property that iterators are structured , i . e . , strictly bound to a controlling structured statement . we motivate and describe the construct along with several simple examples . we compare it with iteration based on clu iterators , cursors , riders , streams , series , generators , coroutines , blocks , closures , and lambda expressions . finally , we describe experiences with iterators in the sather compiler and libraries .
the mean square discrepancy of randomized nets . <eos> one popular family of low <unk> sets is the ( t , m , s ) nets . recently a randomization of these nets that preserves their net property has been introduced . in this article a formula for the mean square l2 discrepancy of ( <digit> , m , s ) nets in base b is derived . this formula has a computational complexity of only o ( s log ( n ) s2 ) for large n or s , where bm is the number of points . moreover , the root mean square l2 discrepancy of ( <digit> , m , s ) nets is show to be o ( n <digit> log ( n ) ( s <digit> ) <digit> ) as n tends to infinity , the same asymptotic order as the known lower bound for the l2 discrepancy of an arbitrary set .
analysis of bounded time warp and comparison with yawns . <eos> this article studies an analytic model of parallel discrete event simulation , comparing the yawns conservative synchronization protocol with bounded time warp . the assumed simulation problem is a heavily loaded queuing network where the probability of an idle server is closed to zero . we model workload and job routing in standard ways , then develop and validate methods for computing approximated performance measures as a function of the degree of optimism allowed , overhead costs of state saving , rollback , and barrier synchronization , and workload aggregation . we find that bounded time warp is superior when the number of servers per physical processor is low ( i . e . , sparse load ) , but that aggregating workload improves yawns relative performance .
the block distributed memory model . <eos> <unk> introduce a computation model for developing and analyzing parallel algorithms on distributed memory machines . the model allows the design of algorithms using a single address space and does not assume any particular interconnection topology . we capture performance by incorporating a cost measure for interprocessor communication induced by remote memory accesses . the cost measure includes parameters reflecting memory latency , communication bandwidth , and spatial locality . our model allows the initial placement of the input data and pipelined prefetching . we use our model to develop parallel algorithms for various data rearrangement problems , load balancing , sorting , fft , and matrix multiplication . we show that most of these algorithms achieve optimal or near optimal communication complexity while simultaneously guaranteeing an optimal speed up in computational complexity . ongoing experimental work in testing and evaluating these algorithms has thus far shown very promising results .
a necessary and sufficient condition for deadlock free routing in cut through and store and forward networks . <eos> <unk> paper develops the theoretical background for the design of deadlock free adaptive routing algorithms for virtual cut through and store and forward switching . this theory is valid for networks using either central buffers or edge buffers . some basic definitions and three theorems are proposed , developing conditions to verify that an adaptive algorithm is deadlock free , even when there are cyclic dependencies between routing resources . moreover , we propose a necessary and sufficient condition for deadlock free routing . also , a design methodology is proposed . it supplies fully adaptive , minimal and non minimal routing algorithms , guaranteeing that they are deadlock free . the theory proposed in this paper extends the necessary and sufficient condition for wormhole switching previously proposed by us . the resulting routing algorithms are more flexible than the ones for wormhole switching . also , the design methodology is much easier to apply because it automatically supplies deadlock free routing algorithms .
localizing failures in distributed synchronization . <eos> <unk> fault tolerance of distributed algorithms is investigated in asynchronous message passing systems with undetectable process failures . two specific synchronization problems are considered , the dining philosophers problem and the binary committee coordination problem . the abstraction of a bounded doorway is introduced as a general mechanism for achieving individual progress and good failure locality . using it as a building block , optimal fault tolerant algorithms are constructed for the two problems .
static assignment of stochastic tasks using majorization . <eos> <unk> consider the problem of statically assigning many tasks to a ( smaller ) system of homogeneous processors , where a task ' s structure is modeled as a branching process , all tasks are assumed to have identical behavior , and the tasks may synchronize frequently . we show how the theory of majorization can be used to obtain a partial order among possible task assignments . we show that if the vector of numbers of tasks assigned to each processor under one mapping is majorized by that of another mapping , then the former mapping is better than the latter with respect to a large number of objective functions . in particular , we show how the metrics of finishing time , the space time product , and reliability are all captured . we also apply majorization to the problem of partitioning a pool of processors for distribution among parallelizable tasks . limitations of the approach , which include the static nature of the assignment , are also discussed .
an analysis of the average message overhead in replica control protocols . <eos> <unk> of replicated data has received considerable attention in the last few years . several replica control schemes have been proposed which work in the presence of both node and communication link failures . however , this resiliency to failure inflicts a performance penalty in terms of the communication overhead incurred . though the issue of performance of these schemes from the standpoint of availability of the system has been well addressed , the issue of message overhead has been limited to the analysis of worst case and best case message bounds . in this paper we derive expressions for computing the average message overhead of several well known replica control protocols and provide a comparative study of the different protocols with respect to both average message overhead and system availabilities .
parallel divide and conquer on meshes . <eos> <unk> address the problem of mapping divide and conquer programs to mesh connected multicomputers with wormhole or store and forward routing . we propose the binomial tree as an efficient model of parallel divide and conquer and present two mappings of the binomial tree to the 2d mesh . our mappings exploit regularity in the communication structure of the divide and conquer computation and are also sensitive to the underlying flow control scheme of the target architecture . we evaluate these mappings using new metrics which are extensions of the classical notions of dilation and contention . we introduce the notion of communication slowdown as a measure of the total communication overhead incurred by a parallel computation . we conclude that significant performance gains can be realized when the mapping is sensitive to the flow control scheme of the target architecture .
file access characteristics of parallel scientific workloads . <eos> <unk> improvements in the computational performance of multiprocessors have not been matched by comparable gains in i o system performance . this imbalance has resulted in i o becoming a significant bottleneck for many scientific applications . one key to overcoming this bottleneck is improving the performance of multiprocessor file systems . the design of a high performance multiprocessor file system requires a comprehensive understanding of the expected workload . unfortunately , until recently , no general workload studies of multiprocessor file systems have been conducted . the goal of the charisma project was to remedy this problem by characterizing the behavior of several production workloads , on different machines , at the level of individual reads and writes . the first set of results from the charisma project describe the workloads observed on an intel ipsc <digit> and a thinking machines cm <digit> . this paper is intended to compare and contrast these two workloads for an understanding of their essential similarities and differences , isolating common trends and platform dependent variances . using this comparison , we are able to gain more insight into the general principles that should guide multiprocessor file system design .
parallelized direct execution simulation of message passing parallel programs . <eos> <unk> massively parallel computers proliferate , there is growing interest in finding ways by which performance of massively parallel codes can be efficiently predicted . this problem arises in diverse contexts such as parallelizing compilers , parallel performance monitoring , and parallel algorithm development . in this paper , we describe one solution where one directly executes the application code , but uses a discrete event simulator to model details of the presumed parallel machine , such as operating system and communication network behavior . because this approach is computationally expensive , we are interested in its own parallelization , specifically the parallelization of the discrete event simulator . we describe methods suitable for parallelized direct execution simulation of message passing parallel programs , and report on the performance of such a system , lapse ( large application parallel simulation environment ) , we have built on the intel paragon . on all codes measured to date , lapse predicts performance well , typically within <digit> % relative error . depending on the nature of the application code , we have observed low slowdowns ( relative to natively executing code ) and high relative speedups using up to <digit> processors .
on tridiagonalizing and diagonalizing symmetric matrices with repeated eigenvalues . <eos> we describe a divide and conquer tridiagonalization approach for matrices with repeated eigenvalues . our algorithm hinges on the fact that , under easily constructively verifiable conditions , a symmetric matrix with band width b and k distinct eigenvalues must be block diagonal with diagonal blocks of size at most b k . a slight modification of the usual orthogonal band reduction algorithm allows us to reveal this structure , which then leads to potential parallelism in the form of independent diagonal blocks . compared to the usual householder reduction algorithm , the new approach exhibits improved data locality , significantly more scope for parallelism , and the potential to reduce arithmetic complexity by close to <digit> % for matrices that have only two numerically distinct eigenvalues . the actual improvement depends to a large extent on the number of distinct eigenvalues and a good estimate thereof . however , at worst the algorithms behave like a successive band reduction approach to tridiagonalization . moreover , we provide a numerically reliable and effective algorithm for computing the eigenvalue decomposition of a symmetric matrix with two numerically distinct eigenvalues . such matrices arise , for example , in invariant subspace decomposition approaches to the symmetric eigenvalue problem .
an approximate minimum degree ordering algorithm . <eos> an approximate minimum degree ( amd ) ordering algorithm for preordering a symmetric sparse matrix prior to numerical factorization is presented . we use techniques based on the quotient graph for matrix factorization that allow us to obtain computationally cheap bounds for the minimum degree . we show that these bounds are often equal to the actual degree . the resulting algorithm is typically much faster than previous minimum degree ordering algorithms and produces results that are comparable in quality with the best orderings from other minimum degree algorithms .
group invariance and convex matrix analysis . <eos> certain interesting classes of functions on a real inner product space are invariant under an associated group of orthogonal linear transformations . this invariance can be made explicit via a simple decomposition . for example , rotationally invariant functions on bf r <digit> are just even functions of the euclidean norm , and functions on the hermitian matrices ( with trace inner product ) which are invariant under unitary similarity transformations are just symmetric functions of the eigenvalues . we develop a framework for answering geometric and analytic ( both classical and nonsmooth ) questions about such a function by answering the corresponding question for the ( much simpler ) function appearing in the decomposition . the aim is to understand and extend the foundations of eigenvalue optimization , matrix approximation , and semidefinite programming .
an analytical model for designing memory hierarchies . <eos> <unk> hierarchies have long been studied by many means system building , trace driven simulation , and mathematical analysis . yet little help is available for the system designer wishing to quickly size the different levels in a memory hierarchy to a first order approximation . in this paper , we present a simple analysis for providing this practical help and some unexpected results and intuition that come out of the analysis . by applying a specific , parameterized model of workload locality , we are able to derive a closed form solution for the optimal size of each hierarchy level . we verify the accuracy of this solution against exhaustive simulation with two case studies a three level i o storage hierarchy and a three level processor cache hierarchy . in all but one case , the configuration recommended by the model performs within <digit> % of optimal . one result of our analysis is that the first place to spend money is the cheapest ( rather than the fastest ) cache level , particularly with small system budgets . another is that money spent on an n level hierarchy is spent in a fixed proportion until another level is added .
a persistent rescheduled page cache for low overhead object code compatibility in vliw architectures . <eos> object code compatibility between processor generations is an open issue for vliw architectures . a potential solution is a technique termed dynamic rescheduling , which performs run time software rescheduling at the first time page faults . the time required for rescheduling the pages constitutes a large portion of the overhead of this method . a disk caching scheme that uses a persistent rescheduled page cache ( prc ) is presented . the scheme reduces the overhead associated with dynamic rescheduling by saving rescheduled pages on disk , across program executions . operating system support is required for dynamic rescheduling and management of the prc . the implementation details for the prc are discussed . results of simulations used to gauge the effectiveness of prc indicate that the prc is effective in reducing the overhead of dynamic rescheduling and due to different overhead requirements of programs , a split prc organization performs better than a unified prc . the unified prc was studied for two different page replacement policies lru and overhead based replacement . it was found that with lru replacement , all the programs consistently perform better with increasing prc sizes , but the high overhead programs take a consistent performance hit compared to the low overhead programs . with overhead based replacement , the performance of high overhead programs improves substantially , while the low overhead programs perform only slightly worse than in the case of the lru replacement .
profile driven instruction level parallel scheduling with application to super blocks . <eos> code scheduling to exploit instruction level parallelism ( ilp ) is a critical problem in compiler optimization research in light of the increased use of long instruction word machines . unfortunately optimum scheduling is computationally intractable , and one must resort to carefully crafted heuristics in practice . if the scope of application of a scheduling heuristic is limited to basic blocks , considerable performance loss may be incurred at block boundaries . to overcome this obstacle , basic blocks can be coalesced across branches to form larger regions such as super blocks . in the literature , these regions are typically scheduled using algorithms that are either oblivious to profile information ( under the assumption that the process of forming the region has fully utilized the profile information ) , or use the profile information as an addendum to classical scheduling techniques . we believe that even for the simple case of linear code regions such as super blocks , additional performance improvement can be gained by utilizing the profile information in scheduling as well . we propose a general paradigm for converting any profile insensitive list scheduler to a profile sensitive scheduler . our technique is developed via a theoretical analysis of a simplified abstract model of the general problem of profile driven scheduling over any acyclic code region , yielding a scoring measure for ranking branch instructions .
modulo scheduling of loops in control intensive non numeric programs . <eos> much of the previous work on modulo scheduling has targeted numeric programs , in which , often , the majority of the loops are well behaved loop counter based loops without early exits . in control intensive non numeric programs , the loops frequently have characteristics that make it more difficult to effectively apply modulo scheduling . these characteristics include multiple control flow paths , loops that are not based on a loop counter , and multiple exits . in these loops , the presence of unimportant paths with high resource usage or long dependence chains can penalize the important paths . a path that contains a hazard such as another nested loop can prohibit modulo scheduling of the loop . control dependences can severely restrict the overlap of the blocks within and across iterations . this paper describes a set of methods that allow effective modulo scheduling of loops with multiple exits . the techniques include removal of control dependences to enable speculation , extensions to modulo variable expansion , and a new epilogue generation scheme . these methods can be used with superblock and hyperblock techniques to allow modulo scheduling of the selected paths of loops with arbitrary control flow . a case study is presented to show how these methods , combined with superblock techniques , enable modulo scheduling to be effectively applied to control intensive non numeric programs . performance results for several spec cint92 benchmarks and unix utility programs are reported and demonstrate the applicability of modulo scheduling to this class of programs .
assigning confidence to conditional branch predictions . <eos> many high performance processors predict conditional branches and consume processor resources based on the prediction . in some situations , resource allocation can be better optimized if a confidence level is assigned to a branch prediction i . e . if the quantity of resources allocated is a function of the confidence level . to support such optimizations , we consider hardware mechanisms that partition conditional branch predictions into two sets those which are accurate a relatively high percentage of the time , and those which are accurate a relatively low percentage of the time . the objective is to concentrate as many of the mispredictions as practical into a relatively small set of low confidence dynamic branches . we first study an ideal method that profiles branch predictions and sorts static branches into high and low confidence sets , depending on the accuracy with which they are dynamically predicted . we find that about <digit> percent of the mispredictions can be localized to a set of static branches that account for <digit> percent of the dynamic branches . we then study idealized dynamic confidence methods using both one and two levels of branch correctness history . we find that the single level method performs at least as well as the more complex two level method and is able to isolate <digit> percent of the mispredictions into a set containing <digit> percent of the dynamic branches . finally , we study practical , less expensive implementations and find that they achieve most of the performance of the idealized methods .
instruction fetch mechanisms for vliw architectures with compressed encodings . <eos> vliw architectures use very wide instruction words in conjunction with high bandwidth to the instruction cache to achieve multiple instruction issue . this report uses the tinker experimental testbed to examine instruction fetch and instruction cache mechanisms for vliws . a compressed instruction encoding for vliws is defined and a classification scheme for i fetch hardware for such an encoding is introduced . several interesting cache and i fetch organizations are described and evaluated through trace driven simulations . a new i fetch mechanism using a silo cache is found to have the best performance .
estimating optical flow in segmented images using variable order parametric models with local deformations . <eos> <unk> paper presents a new model for estimating optical flow based on the motion of planar regions plus local deformations . the approach exploits brightness information to organize and constrain the interpretation of the motion by using segmented regions of piecewise smooth brightness to hypothesize planar regions in the scene . parametric flow models are estimated in these regions in a two step process which first computes a coarse fit and estimates the appropriate parameterization of the motion of the region ( two , six , or eight parameters ) . the initial fit is refined using a generalization of the standard area based regression approaches . since the assumption of planarity is likely to be violated , we allow local deformations from the planar assumption in the same spirit as physically based approaches which model shape using coarse parametric models plus local deformations . this parametric deformation model exploits the strong constraints of parametric approaches while retaining the adaptive nature of regularization approaches . experimental results on a variety of images indicate that the parametric deformation model produces accurate flow estimates while the incorporation of brightness segmentation provides precise localization of motion boundaries .
scale space properties of quadratic feature detectors . <eos> <unk> detectors using a quadratic nonlinearity in the filtering stage are known to have some advantages over linear detectors here , we consider their scale space properties . in particular , we investigate whether , like linear detectors , quadratic feature detectors permit a scale selection scheme with the causality property , which guarantees that features are never created as scale is coarsened . we concentrate on the design most common in practice , i . e . , one dimensional detectors with two constituent filters , with scale selection implemented as convolution with a scaling function . we consider two special cases of interest constituent filter pairs related by the hilbert transform , and by the first spatial derivative . we show that , under reasonable assumptions , hilbert pair quadratic detectors can not have the causality property . in the case of derivative pair detectors , we describe a family of scaling functions related to fractional derivatives of the gaussian that are necessary and sufficient for causality . in addition , we report experiments that show the effects of these properties in practice . thus we show that at least one class of quadratic feature detectors has the same desirable scaling property as the more familiar detectors based on linear filtering .
a framework for resource constrained rate optimal software pipelining . <eos> <unk> rapid advances in high performance computer architecture and compilation techniques provide both challenges and opportunities to exploit the rich solution space of software pipelined loop schedules . in this paper , we develop a framework to construct a software pipelined loop schedule which runs on the given architecture ( with a fixed number of processor resources ) at the maximum possible iteration rate ( la rate optimal ) while minimizing the number of <unk> close approximation to minimizing the number of registers . the main contributions of this paper are first , we demonstrate that such problem can be described by a simple mathematical formulation with precise optimization objectives under a periodic linear scheduling framework . the mathematical formulation provides a clear picture which permits one to visualize the overall solution space ( for rate optimal schedules ) under different sets of constraints . secondly , we show that a precise mathematical formulation and its solution does make a significant performance difference . we evaluated the performance of our method against three leading contemporary heuristic methods . experimental results show that the method described in this paper performed significantly better than these methods . the techniques proposed in this paper are useful in two different ways <digit> ) as a compiler option which can be used in generating faster schedules for performance critical loops ( if the interested users are willing to trade the cost of longer compile time with faster runtime ) . <digit> ) as a framework for compiler writers to evaluate and improve other heuristics based approaches by providing quantitative information as to where and how much their heuristic methods could be further improved .
on the fully commutative elements of coxeter groups . <eos> let w be a coxeter group . we define an element w w to be fully commutative if any reduced expression for w can be obtained from any other by means of braid relations that only involve commuting generators . we give several combinatorial characterizations of this property , classify the coxeter groups with finitely many fully commutative elements , and classify the parabolic quotients whose members are all fully commutative . as applications of the latter , we classify all parabolic quotients with the property that ( <digit> ) the bruhat ordering is a lattice , ( <digit> ) the bruhat ordering is a distributive lattice , ( <digit> ) the weak ordering is a distributive lattice , and ( <digit> ) the weak ordering and bruhat ordering coincide .
analytical delay models for vlsi interconnects under ramp input . <eos> elmore delay has been widely used as an analytical estimate of interconnect delays in the performance driven synthesis and layout of vlsi routing topologies . however , for typical rlc interconnections with ramp input , elmore delay can deviate by up to <digit> % or more from spice computed delay since it is independent of rise time of the input ramp signal . we develop new analytical delay models based on the first and second moments of the interconnect transfer function when the input is a ramp signal with finite rise time . delay estimates using our first moment based analytical models are within <digit> % of spice computed delay , and models based on both first and second moments are within <digit> . <digit> % of spice , across a wide range of interconnect parameter values . evaluation of our analytical models is several orders of magnitude faster than simulation using spice . we also describe extensions of our approach for estimation of source sink delays in arbitrary interconnect trees .
register transfer level estimation techniques for switching activity and power consumption . <eos> we present techniques for estimating switching activity and power consumption in register transfer level ( rtl ) circuits . previous work on this topic has ignored the presence of glitching activity at various data path and control signals , which can lead to significant underestimation of switching activity . for data path blocks that operate on word level data , we construct piecewise linear models that capture the variation of output glitching activity and power consumption with various word level parameters like mean , standard deviation , spatial and temporal correlations , and glitching activity at the block ' s inputs . for rtl blocks that operate on data that need not have an associated word level value , we present accurate bit level modeling techniques for glitching activity as well as power consumption . this allows us to perform accurate power estimation for control flow intensive circuits , where most of the power consumed is dissipated in non arithmetic components like multiplexers , registers , vector logic operators , etc . since the final implementation of the controller is not available during high level design iterations , we develop techniques that estimate glitching activity at control signals using control expressions and partial delay information . experiments on example rtl designs resulted in power estimates that were within <digit> % of those produced by an inhouse power analysis tool on the final gate level implementation .
an efficient approach to simultaneous transistor and interconnect sizing . <eos> in this paper , we study the simultaneous transistor and interconnect sizing ( stis ) problem . we define a class of optimization problems as ch posynomial programs and reveal a general dominance property for all ch posynomial programs . we show that the stis problems under a number of transistor delay models are ch posynomial programs and propose an efficient and near optimal stis algorithm based on the dominance property . when used to solve the simultaneous driver buffer and wire sizing problem for real designs , it reduces the maximum delay by up to <digit> . <digit> % , and more significantly , reduces the power consumption by a factor of <digit> . 63x , when compared with the original designs . when used to solve the transistor sizing problem , it achieves a smooth area delay trade off . moreover , the algorithm optimizes a clock net of <digit> drivers buffers and <digit> spl mu m long wire in <digit> seconds , and a <digit> bit adder with <digit> transistors in <digit> seconds on a sparc <digit> workstation .
vlsi circuit partitioning by cluster removal using iterative improvement techniques . <eos> move based iterative improvement partitioning methods such as the fiduccia mattheyses ( fm ) algorithm and krishnamurthy ' s look ahead ( la ) algorithm are widely used in vlsi cad applications largely due to their time efficiency and ease of implementation . this class of algorithms is of the local improvement type . they generate relatively high quality results for small and medium size circuits . however , as vlsi circuits become larger , these algorithms are not so effective on them as direct partitioning tools . we propose new iterative improvement methods that select cells to move with a view to moving clusters that straddle the two subsets of a partition into one of the subsets . the new algorithms significantly improve partition quality while preserving the advantage of time efficiency . experimental results on <digit> medium to large size acm sigda benchmark circuits show up to <digit> % improvement over fm in cutsize , with an average of per circuit percent improvements of about <digit> % , and a total cut improvement of about <digit> % . they also outperform the recent placement based partitioning tool paraboli and the spectral partitioner melo by about <digit> % and <digit> % , respectively , with less cpu time . this demonstrates the potential of iterative improvement algorithms in dealing with the increasing complexity of modern vlsi circuitry .
implementing fail silent nodes for distributed systems . <eos> abstracta fail silent node is a self checking node that either functions correctly or stops functioning after an internal failure is detected . such a node can be constructed from a number of conventional processors . in a software implemented fail silent node , the nonfaulty processors of the node need to execute message order and comparison protocols to keep in step and check each other , respectively . in this paper , the design and implementation of efficient protocols for a two processor fail silent node are described in detail . the performance figures obtained indicate that in a wide class of applications requiring a high degree of fault tolerance , software implemented fail silent nodes constructed simply by utilizing standard off the shelf components are an attractive alternative to their hardware implemented counterparts that do require special purpose hardware components , such as fault tolerant clocks , comparator , and bus interface circuits .
the strict time lower bound and optimal schedules for parallel prefix with resource constraints . <eos> <unk> computation is a basic operation at the core of many important applications , e . g . , some of the grand challenge problems , circuit design , digital signal processing , graph optimizations , and computational geometry . <digit> in this paper , we present new and strict time optimal parallel schedules for prefix computation with resource constraints under the concurrent read exclusive write ( crew ) parallel random access machine ( pram ) model . for prefix of n elements on p processors ( p independent of n ) when n > p ( p <digit> ) <digit> , we derive harmonic schedules that achieve the strict optimal time ( steps ) , left lceil <digit> left ( n <digit> right ) mathord left vphantom <digit> left ( n <digit> right ) left ( p <digit> right ) right . kern nulldelimiterspace left ( p <digit> right ) right rceil . we also derive pipelined schedules that have better program space efficiency than the harmonic schedule , yet only require a small constant number of steps more than the optimal time achieved by the harmonic schedule . both the harmonic schedules and the pipelined schedules are simple and easy to implement . for prefix of n elements on p processors ( p independent of n ) where np ( p <digit> ) <digit> , the harmonic schedules are not time optimal . for these cases , we establish an optimization method for determining key parameters of time optimal schedules , based on connections between the structure of parallel prefix and pascal ' s triangle . using the derived parameters , we devise an algorithm to construct such schedules . for a restricted class of values of n and p , we prove that the constructed schedules are strictly time optimal . we also give strong empirical evidence that our algorithm constructs strict time optimal schedules for all cases where np ( p
latch optimization in circuits generated from high level descriptions . <eos> in a gate level description of a finite state machine ( fsm ) , there is a tradeoff between the number of latches and the size of the logic implementing the next state and output functions . typically , an initial implementation is generated via explicit state assignment or translation from a high level language , and the tradeoff is subsequently only lightly explored . we efficiently explore good latch logic tradeoffs for large designs generated from high level specifications . we reduce the number of latches while controlling the logic size . we demonstrate the efficacy of our techniques on some large industrial examples .
rigidity checking of 3d point correspondences under perspective projection . <eos> <unk> algorithm is described which rapidly verifies the potential rigidity of three dimensional point correspondences from a pair of two dimensional views under perspective projection . the output of the algorithm is a simple yes or no answer to the question could these corresponding points from two views be the projection of a rigid configuration potential applications include 3d object recognition from a single previous view and correspondence matching for stereo or motion over widely separated views . the rigidity checking problem is different from the structure from motion problem because it is often the case that two views can not provide an accurate structure from motion estimate due to ambiguity and ill conditioning , whereas it is still possible to give an accurate yes no answer to the rigidity question . rigidity checking verifies point correspondences using 3d recovery equations as a matching condition . the proposed algorithm improves upon other methods that fall under this approach because it works with as few as six corresponding points under full perspective projection , handles correspondences from widely separated views , makes full use of the disparity of the correspondences , and is integrated with a linear algorithm for 3d recovery due to <unk> . results are given for experiments with synthetic and real image data . a complete implementation of this algorithm is being made publicly available .
real time focus range sensor . <eos> <unk> of dynamic scenes can only be recovered using a real time range sensor . depth from defocus offers an effective solution to fast and dense range estimation . however , accurate depth estimation requires theoretical and practical solutions to a variety of problems including recovery of textureless surfaces , precise blur estimation , and magnification variations caused by defocusing . both textured and textureless surfaces are recovered using an illumination pattern that is projected via the same optical path used to acquire images . the illumination pattern is optimized to maximize accuracy and spatial resolution in computed depth . the relative blurring in two images is computed using a narrow band linear operator that is designed by considering all the optical , sensing , and computational elements of the depth from defocus system . defocus invariant magnification is achieved by the use of an additional aperture in the imaging optics . a prototype focus range sensor has been developed that has a workspace of <digit> cubic foot and produces up to <digit> <digit> depth estimates at hz with an average rms error of <digit> . <digit> % . several experimental results are included to demonstrate the performance of the sensor .
motion estimation with quadtree splines . <eos> <unk> paper presents a motion estimation algorithm based on a new multiresolution representation , the quadtree spline . this representation describes the motion field as a collection of smoothly connected patches of varying size , where the patch size is automatically adapted to the complexity of the underlying motion . the topology of the patches is determined by a quadtree data structure , and both split and merge techniques are developed for estimating this spatial subdivision . the quadtree spline is implemented using another novel representation , the adaptive hierarchical basis spline , and combines the advantages of adaptively sized correlation windows with the speedups obtained with hierarchical basis preconditioners . results are presented on some standard motion sequences .
an application of petri net reduction for ada tasking deadlock analysis . <eos> <unk> part of our continuing research on using petri nets to support automated analysis of ada tasking behavior , we have investigated the application of petri net reduction for deadlock analysis . although reachability analysis is an important method to detect deadlocks , it is in general inefficient or even intractable . net reduction can aid the analysis by reducing the size of the net while preserving relevant properties . we introduce a number of reduction rules and show how they can be applied to ada nets , which are automatically generated petri net models of ada tasking . we define a reduction process and a method by which a useful description of a detected deadlock state can be obtained from the reduced net ' s information . a reduction tool and experimental results from applying the reduction process are discussed .
detection of strong unstable predicates in distributed programs . <eos> <unk> paper discusses detection of global predicates in a distributed program . a run of a distributed program results in a set of sequential traces , one for each process . these traces may be combined to form many global sequences consistent with the single run of the program . a strong global predicate is true in a run if it is true for all global sequences consistent with the run . we present algorithms which detect if the given strong global predicate became true in a run of a distributed program . our algorithms can be executed on line as well as off line . moreover , our algorithms do not assume that underlying channels satisfy fifo ordering .
generation of high quality tests for robustly untestable path delay faults . <eos> <unk> many designs a large portion of path delay faults is not robustly testable . in this paper , we investigate testing strategies for robustly untestable faults . we show that the quality of nonrobust tests may be very poor in detecting small defects caused by manufacturing process variation . we demonstrate that better quality nonrobust tests can be obtained by including timing information into the process of test generation . a good nonrobust test can tolerate larger timing variations on the off inputs . we also show that not all <unk> untestable path delay faults may be ignored in high quality delay testing . functional sensitizable paths are <unk> untestable but , under some faulty conditions , may degrade the performance of the circuit . however , up till now , there was no strategy for generating tests for such faults . in this paper , we present algorithms for generating high quality nonrobust and functional sensitizable tests . we also devise an algorithm for generating tests for validatable nonrobust faults which have a high quality in detecting defects but are hard to be generated automatically . our experimental results show that the quality of delay testing increases if validatable and high quality nonrobust tests , as well as tests for functional sensitizable path delay faults are included .
stability radii of systems with stochastic uncertainty and their optimization by output feedback . <eos> we consider linear plants controlled by dynamic output feedback which are subjected to <unk> stochastic parameter perturbations . the stability radii of these systems are characterized , and it is shown that , for real data , the real and the complex stability radii coincide . a corresponding result does not hold in the deterministic case , even for perturbations of single output feedback type . in a second part of the paper we study the problem of optimizing the stability radius by dynamic linear output feedback . necessary and sufficient conditions are derived for the existence of a compensator which achieves a suboptimal stability radius . these conditions consist of a parametrized riccati equation , a parametrized liapunov inequality , a coupling inequality , and a number of linear matrix inequalities ( one for each disturbance term ) . the corresponding problem in the deterministic case , the optimal mu synthesis problem , is still unsolved .
finite difference preconditioning for solving orthogonal collocation equations for boundary value problems . <eos> a technique to construct a low order finite difference preconditioner for solving orthogonal collocation equations for boundary value problems is presented . it is shown numerically and theoretically that the spectral condition numbers of the preconditioned collocation matrices are bounded by constants independent of the number of mesh nodes when certain exact low order finite difference preconditionings are used . preconditioners based on incomplete lu factorization are also discussed . numerical experiments show the efficiency and robustness of the preconditioning .
an evolutionary approach to constructing effective software reuse repositories . <eos> repositories for software reuse are faced with two interrelated problems ( <digit> ) acquiring the knowledge to initially construct the repository and ( <digit> ) modifying the repository to meet the evolving and dynamic needs of software development organizations . current software repository methods rely heavily on classification , which exacerbates <unk> and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively , this article outlines an approach that avoids these problems by choosing a retrieval method that utilizes minimal repository structure to effectively support the process of finding software <unk> . the approach is demonstrated through a pair of proof of concept prototypes peel , a tool to semiautomatically identify reusable components , and codefinder , a retrieval system that compensates for the lack of explicit knowledge structures through a spreading activation retrieval process . codefinder also allows component representations to be modified while users are searching for information . this mechanism adapts to the changing nature of the information in the repository and incrementally improves the repository while people use it . the combination of these techniques holds potential for designing software repositories that minimize up front costs , effectively support the search process , and evolve with an organization ' s changing needs .
a graduated assignment algorithm for graph matching . <eos> abstracta graduated assignment algorithm for graph matching is presented which is fast and accurate even in the presence of high noise . by combining graduated nonconvexity , two way ( assignment ) constraints , and sparsity , large improvements in accuracy and speed are achieved . its low order computational complexity o ( lm ) , where l and m are the number of links in the two graphs and robustness in the presence of noise offer advantages over traditional combinatorial approaches . the algorithm , not restricted to any special class of graph , is applied to subgraph isomorphism , weighted graph matching , and attributed relational graph matching . to illustrate the performance of the algorithm , attributed relational graphs derived from objects are matched . then , results from twenty five thousand experiments conducted on <digit> node random graphs of varying types ( graphs with only zero one links , weighted graphs , and graphs with node attributes and multiple link types ) are reported . no comparable results have been reported by any other graph matching algorithm before in the research literature . twenty five hundred control experiments are conducted using a relaxation labeling algorithm and large improvements in accuracy are demonstrated .
a first order isomorphism theorem . <eos> we show that for most complexity classes of interest , all sets complete under first order projections ( fops ) are isomorphic under first order isomorphisms . that is , a very restricted version of the berman hartmanis conjecture holds . since natural complete problems seem to stay complete via fops , this indicates that up to first order isomorphism there is only one natural complete problem for each nice complexity class .
stochastic scheduling with variable profile and precedence constraints . <eos> in this paper , we consider the stochastic profile scheduling problem of a partially ordered set of tasks on uniform processors . the set of available processors varies in time . the running times of the tasks are independent random variables with exponential distributions . we obtain a sufficient condition under which a list policy stochastically minimizes the makespan within the class of preemptive policies . this result allows us to obtain a simple optimal policy when the partial order is an interval order , an in forest , or an out forest .
on bounded queries and approximation . <eos> this paper investigates the computational complexity of approximating several np optimization problems using the number of queries to an np oracle as a complexity measure . the results show a tradeoff between the closeness of the approximation and the number of queries required . for an approximation factor k ( n ) , log log k ( n ) n queries to an np oracle can be used to approximate the maximum clique size of a graph within a factor of k ( n ) . however , this approximation can not be achieved using fewer than log log k ( n ) n c queries to any oracle unless is a constant that does not depend on k . these results hold for approximation factors k ( n ) geq <digit> that belong to a class of functions which includes any integer constant function , log n , log a n , and n <digit> a . similar results are obtained for graph coloring , set cover , and other np optimization problems .
resource bounds for self stabilizing message driven protocols . <eos> self stabilizing message driven protocols are defined and discussed . the class weak exclusion that contains many natural tasks such as ell exclusion and token passing is defined , and it is shown that in any execution of any self stabilizing protocol for a task in this class , the configuration size must grow at least in a logarithmic rate . this last lower bound is valid even if the system is supported by a time out mechanism that prevents communication deadlocks . then we present three self stabilizing message driven protocols for token passing . the rate of growth of configuration size for all three protocols matches the aforementioned lower bound . our protocols are presented for two processor systems but can be easily adapted to rings of arbitrary size . our results have an interesting interpretation in terms of automata theory .
universal wormhole routing . <eos> <unk> this paper , we examine the wormhole routing problem in terms of the congestion c and dilation d for a set of packet paths . we show , with mild restrictions , that there is a simple randomized algorithm for routing any set of p packets in o left ( cd eta cl eta , , rm log , , p right ) time with high probability , where l is the number of flits in a packet , and only a constant number of flits are stored in each queue at any time . using this result , we show that a fat tree network of area ( a ) can simulate wormhole routing on any network of comparable area with o ( <unk> ) slowdown , when all worms have the same length . variable length worms are also considered . we run some simulations on the fat tree which show that not only does wormhole routing tend to perform better than the more heavily studied store and forward routing in this context , but that performance superior to our provable bound is attainable in practice .
reducing communication latency with path multiplexing in optically interconnected multiprocessor systems . <eos> <unk> communication latency , which is a performance bottleneck in optically interconnected multiprocessor systems , is of prominent importance . a conventional approach for establishing connections in multiplexed networks uses a set of independent time slots ( or virtual channels ) along a path for each connection . this approach requires the use of switching devices capable of interchanging time slots , and thus introduces latency in addition to hardware and control complexity . in this paper , we propose an approach to all optical time division multiplexed ( tdm ) communications in multiprocessor systems . the idea is to establish a connection along a path using a set of time slots ( or virtual channels ) that are dependent on each other , so that no time slot interchanging is required . we compare the proposed approach with the conventional one in terms of the overall communication latency . we found that , despite the possibility that establishing a connection may take a longer time , the proposed approach will result in lower overall communication latency as it eliminates the delays introduced by the time slot interchanging switching devices .
a general method for maximizing the error detecting ability of distributed algorithms . <eos> <unk> bound on component failures and their spatial distribution govern the fault tolerance of any candidate error detecting algorithm . for distributed memory multiprocessors , the specific algorithm and the topology of the processor interconnection network define these bounds . this paper introduces the maximal fault index , derived from the system topology and local communication patterns , to demonstrate how a maximal number of simultaneous component failures can be tolerated for a particular interconnection network and error detecting algorithm . the index is used to design a mapping of processes to processor groups such that the error detecting ability of the algorithm is preserved for certain multiple simultaneous processor failures .
on runtime parallel scheduling for processor load balancing . <eos> <unk> scheduling is a new approach for load balancing . in parallel scheduling , all processors cooperate to schedule work . parallel scheduling is able to accurately balance the load by using global load information at compile time or runtime . it provides high quality load balancing . this paper presents an overview of the parallel scheduling technique . scheduling algorithms for tree , hypercube , and mesh networks are presented . these algorithms can fully balance the load and maximize locality at runtime . communication costs are significantly reduced compared to other existing algorithms .
an empirical evaluation of performance memory trade offs in time warp . <eos> <unk> performance of the time warp mechanism is experimentally evaluated when only a limited amount of memory is available to the parallel computation . an implementation of the cancelback protocol is used for memory management on a shared memory architecture , viz . , ksr to evaluate the performance vs . memory tradeoff . the implementation of the cancelback protocol supports canceling back more than one memory object when memory has been exhausted ( the precise number is referred to as the salvage parameter ) and incorporates a non work conserving processor scheduling technique to prevent starvation . several synthetic and benchmark programs are used that provide interesting stress cases for evaluating the limited memory behavior . the experiments are extensively monitored to determine the extent to which various factors may affect performance . several observations are made by analyzing the behavior of time warp under limited memory <digit> ) depending on the available memory and asymmetry in the workload , canceling back several memory objects at one time ( i . e . , a salvage parameter value of more than one ) improves performance significantly , by reducing certain overheads . however , performance is relatively insensitive to the salvage parameter except at extreme values . <digit> ) the speedup vs . memory curve for time warp programs has a well defined knee before which speedup increases very rapidly with memory and beyond which there is little performance gain with increased memory . performance nearly equivalent to that with large amounts of memory can be achieved with only a modest amount of additional memory beyond that required for sequential execution , if memory management overheads are small compared to the event granularity . these results indicate that contrary to the common belief , memory usage by time warp can be controlled within reasonable limits without any significant loss of performance .
crash resilient communication in dynamic networks . <eos> <unk> end to end data delivery protocol for dynamic communication networks is presented . the protocol uses bounded sequence numbers and can tolerate both link failures and ( intermediate ) processor crashes . previous bounded end to end protocols could not tolerate crashes . we present a self stabilizing version of the algorithm that can recover from crashes of the sender and the receiver as well as of intermediate processors . starting with the network in an arbitrary state , the self stabilizing version guarantees proper transmission of messages following a finite convergence period .
a graph partitioning approach to sequential diagnosis . <eos> <unk> paper describes a generalized sequential diagnosis algorithm whose analysis leads to strong diagnosability results for a variety of multiprocessor interconnection topologies . the overall complexity of this algorithm in terms of total testing and syndrome decoding time is linear in the number of edges in the interconnection graph and the total number of iterations of diagnosis and repair needed by the algorithm is bounded by the diameter of the interconnection graph . the degree of diagnosability of this algorithm for a given interconnection graph is shown to be directly related to a graph parameter which we refer to as the partition number . we approximate this graph parameter for several interconnection topologies and thereby obtain lower bounds on degree of diagnosability achieved by our algorithm on these topologies . if we let n denote total number of vertices in the interconnection graph and denote the maximum degree of any vertex in it , then our results may be summarized as follows . we show that a symmetric d dimensional grid graph is sequentially omega left ( n d over d <digit> right ) diagnosable for any fixed d . for hypercubes , <unk> log n dimensional grid graphs , it is shown that our algorithm leads to a surprising omega left ( n , rm log , log , n over log , n right ) degree of diagnosability . next we show that the degree of diagnosability of an arbitrary interconnection graph by our algorithm is omega left ( sqrt n over delta right ) . this bound translates to an omega left ( sqrt n right ) degree of diagnosability for cube connected cycles and an omega left ( sqrt n over k right ) degree of diagnosability for k ary trees . finally , we augment our algorithm with another algorithm to show that every topology is omega left ( n <digit> over <digit> right ) diagnosable .
a traffic balanced adaptive wormhole routing scheme for two dimensional meshes . <eos> <unk> this paper , we analyze several issues involved in developing low latency adaptive wormhole routing schemes for two dimensional meshes . it is observed that along with adaptivity , balanced distribution of traffic has a significant impact on the system performance . motivated by this observation , we develop a new fully adaptive routing algorithm called positive first negative first for two dimensional meshes . the algorithm uses only two virtual channels per physical channel creating two virtual networks . the messages are routed positive first in one virtual network and negative first in the other . because of this combination , the algorithm distributes the system load uniformly throughout the network and is also fully adaptive . it is shown that the proposed algorithm results in providing better performance in terms of the average network latency and throughput when compared with the previously proposed routing algorithms .
control and energy consumption in communications for nomadic computing . <eos> <unk> consider the problem of communications over a wireless channel in support of data transmissions from the perspective of small portable devices that must rely on limited battery energy . we model the channel outages as statistically correlated errors . classic arq strategies are found to lead to a considerable waste of energy , due to the large number of transmissions . the use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria . as an example , a simple probing scheme , which slows down the transmission rate when the channel is impaired , is shown to be more energy efficient , with a slight loss in throughput . a modified scheme that yields slightly better performance but requires some additional complexity is also studied . some references on the modeling of battery cells are discussed to highlight the fact that battery charge capacity is strongly influenced by the available relaxation time between current pulses . a formal approach that can track complex models for power sources , including dynamic charge recovery , is also developed .
improving nfs performance over wireless links . <eos> <unk> is a widely used remote file access protocol that has been tuned to perform well on traditional lans which exhibit low error rates . users migrating to mobile hosts would like continued remote file access via nfs . however , low bandwidth and high error rates degrade performance on mobile hosts using wireless links , hindering the use of nfs . we conducted experiments to study the behavior of nfs in a wireless testbed . based on these experiments , we incorporated modifications into the mobile nfs client . this paper presents two mechanisms which improve nfs performance over wireless links an aggressive nfs client and link level retransmissions . our experiments show that these mechanisms improve response time by up to <digit> % , which brings the performance to within <digit> % of that obtained in zero error conditions .
an efficient protocol for call setup and path migration in ieee <digit> . <digit> based personal communication networks . <eos> <unk> , dqdb ( ieee <digit> . <digit> ) man has been proposed as a component of personal communication networks , in which base stations of wireless infrastructures are connected by a number of <unk> which in turn are connected via bridges . we propose a protocol for call setup and path migration in a cluster of <unk> . the protocol uses a link state like routing method for path selection and a source routing based scheme for path establishment . in addition , we propose a labeling scheme that makes it possible to carry the path information needed by the source routing protocol in a single <digit> octet dqdb slot . without such a labeling scheme , source routing would be inefficient for our purpose .
centralized and decentralized supervisory control of nondeterministic systems under partial observation . <eos> in this paper we extend our earlier work on supervisory control of nondeterministic systems using prioritized synchronization as the mechanism of control and trajectory model as the modeling formalism by considering design of supervisors under partial observation . we introduce the notion of observation compatible systems and show that prioritized synchronous composition ( psc ) of observation compatible systems can be used as a mechanism of control of nondeterministic systems under partial observation in presence of driven events . necessary and sufficient conditions that depend on the trajectory model as opposed to the language model of the plant are obtained for the existence of centralized as well as decentralized supervision . our work on centralized control shows that the results of the traditional supervisory control can be extended to the above setting , provided that the supervisor is deterministic and the observation mask is projection type . on the other hand , our work on decentralized control is based on a new relation between controllability , observability , co observability , and psc that we derive in this paper .
a predictor corrector algorithm for a class of nonlinear saddle point problems . <eos> an interior path following algorithm is proposed for solving the nonlinear saddle point problem rm minimax c tx ph ( x ) b ty psi ( y ) y tax <unk> <unk> rm subject to ( x , y ) in x ti y su r n ti r m , <unk> where ph ( x ) and ps ( y ) are smooth convex functions and x and y are boxes ( hyperrectangles ) . this problem is closely related to the models in stochastic programming and optimal control studied by rockafellar and wets ( math . programming studies , <digit> ( <digit> ) , pp . <digit> <digit> siam j . control optim . , <digit> ( <digit> ) , pp . <digit> <digit> ) . existence and error bound results on a central path are derived . starting from an initial solution near the central path with duality gap o ( mu ) , the algorithm finds an ep optimal solution of the problem in o ( sqrt m n , log mu ep ) iterations if both ph ( x ) and ps ( y ) satisfy a scaled lipschitz condition .
recent developments in high level synthesis . <eos> we survey recent developments in high level synthesis technology for vlsi design . the need for higher level design automation tools are discussed first . we then describe some basic techniques for various subtasks of high level synthesis . techniques that have been proposed in the past few years ( since <digit> ) for various subtasks of high level synthesis are surveyed . we also survey some new synthesis objectives including testability , power efficiency , and reliability .
complete orthogonal decomposition for weighted least squares . <eos> this paper proposes a complete orthogonal decomposition ( cod ) algorithm for solving weighted least squares problems . in applications , the weight matrix can be highly ill conditioned , and this can cause standard methods like qr factorization to return inaccurate answers in floating point arithmetic . stewart and todd independently established a norm bound for the weighted least squares problem that is independent of the weight matrix . vavasis proposed a definition of a stable solution of weighted least squares based on this norm bound the solution computed by a stable algorithm must satisfy an accuracy bound that is not affected by ill conditioning in the weight matrix . a forward error analysis shows that the cod algorithm is stable in this sense , but it is simpler and more efficient than the algorithm proposed by vavasis . our forward error bound is contrasted to the backward error analysis of other previous works on weighted least squares .
